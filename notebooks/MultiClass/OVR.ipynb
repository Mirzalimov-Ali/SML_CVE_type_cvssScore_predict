{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77a9abae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11b3cf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\SML_Projects\\SML_CVE_type_cwe_predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28d6c045",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/preprocessed/preprocessed_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a99554b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=0.1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8c69786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10296 entries, 0 to 10295\n",
      "Data columns (total 29 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   cve_id                      10296 non-null  float64\n",
      " 1   description                 10296 non-null  float64\n",
      " 2   cvss_score                  10296 non-null  float64\n",
      " 3   cwe                         10296 non-null  float64\n",
      " 4   vendor                      10296 non-null  float64\n",
      " 5   product                     10296 non-null  float64\n",
      " 6   publish_date                10296 non-null  float64\n",
      " 7   type                        10296 non-null  float64\n",
      " 8   vendor_freq                 10296 non-null  float64\n",
      " 9   product_freq                10296 non-null  float64\n",
      " 10  desc_len                    10296 non-null  float64\n",
      " 11  desc_word_count             10296 non-null  float64\n",
      " 12  desc_num_count              10296 non-null  float64\n",
      " 13  desc_upper_ratio            10296 non-null  float64\n",
      " 14  desc_exclamation            10296 non-null  float64\n",
      " 15  desc_question               10296 non-null  float64\n",
      " 16  vendor_product_interaction  10296 non-null  float64\n",
      " 17  XSS_score                   10296 non-null  float64\n",
      " 18  SQLi_score                  10296 non-null  float64\n",
      " 19  RCE_score                   10296 non-null  float64\n",
      " 20  DoS_score                   10296 non-null  float64\n",
      " 21  CSRF_score                  10296 non-null  float64\n",
      " 22  AuthBypass_score            10296 non-null  float64\n",
      " 23  PrivEsc_score               10296 non-null  float64\n",
      " 24  PathTraversal_score         10296 non-null  float64\n",
      " 25  SSRF_score                  10296 non-null  float64\n",
      " 26  InfoDisclosure_score        10296 non-null  float64\n",
      " 27  Other_score                 10296 non-null  float64\n",
      " 28  cvss_keywords_score         10296 non-null  float64\n",
      "dtypes: float64(29)\n",
      "memory usage: 2.3 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0c95da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, HistGradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "352d4b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(['type', 'cvss_score'], axis=1)   \n",
    "y = df[['type', 'cvss_score']] \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1437fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079b0211",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "726e35c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy for 'type': 0.7932038834951456\n",
      "Logistic Regression Accuracy for 'cvss_score' : 0.5927184466019417\n",
      "K-Fold mean F1 (type): 0.6578902419513474\n",
      "K-Fold std  F1 (type): 0.01582757574466673\n",
      "K-Fold mean F1 (cvss_score): 0.2945602486752269\n",
      "K-Fold std  F1 (cvss_score): 0.004937807713439468\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        28\n",
      "         1.0       0.95      0.78      0.86        50\n",
      "         2.0       0.78      0.94      0.85       103\n",
      "         3.0       0.36      0.13      0.19       200\n",
      "         4.0       0.73      0.87      0.79       834\n",
      "         5.0       0.93      0.68      0.79        41\n",
      "         6.0       0.44      0.47      0.45        36\n",
      "         7.0       0.81      0.75      0.78       155\n",
      "         8.0       1.00      0.98      0.99       176\n",
      "         9.0       1.00      0.57      0.73        14\n",
      "        10.0       0.95      0.95      0.95       423\n",
      "\n",
      "    accuracy                           0.79      2060\n",
      "   macro avg       0.72      0.65      0.67      2060\n",
      "weighted avg       0.76      0.79      0.77      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.00      0.01       234\n",
      "         1.0       0.50      0.39      0.44       637\n",
      "         2.0       0.00      0.00      0.00        87\n",
      "         3.0       0.62      0.88      0.73      1102\n",
      "\n",
      "    accuracy                           0.59      2060\n",
      "   macro avg       0.41      0.32      0.29      2060\n",
      "weighted avg       0.54      0.59      0.53      2060\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "lr = OneVsRestClassifier(LogisticRegression())\n",
    "multi_lr = MultiOutputClassifier(lr)\n",
    "\n",
    "multi_lr.fit(x_train, y_train)\n",
    "y_pred = multi_lr.predict(x_test)\n",
    "\n",
    "lr_accuracy_type = accuracy_score(y_test['type'], y_pred[:,0])\n",
    "lr_accuracy_cvss_score  = accuracy_score(y_test['cvss_score'], y_pred[:,1])\n",
    "\n",
    "lr_scores_type = cross_val_score(lr, x, y['type'], cv=kf, scoring='f1_macro')\n",
    "lr_scores_cvss_score  = cross_val_score(lr, x, y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"Logistic Regression Accuracy for 'type':\", lr_accuracy_type)\n",
    "print(\"Logistic Regression Accuracy for 'cvss_score' :\", lr_accuracy_cvss_score)\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", lr_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", lr_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", lr_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", lr_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d8747e",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de0543cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy for 'type': 0.925242718446602\n",
      "Decision Tree Accuracy for 'cvss_score' : 0.6305825242718447\n",
      "K-Fold mean F1 (type): 0.9008647012825316\n",
      "K-Fold std  F1 (type): 0.004726262095407054\n",
      "K-Fold mean F1 (cvss_score): 0.46223475376919737\n",
      "K-Fold std  F1 (cvss_score): 0.006891375042907755\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.82      0.88        28\n",
      "         1.0       1.00      0.90      0.95        50\n",
      "         2.0       0.87      0.81      0.84       103\n",
      "         3.0       0.97      0.90      0.94       200\n",
      "         4.0       0.96      0.93      0.94       834\n",
      "         5.0       0.95      0.85      0.90        41\n",
      "         6.0       0.78      0.78      0.78        36\n",
      "         7.0       0.88      0.86      0.87       155\n",
      "         8.0       1.00      0.98      0.99       176\n",
      "         9.0       0.88      1.00      0.93        14\n",
      "        10.0       0.86      0.99      0.92       423\n",
      "\n",
      "    accuracy                           0.93      2060\n",
      "   macro avg       0.92      0.89      0.90      2060\n",
      "weighted avg       0.93      0.93      0.93      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.47      0.18      0.26       234\n",
      "         1.0       0.61      0.41      0.49       637\n",
      "         2.0       0.51      0.36      0.42        87\n",
      "         3.0       0.65      0.88      0.75      1102\n",
      "\n",
      "    accuracy                           0.63      2060\n",
      "   macro avg       0.56      0.45      0.48      2060\n",
      "weighted avg       0.61      0.63      0.60      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt = OneVsRestClassifier(DecisionTreeClassifier(random_state=42))\n",
    "multi_dt = MultiOutputClassifier(dt)\n",
    "\n",
    "multi_dt.fit(x_train, y_train)\n",
    "y_pred = multi_dt.predict(x_test)\n",
    "\n",
    "dt_accuracy_type = accuracy_score(y_test['type'], y_pred[:,0])\n",
    "dt_accuracy_cvss_score  = accuracy_score(y_test['cvss_score'], y_pred[:,1])\n",
    "\n",
    "dt_scores_type = cross_val_score(dt, x, y['type'], cv=kf, scoring='f1_macro')\n",
    "dt_scores_cvss_score  = cross_val_score(dt, x, y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"Decision Tree Accuracy for 'type':\", dt_accuracy_type)\n",
    "print(\"Decision Tree Accuracy for 'cvss_score' :\", dt_accuracy_cvss_score)\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", dt_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", dt_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", dt_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", dt_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a710c757",
   "metadata": {},
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0462f063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy for 'type': 0.9257281553398058\n",
      "Random Forest Accuracy for 'cvss_score' : 0.6810679611650485\n",
      "K-Fold mean F1 (type): 0.8613415596451307\n",
      "K-Fold std  F1 (type): 0.00942105051657691\n",
      "K-Fold mean F1 (cvss_score): 0.5558808743297208\n",
      "K-Fold std  F1 (cvss_score): 0.005453467974920201\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.18      0.30        28\n",
      "         1.0       0.96      0.90      0.93        50\n",
      "         2.0       0.85      0.97      0.90       103\n",
      "         3.0       0.83      0.92      0.87       200\n",
      "         4.0       0.93      0.96      0.95       834\n",
      "         5.0       0.94      0.71      0.81        41\n",
      "         6.0       0.72      0.58      0.65        36\n",
      "         7.0       0.90      0.85      0.87       155\n",
      "         8.0       1.00      0.99      0.99       176\n",
      "         9.0       0.93      1.00      0.97        14\n",
      "        10.0       0.97      0.96      0.96       423\n",
      "\n",
      "    accuracy                           0.93      2060\n",
      "   macro avg       0.91      0.82      0.84      2060\n",
      "weighted avg       0.93      0.93      0.92      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.26      0.35       234\n",
      "         1.0       0.61      0.54      0.57       637\n",
      "         2.0       0.88      0.40      0.55        87\n",
      "         3.0       0.72      0.88      0.79      1102\n",
      "\n",
      "    accuracy                           0.68      2060\n",
      "   macro avg       0.69      0.52      0.57      2060\n",
      "weighted avg       0.67      0.68      0.66      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = OneVsRestClassifier(RandomForestClassifier(random_state=42))\n",
    "multi_rf = MultiOutputClassifier(rf)\n",
    "\n",
    "multi_rf.fit(x_train, y_train)\n",
    "y_pred = multi_rf.predict(x_test)\n",
    "\n",
    "rf_accuracy_type = accuracy_score(y_test['type'], y_pred[:,0])\n",
    "rf_accuracy_cvss_score  = accuracy_score(y_test['cvss_score'], y_pred[:,1])\n",
    "\n",
    "rf_scores_type = cross_val_score(rf, x, y['type'], cv=kf, scoring='f1_macro')\n",
    "rf_scores_cvss_score  = cross_val_score(rf, x, y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"Random Forest Accuracy for 'type':\", rf_accuracy_type)\n",
    "print(\"Random Forest Accuracy for 'cvss_score' :\", rf_accuracy_cvss_score)\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", rf_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", rf_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", rf_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", rf_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c0ba7c",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "daaa426e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Accuracy for 'type': 0.9446601941747573\n",
      "Gradient Boosting Accuracy for 'cvss_score' : 0.6529126213592233\n",
      "K-Fold mean F1 (type): 0.9298995787444418\n",
      "K-Fold std  F1 (type): 0.009081636310672696\n",
      "K-Fold mean F1 (cvss_score): 0.5083492841901606\n",
      "K-Fold std  F1 (cvss_score): 0.008168248896195062\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.86      0.92        28\n",
      "         1.0       1.00      0.96      0.98        50\n",
      "         2.0       0.87      0.97      0.92       103\n",
      "         3.0       0.97      0.92      0.94       200\n",
      "         4.0       0.93      0.95      0.94       834\n",
      "         5.0       0.95      0.90      0.93        41\n",
      "         6.0       0.79      0.86      0.83        36\n",
      "         7.0       0.94      0.84      0.89       155\n",
      "         8.0       0.99      0.98      0.99       176\n",
      "         9.0       0.88      1.00      0.93        14\n",
      "        10.0       0.96      0.97      0.97       423\n",
      "\n",
      "    accuracy                           0.94      2060\n",
      "   macro avg       0.94      0.93      0.93      2060\n",
      "weighted avg       0.95      0.94      0.94      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.22      0.31       234\n",
      "         1.0       0.59      0.43      0.49       637\n",
      "         2.0       0.85      0.38      0.52        87\n",
      "         3.0       0.67      0.90      0.77      1102\n",
      "\n",
      "    accuracy                           0.65      2060\n",
      "   macro avg       0.67      0.48      0.53      2060\n",
      "weighted avg       0.64      0.65      0.62      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gb = OneVsRestClassifier(GradientBoostingClassifier(random_state=42))\n",
    "multi_gb = MultiOutputClassifier(gb)\n",
    "\n",
    "multi_gb.fit(x_train, y_train)\n",
    "y_pred = multi_gb.predict(x_test)\n",
    "\n",
    "gb_accuracy_type = accuracy_score(y_test['type'], y_pred[:,0])\n",
    "gb_accuracy_cvss_score  = accuracy_score(y_test['cvss_score'], y_pred[:,1])\n",
    "\n",
    "gb_scores_type = cross_val_score(gb, x, y['type'], cv=kf, scoring='f1_macro')\n",
    "gb_scores_cvss_score  = cross_val_score(gb, x, y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"Gradient Boosting Accuracy for 'type':\", gb_accuracy_type)\n",
    "print(\"Gradient Boosting Accuracy for 'cvss_score' :\", gb_accuracy_cvss_score)\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", gb_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", gb_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", gb_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", gb_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38e3c99",
   "metadata": {},
   "source": [
    "# Extra Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f70ca78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Tree Accuracy for 'type': 0.9053398058252428\n",
      "Extra Tree Accuracy for 'cvss_score' : 0.6854368932038835\n",
      "K-Fold mean F1 (type): 0.8481169578028119\n",
      "K-Fold std  F1 (type): 0.007760029574715206\n",
      "K-Fold mean F1 (cvss_score): 0.5595448739159633\n",
      "K-Fold std  F1 (cvss_score): 0.011210854839779022\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.25      0.39        28\n",
      "         1.0       1.00      0.90      0.95        50\n",
      "         2.0       0.83      0.93      0.88       103\n",
      "         3.0       0.79      0.82      0.81       200\n",
      "         4.0       0.90      0.95      0.92       834\n",
      "         5.0       0.90      0.68      0.78        41\n",
      "         6.0       0.72      0.64      0.68        36\n",
      "         7.0       0.89      0.81      0.85       155\n",
      "         8.0       0.99      0.98      0.99       176\n",
      "         9.0       0.88      1.00      0.93        14\n",
      "        10.0       0.97      0.94      0.96       423\n",
      "\n",
      "    accuracy                           0.91      2060\n",
      "   macro avg       0.89      0.81      0.83      2060\n",
      "weighted avg       0.91      0.91      0.90      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.28      0.38       234\n",
      "         1.0       0.62      0.56      0.59       637\n",
      "         2.0       0.85      0.45      0.59        87\n",
      "         3.0       0.72      0.86      0.78      1102\n",
      "\n",
      "    accuracy                           0.69      2060\n",
      "   macro avg       0.69      0.54      0.58      2060\n",
      "weighted avg       0.68      0.69      0.67      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "et = OneVsRestClassifier(ExtraTreesClassifier(random_state=42))\n",
    "multi_et = MultiOutputClassifier(et)\n",
    "\n",
    "multi_et.fit(x_train, y_train)\n",
    "y_pred = multi_et.predict(x_test)\n",
    "\n",
    "et_accuracy_type = accuracy_score(y_test['type'], y_pred[:,0])\n",
    "et_accuracy_cvss_score  = accuracy_score(y_test['cvss_score'], y_pred[:,1])\n",
    "\n",
    "et_scores_type = cross_val_score(et, x, y['type'], cv=kf, scoring='f1_macro')\n",
    "et_scores_cvss_score  = cross_val_score(et, x, y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"Extra Tree Accuracy for 'type':\", et_accuracy_type)\n",
    "print(\"Extra Tree Accuracy for 'cvss_score' :\", et_accuracy_cvss_score)\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", et_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", et_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", et_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", et_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43e93bb",
   "metadata": {},
   "source": [
    "# Hist Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "243723a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hist Gradient Boosting Accuracy for 'type': 0.9490291262135923\n",
      "Hist Gradient Boosting Accuracy for 'cvss_score' : 0.6810679611650485\n",
      "\n",
      "K-Fold mean F1 (type): 0.927155457699199\n",
      "K-Fold std  F1 (type): 0.0030995143570212136\n",
      "\n",
      "K-Fold mean F1 (cvss_score): 0.5520397187647562\n",
      "K-Fold std  F1 (cvss_score): 0.0051424719455767\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.89      0.94        28\n",
      "         1.0       1.00      0.96      0.98        50\n",
      "         2.0       0.86      0.93      0.89       103\n",
      "         3.0       0.95      0.93      0.94       200\n",
      "         4.0       0.95      0.96      0.95       834\n",
      "         5.0       0.95      0.93      0.94        41\n",
      "         6.0       0.82      0.89      0.85        36\n",
      "         7.0       0.91      0.85      0.88       155\n",
      "         8.0       0.99      0.99      0.99       176\n",
      "         9.0       1.00      1.00      1.00        14\n",
      "        10.0       0.98      0.98      0.98       423\n",
      "\n",
      "    accuracy                           0.95      2060\n",
      "   macro avg       0.95      0.94      0.94      2060\n",
      "weighted avg       0.95      0.95      0.95      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.27      0.37       234\n",
      "         1.0       0.61      0.54      0.57       637\n",
      "         2.0       0.83      0.39      0.53        87\n",
      "         3.0       0.71      0.87      0.78      1102\n",
      "\n",
      "    accuracy                           0.68      2060\n",
      "   macro avg       0.69      0.52      0.57      2060\n",
      "weighted avg       0.67      0.68      0.66      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hgb = OneVsRestClassifier(HistGradientBoostingClassifier(random_state=42))\n",
    "multi_hgb = MultiOutputClassifier(hgb)\n",
    "\n",
    "multi_hgb.fit(x_train, y_train)\n",
    "y_pred = multi_hgb.predict(x_test)\n",
    "\n",
    "hgb_accuracy_type = accuracy_score(y_test['type'], y_pred[:,0])\n",
    "hgb_accuracy_cvss_score  = accuracy_score(y_test['cvss_score'], y_pred[:,1])\n",
    "\n",
    "hgb_scores_type = cross_val_score(hgb, x, y['type'], cv=kf, scoring='f1_macro')\n",
    "hgb_scores_cvss_score  = cross_val_score(hgb, x, y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"Hist Gradient Boosting Accuracy for 'type':\", hgb_accuracy_type)\n",
    "print(\"Hist Gradient Boosting Accuracy for 'cvss_score' :\", hgb_accuracy_cvss_score)\n",
    "\n",
    "print(\"\\nK-Fold mean F1 (type):\", hgb_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", hgb_scores_type.std())\n",
    "\n",
    "print(\"\\nK-Fold mean F1 (cvss_score):\", hgb_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", hgb_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e23cb1",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ec78b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy for 'type': 0.8121359223300971\n",
      "KNN Accuracy for 'cvss_score' : 0.6029126213592233\n",
      "K-Fold mean F1 (type): 0.703665024476167\n",
      "K-Fold std  F1 (type): 0.009585380349919074\n",
      "K-Fold mean F1 (cvss_score): 0.47555034828233395\n",
      "K-Fold std  F1 (cvss_score): 0.001946083182048487\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.43      0.21      0.29        28\n",
      "         1.0       0.91      0.84      0.88        50\n",
      "         2.0       0.78      0.78      0.78       103\n",
      "         3.0       0.57      0.66      0.61       200\n",
      "         4.0       0.81      0.85      0.83       834\n",
      "         5.0       0.67      0.63      0.65        41\n",
      "         6.0       0.58      0.31      0.40        36\n",
      "         7.0       0.77      0.72      0.74       155\n",
      "         8.0       0.97      0.97      0.97       176\n",
      "         9.0       0.92      0.79      0.85        14\n",
      "        10.0       0.92      0.90      0.91       423\n",
      "\n",
      "    accuracy                           0.81      2060\n",
      "   macro avg       0.76      0.69      0.72      2060\n",
      "weighted avg       0.81      0.81      0.81      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.31      0.34       234\n",
      "         1.0       0.55      0.54      0.54       637\n",
      "         2.0       0.30      0.36      0.33        87\n",
      "         3.0       0.70      0.72      0.71      1102\n",
      "\n",
      "    accuracy                           0.60      2060\n",
      "   macro avg       0.48      0.48      0.48      2060\n",
      "weighted avg       0.60      0.60      0.60      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn = OneVsRestClassifier(KNeighborsClassifier(n_neighbors=1))\n",
    "multi_knn = MultiOutputClassifier(knn)\n",
    "\n",
    "multi_knn.fit(x_train, y_train)\n",
    "y_pred = multi_knn.predict(x_test)\n",
    "\n",
    "knn_accuracy_type = accuracy_score(y_test['type'], y_pred[:,0])\n",
    "knn_accuracy_cvss_score  = accuracy_score(y_test['cvss_score'], y_pred[:,1])\n",
    "\n",
    "knn_scores_type = cross_val_score(knn, x, y['type'], cv=kf, scoring='f1_macro')\n",
    "knn_scores_cvss_score  = cross_val_score(knn, x, y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"KNN Accuracy for 'type':\", knn_accuracy_type)\n",
    "print(\"KNN Accuracy for 'cvss_score' :\", knn_accuracy_cvss_score)\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", knn_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", knn_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", knn_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", knn_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf69381b",
   "metadata": {},
   "source": [
    "# Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "041440e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost Accuracy for 'type': 0.9276699029126214\n",
      "Adaboost Accuracy for 'cvss_score' : 0.6038834951456311\n",
      "K-Fold mean F1 (type): 0.9023378584644227\n",
      "K-Fold std  F1 (type): 0.008892976991522696\n",
      "K-Fold mean F1 (cvss_score): 0.35596267681870053\n",
      "K-Fold std  F1 (cvss_score): 0.003983794317930631\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.75      0.86        28\n",
      "         1.0       1.00      0.90      0.95        50\n",
      "         2.0       0.84      0.98      0.91       103\n",
      "         3.0       0.93      0.86      0.89       200\n",
      "         4.0       0.91      0.95      0.93       834\n",
      "         5.0       0.95      0.85      0.90        41\n",
      "         6.0       0.78      0.58      0.67        36\n",
      "         7.0       0.89      0.80      0.84       155\n",
      "         8.0       0.99      0.99      0.99       176\n",
      "         9.0       1.00      1.00      1.00        14\n",
      "        10.0       0.96      0.98      0.97       423\n",
      "\n",
      "    accuracy                           0.93      2060\n",
      "   macro avg       0.93      0.88      0.90      2060\n",
      "weighted avg       0.93      0.93      0.93      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.51      0.17      0.25       234\n",
      "         1.0       0.50      0.43      0.46       637\n",
      "         2.0       0.00      0.00      0.00        87\n",
      "         3.0       0.65      0.85      0.73      1102\n",
      "\n",
      "    accuracy                           0.60      2060\n",
      "   macro avg       0.41      0.36      0.36      2060\n",
      "weighted avg       0.56      0.60      0.56      2060\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "ab = OneVsRestClassifier(AdaBoostClassifier(n_estimators=200))\n",
    "multi_ab = MultiOutputClassifier(ab)\n",
    "\n",
    "multi_ab.fit(x_train, y_train)\n",
    "y_pred = multi_ab.predict(x_test)\n",
    "\n",
    "ab_accuracy_type = accuracy_score(y_test['type'], y_pred[:,0])\n",
    "ab_accuracy_cvss_score  = accuracy_score(y_test['cvss_score'], y_pred[:,1])\n",
    "\n",
    "ab_scores_type = cross_val_score(ab, x, y['type'], cv=kf, scoring='f1_macro')\n",
    "ab_scores_cvss_score  = cross_val_score(ab, x, y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"Adaboost Accuracy for 'type':\", ab_accuracy_type)\n",
    "print(\"Adaboost Accuracy for 'cvss_score' :\", ab_accuracy_cvss_score)\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", ab_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", ab_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", ab_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", ab_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9124d4e1",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4314b54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 83, number of negative: 8153\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000788 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2988\n",
      "[LightGBM] [Info] Number of data points in the train set: 8236, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010078 -> initscore=-4.587301\n",
      "[LightGBM] [Info] Start training from score -4.587301\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 201, number of negative: 8035\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000799 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2988\n",
      "[LightGBM] [Info] Number of data points in the train set: 8236, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.024405 -> initscore=-3.688257\n",
      "[LightGBM] [Info] Start training from score -3.688257\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 457, number of negative: 7779\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000775 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2988\n",
      "[LightGBM] [Info] Number of data points in the train set: 8236, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.055488 -> initscore=-2.834500\n",
      "[LightGBM] [Info] Start training from score -2.834500\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 7439\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000779 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2988\n",
      "[LightGBM] [Info] Number of data points in the train set: 8236, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.096770 -> initscore=-2.233637\n",
      "[LightGBM] [Info] Start training from score -2.233637\n",
      "[LightGBM] [Info] Number of positive: 3498, number of negative: 4738\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000777 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2988\n",
      "[LightGBM] [Info] Number of data points in the train set: 8236, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.424721 -> initscore=-0.303424\n",
      "[LightGBM] [Info] Start training from score -0.303424\n",
      "[LightGBM] [Info] Number of positive: 215, number of negative: 8021\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000766 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2988\n",
      "[LightGBM] [Info] Number of data points in the train set: 8236, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026105 -> initscore=-3.619180\n",
      "[LightGBM] [Info] Start training from score -3.619180\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 148, number of negative: 8088\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000875 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2988\n",
      "[LightGBM] [Info] Number of data points in the train set: 8236, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.017970 -> initscore=-4.000924\n",
      "[LightGBM] [Info] Start training from score -4.000924\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 605, number of negative: 7631\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000756 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2988\n",
      "[LightGBM] [Info] Number of data points in the train set: 8236, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.073458 -> initscore=-2.534746\n",
      "[LightGBM] [Info] Start training from score -2.534746\n",
      "[LightGBM] [Info] Number of positive: 645, number of negative: 7591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000743 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2988\n",
      "[LightGBM] [Info] Number of data points in the train set: 8236, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.078315 -> initscore=-2.465468\n",
      "[LightGBM] [Info] Start training from score -2.465468\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 70, number of negative: 8166\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000774 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2988\n",
      "[LightGBM] [Info] Number of data points in the train set: 8236, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.008499 -> initscore=-4.759239\n",
      "[LightGBM] [Info] Start training from score -4.759239\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 1517, number of negative: 6719\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000761 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2988\n",
      "[LightGBM] [Info] Number of data points in the train set: 8236, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.184191 -> initscore=-1.488205\n",
      "[LightGBM] [Info] Start training from score -1.488205\n",
      "[LightGBM] [Info] Number of positive: 796, number of negative: 7440\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000816 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2988\n",
      "[LightGBM] [Info] Number of data points in the train set: 8236, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.096649 -> initscore=-2.235027\n",
      "[LightGBM] [Info] Start training from score -2.235027\n",
      "[LightGBM] [Info] Number of positive: 2721, number of negative: 5515\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000733 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2988\n",
      "[LightGBM] [Info] Number of data points in the train set: 8236, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.330379 -> initscore=-0.706472\n",
      "[LightGBM] [Info] Start training from score -0.706472\n",
      "[LightGBM] [Info] Number of positive: 350, number of negative: 7886\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000806 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2988\n",
      "[LightGBM] [Info] Number of data points in the train set: 8236, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.042496 -> initscore=-3.114911\n",
      "[LightGBM] [Info] Start training from score -3.114911\n",
      "[LightGBM] [Info] Number of positive: 4369, number of negative: 3867\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000717 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2988\n",
      "[LightGBM] [Info] Number of data points in the train set: 8236, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.530476 -> initscore=0.122055\n",
      "[LightGBM] [Info] Start training from score 0.122055\n",
      "[LightGBM] [Info] Number of positive: 71, number of negative: 6793\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000731 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2969\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010344 -> initscore=-4.560968\n",
      "[LightGBM] [Info] Start training from score -4.560968\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 169, number of negative: 6695\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000557 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2969\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.024621 -> initscore=-3.679218\n",
      "[LightGBM] [Info] Start training from score -3.679218\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 386, number of negative: 6478\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000690 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2969\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.056235 -> initscore=-2.820330\n",
      "[LightGBM] [Info] Start training from score -2.820330\n",
      "[LightGBM] [Info] Number of positive: 673, number of negative: 6191\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000670 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2969\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098048 -> initscore=-2.219107\n",
      "[LightGBM] [Info] Start training from score -2.219107\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 3941\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000606 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2969\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.425845 -> initscore=-0.298824\n",
      "[LightGBM] [Info] Start training from score -0.298824\n",
      "[LightGBM] [Info] Number of positive: 178, number of negative: 6686\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000685 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2969\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.025932 -> initscore=-3.625988\n",
      "[LightGBM] [Info] Start training from score -3.625988\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 125, number of negative: 6739\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000669 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2969\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.018211 -> initscore=-3.987353\n",
      "[LightGBM] [Info] Start training from score -3.987353\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 498, number of negative: 6366\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000676 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2969\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.072552 -> initscore=-2.548127\n",
      "[LightGBM] [Info] Start training from score -2.548127\n",
      "[LightGBM] [Info] Number of positive: 519, number of negative: 6345\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000631 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2969\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.075612 -> initscore=-2.503518\n",
      "[LightGBM] [Info] Start training from score -2.503518\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 49, number of negative: 6815\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000704 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2969\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007139 -> initscore=-4.935061\n",
      "[LightGBM] [Info] Start training from score -4.935061\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 1273, number of negative: 5591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000572 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2969\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.185460 -> initscore=-1.479782\n",
      "[LightGBM] [Info] Start training from score -1.479782\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 76, number of negative: 6788\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000573 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011072 -> initscore=-4.492178\n",
      "[LightGBM] [Info] Start training from score -4.492178\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 168, number of negative: 6696\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000675 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.024476 -> initscore=-3.685302\n",
      "[LightGBM] [Info] Start training from score -3.685302\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 354, number of negative: 6510\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000690 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.051573 -> initscore=-2.911798\n",
      "[LightGBM] [Info] Start training from score -2.911798\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 662, number of negative: 6202\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000633 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.096445 -> initscore=-2.237362\n",
      "[LightGBM] [Info] Start training from score -2.237362\n",
      "[LightGBM] [Info] Number of positive: 2855, number of negative: 4009\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000651 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.415938 -> initscore=-0.339470\n",
      "[LightGBM] [Info] Start training from score -0.339470\n",
      "[LightGBM] [Info] Number of positive: 171, number of negative: 6693\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000661 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.024913 -> initscore=-3.667154\n",
      "[LightGBM] [Info] Start training from score -3.667154\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 118, number of negative: 6746\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000644 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.017191 -> initscore=-4.046020\n",
      "[LightGBM] [Info] Start training from score -4.046020\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 536, number of negative: 6328\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000632 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.078089 -> initscore=-2.468605\n",
      "[LightGBM] [Info] Start training from score -2.468605\n",
      "[LightGBM] [Info] Number of positive: 568, number of negative: 6296\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000613 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.082751 -> initscore=-2.405548\n",
      "[LightGBM] [Info] Start training from score -2.405548\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 57, number of negative: 6807\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000564 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.008304 -> initscore=-4.782656\n",
      "[LightGBM] [Info] Start training from score -4.782656\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 1299, number of negative: 5565\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000699 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.189248 -> initscore=-1.454902\n",
      "[LightGBM] [Info] Start training from score -1.454902\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 75, number of negative: 6789\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000559 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010927 -> initscore=-4.505571\n",
      "[LightGBM] [Info] Start training from score -4.505571\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 6699\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000617 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.024038 -> initscore=-3.703768\n",
      "[LightGBM] [Info] Start training from score -3.703768\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 380, number of negative: 6484\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000634 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.055361 -> initscore=-2.836922\n",
      "[LightGBM] [Info] Start training from score -2.836922\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 659, number of negative: 6205\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000712 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.096008 -> initscore=-2.242387\n",
      "[LightGBM] [Info] Start training from score -2.242387\n",
      "[LightGBM] [Info] Number of positive: 2886, number of negative: 3978\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000634 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.420455 -> initscore=-0.320908\n",
      "[LightGBM] [Info] Start training from score -0.320908\n",
      "[LightGBM] [Info] Number of positive: 163, number of negative: 6701\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000619 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.023747 -> initscore=-3.716262\n",
      "[LightGBM] [Info] Start training from score -3.716262\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 125, number of negative: 6739\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000670 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.018211 -> initscore=-3.987353\n",
      "[LightGBM] [Info] Start training from score -3.987353\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 486, number of negative: 6378\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000631 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070804 -> initscore=-2.574401\n",
      "[LightGBM] [Info] Start training from score -2.574401\n",
      "[LightGBM] [Info] Number of positive: 555, number of negative: 6309\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000661 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080857 -> initscore=-2.430764\n",
      "[LightGBM] [Info] Start training from score -2.430764\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 62, number of negative: 6802\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000517 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.009033 -> initscore=-4.697838\n",
      "[LightGBM] [Info] Start training from score -4.697838\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 1308, number of negative: 5556\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000589 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.190559 -> initscore=-1.446379\n",
      "[LightGBM] [Info] Start training from score -1.446379\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 655, number of negative: 6209\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000753 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2969\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.095425 -> initscore=-2.249120\n",
      "[LightGBM] [Info] Start training from score -2.249120\n",
      "[LightGBM] [Info] Number of positive: 2232, number of negative: 4632\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000585 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2969\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.325175 -> initscore=-0.730091\n",
      "[LightGBM] [Info] Start training from score -0.730091\n",
      "[LightGBM] [Info] Number of positive: 299, number of negative: 6565\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000673 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2969\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.043561 -> initscore=-3.089064\n",
      "[LightGBM] [Info] Start training from score -3.089064\n",
      "[LightGBM] [Info] Number of positive: 3678, number of negative: 3186\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000681 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2969\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.535839 -> initscore=0.143603\n",
      "[LightGBM] [Info] Start training from score 0.143603\n",
      "[LightGBM] [Info] Number of positive: 709, number of negative: 6155\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000701 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.103293 -> initscore=-2.161165\n",
      "[LightGBM] [Info] Start training from score -2.161165\n",
      "[LightGBM] [Info] Number of positive: 2246, number of negative: 4618\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000680 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.327214 -> initscore=-0.720811\n",
      "[LightGBM] [Info] Start training from score -0.720811\n",
      "[LightGBM] [Info] Number of positive: 281, number of negative: 6583\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000650 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040938 -> initscore=-3.153891\n",
      "[LightGBM] [Info] Start training from score -3.153891\n",
      "[LightGBM] [Info] Number of positive: 3628, number of negative: 3236\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000674 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.528555 -> initscore=0.114344\n",
      "[LightGBM] [Info] Start training from score 0.114344\n",
      "[LightGBM] [Info] Number of positive: 696, number of negative: 6168\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000690 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101399 -> initscore=-2.181780\n",
      "[LightGBM] [Info] Start training from score -2.181780\n",
      "[LightGBM] [Info] Number of positive: 2238, number of negative: 4626\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000703 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.326049 -> initscore=-0.726110\n",
      "[LightGBM] [Info] Start training from score -0.726110\n",
      "[LightGBM] [Info] Number of positive: 294, number of negative: 6570\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000560 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.042832 -> initscore=-3.106689\n",
      "[LightGBM] [Info] Start training from score -3.106689\n",
      "[LightGBM] [Info] Number of positive: 3636, number of negative: 3228\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000590 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.529720 -> initscore=0.119021\n",
      "[LightGBM] [Info] Start training from score 0.119021\n",
      "LightGBM Accuracy for 'type': 0.954368932038835\n",
      "LightGBM Accuracy for 'cvss_score' : 0.675242718446602\n",
      "K-Fold mean F1 (type): 0.9356520477917837\n",
      "K-Fold std  F1 (type): 0.007680765658042548\n",
      "K-Fold mean F1 (cvss_score): 0.555971733020272\n",
      "K-Fold std  F1 (cvss_score): 0.012909693017609095\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.89      0.94        28\n",
      "         1.0       1.00      0.96      0.98        50\n",
      "         2.0       0.85      0.95      0.90       103\n",
      "         3.0       0.96      0.93      0.94       200\n",
      "         4.0       0.95      0.96      0.96       834\n",
      "         5.0       0.95      0.93      0.94        41\n",
      "         6.0       0.82      0.86      0.84        36\n",
      "         7.0       0.93      0.88      0.90       155\n",
      "         8.0       0.99      0.99      0.99       176\n",
      "         9.0       0.93      1.00      0.97        14\n",
      "        10.0       0.98      0.98      0.98       423\n",
      "\n",
      "    accuracy                           0.95      2060\n",
      "   macro avg       0.94      0.94      0.94      2060\n",
      "weighted avg       0.96      0.95      0.95      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      0.27      0.36       234\n",
      "         1.0       0.60      0.52      0.56       637\n",
      "         2.0       0.88      0.41      0.56        87\n",
      "         3.0       0.71      0.87      0.78      1102\n",
      "\n",
      "    accuracy                           0.68      2060\n",
      "   macro avg       0.68      0.52      0.57      2060\n",
      "weighted avg       0.66      0.68      0.66      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "lgbm = OneVsRestClassifier(LGBMClassifier(random_state=42))\n",
    "multi_lgbm = MultiOutputClassifier(lgbm)\n",
    "\n",
    "multi_lgbm.fit(x_train, y_train)\n",
    "y_pred = multi_lgbm.predict(x_test)\n",
    "\n",
    "lgbm_accuracy_type = accuracy_score(y_test['type'], y_pred[:,0])\n",
    "lgbm_accuracy_cvss_score  = accuracy_score(y_test['cvss_score'], y_pred[:,1])\n",
    "\n",
    "lgbm_scores_type = cross_val_score(lgbm, x, y['type'], cv=kf, scoring='f1_macro')\n",
    "lgbm_scores_cvss_score  = cross_val_score(lgbm, x, y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"LightGBM Accuracy for 'type':\", lgbm_accuracy_type)\n",
    "print(\"LightGBM Accuracy for 'cvss_score' :\", lgbm_accuracy_cvss_score)\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", lgbm_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", lgbm_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", lgbm_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", lgbm_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca1207b",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d33a307a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Accuracy for 'type': 0.9529126213592233\n",
      "Bagging Accuracy for 'cvss_score' : 0.6747572815533981\n",
      "K-Fold mean F1 (type): 0.9299426939089429\n",
      "K-Fold std  F1 (type): 0.00604156191420799\n",
      "K-Fold mean F1 (cvss_score): 0.5425111091228983\n",
      "K-Fold std  F1 (cvss_score): 0.005719605961588248\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.82      0.90        28\n",
      "         1.0       1.00      0.94      0.97        50\n",
      "         2.0       0.86      0.95      0.90       103\n",
      "         3.0       0.98      0.93      0.95       200\n",
      "         4.0       0.95      0.96      0.96       834\n",
      "         5.0       0.95      0.93      0.94        41\n",
      "         6.0       0.78      0.89      0.83        36\n",
      "         7.0       0.92      0.88      0.90       155\n",
      "         8.0       0.99      0.99      0.99       176\n",
      "         9.0       0.93      1.00      0.97        14\n",
      "        10.0       0.97      0.98      0.98       423\n",
      "\n",
      "    accuracy                           0.95      2060\n",
      "   macro avg       0.94      0.93      0.93      2060\n",
      "weighted avg       0.95      0.95      0.95      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.27      0.36       234\n",
      "         1.0       0.60      0.55      0.57       637\n",
      "         2.0       0.73      0.40      0.52        87\n",
      "         3.0       0.72      0.86      0.78      1102\n",
      "\n",
      "    accuracy                           0.67      2060\n",
      "   macro avg       0.65      0.52      0.56      2060\n",
      "weighted avg       0.66      0.67      0.66      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bagging = OneVsRestClassifier(BaggingClassifier(random_state=42))\n",
    "multi_bag = MultiOutputClassifier(bagging)\n",
    "\n",
    "multi_bag.fit(x_train, y_train)\n",
    "y_pred = multi_bag.predict(x_test)\n",
    "\n",
    "bag_accuracy_type = accuracy_score(y_test['type'], y_pred[:,0])\n",
    "bag_accuracy_cvss_score  = accuracy_score(y_test['cvss_score'], y_pred[:,1])\n",
    "\n",
    "bag_scores_type = cross_val_score(bagging, x, y['type'], cv=kf, scoring='f1_macro')\n",
    "bag_scores_cvss_score  = cross_val_score(bagging, x, y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"Bagging Accuracy for 'type':\", bag_accuracy_type)\n",
    "print(\"Bagging Accuracy for 'cvss_score' :\", bag_accuracy_cvss_score)\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", bag_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", bag_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", bag_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", bag_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4360340",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04f11b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Accuracy for 'type': 0.8495145631067961\n",
      "SVC Accuracy for 'cvss_score' : 0.6330097087378641\n",
      "K-Fold mean F1 (type): 0.7522140407362387\n",
      "K-Fold std  F1 (type): 0.005939275133587859\n",
      "K-Fold mean F1 (cvss_score): 0.35284066989479984\n",
      "K-Fold std  F1 (cvss_score): 0.0032606218597976996\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.07      0.13        28\n",
      "         1.0       0.95      0.78      0.86        50\n",
      "         2.0       0.77      0.95      0.85       103\n",
      "         3.0       0.62      0.62      0.62       200\n",
      "         4.0       0.84      0.88      0.86       834\n",
      "         5.0       0.93      0.68      0.79        41\n",
      "         6.0       0.58      0.53      0.55        36\n",
      "         7.0       0.86      0.77      0.82       155\n",
      "         8.0       1.00      0.96      0.98       176\n",
      "         9.0       0.85      0.79      0.81        14\n",
      "        10.0       0.95      0.95      0.95       423\n",
      "\n",
      "    accuracy                           0.85      2060\n",
      "   macro avg       0.82      0.73      0.75      2060\n",
      "weighted avg       0.85      0.85      0.84      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.11      0.19       234\n",
      "         1.0       0.58      0.44      0.50       637\n",
      "         2.0       0.00      0.00      0.00        87\n",
      "         3.0       0.65      0.91      0.76      1102\n",
      "\n",
      "    accuracy                           0.63      2060\n",
      "   macro avg       0.46      0.36      0.36      2060\n",
      "weighted avg       0.59      0.63      0.58      2060\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(kernel='rbf', C=5, decision_function_shape='ovr')\n",
    "multi_svc = MultiOutputClassifier(svc)\n",
    "\n",
    "multi_svc.fit(x_train, y_train)\n",
    "y_pred = multi_svc.predict(x_test)\n",
    "\n",
    "svc_accuracy_type = accuracy_score(y_test['type'], y_pred[:,0])\n",
    "svc_accuracy_cvss_score  = accuracy_score(y_test['cvss_score'], y_pred[:,1])\n",
    "\n",
    "svc_scores_type = cross_val_score(svc, x, y['type'], cv=kf, scoring='f1_macro')\n",
    "svc_scores_cvss_score  = cross_val_score(svc, x, y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"SVC Accuracy for 'type':\", svc_accuracy_type)\n",
    "print(\"SVC Accuracy for 'cvss_score' :\", svc_accuracy_cvss_score)\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", svc_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", svc_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", svc_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", svc_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883c515a",
   "metadata": {},
   "source": [
    "# Hard Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdb372c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Voting Accuracy for type: 0.9092233009708738\n",
      "Hard Voting Accuracy for cvss_score : 0.6849514563106797\n",
      "K-fold F1 mean (type): 0.8535412937991548\n",
      "K-fold F1 std  (type): 0.00542323256362824\n",
      "K-fold F1 mean (cvss_score) : 0.546800250140797\n",
      "K-fold F1 std  (cvss_score) : 0.004329239698999266\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.29      0.43        28\n",
      "         1.0       0.98      0.90      0.94        50\n",
      "         2.0       0.82      0.95      0.88       103\n",
      "         3.0       0.82      0.83      0.83       200\n",
      "         4.0       0.90      0.95      0.93       834\n",
      "         5.0       0.90      0.68      0.78        41\n",
      "         6.0       0.68      0.53      0.59        36\n",
      "         7.0       0.90      0.81      0.85       155\n",
      "         8.0       0.99      0.98      0.99       176\n",
      "         9.0       1.00      1.00      1.00        14\n",
      "        10.0       0.97      0.95      0.96       423\n",
      "\n",
      "    accuracy                           0.91      2060\n",
      "   macro avg       0.90      0.81      0.83      2060\n",
      "weighted avg       0.91      0.91      0.91      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "model1 = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "model2 = ExtraTreesClassifier(n_estimators=200, random_state=42)\n",
    "model3 = LogisticRegression(max_iter=500)\n",
    "\n",
    "voting_hard = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', model1),\n",
    "        ('et', model2),\n",
    "        ('lr', model3)\n",
    "    ],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "multi_voting_hard = MultiOutputClassifier(voting_hard)\n",
    "\n",
    "multi_voting_hard.fit(x_train, y_train)\n",
    "y_pred = multi_voting_hard.predict(x_test)\n",
    "\n",
    "hard_acc_type = accuracy_score(y_test[\"type\"], y_pred[:, 0])\n",
    "hard_acc_cvss_score  = accuracy_score(y_test[\"cvss_score\"],  y_pred[:, 1])\n",
    "\n",
    "\n",
    "hard_scores_type = cross_val_score(voting_hard, x, y['type'], cv=kf, scoring='f1_macro')\n",
    "hard_scores_cvss_score  = cross_val_score(voting_hard, x, y['cvss_score'],  cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"Hard Voting Accuracy for type:\", hard_acc_type)\n",
    "print(\"Hard Voting Accuracy for cvss_score :\", hard_acc_cvss_score)\n",
    "\n",
    "print(\"K-fold F1 mean (type):\", hard_scores_type.mean())\n",
    "print(\"K-fold F1 std  (type):\", hard_scores_type.std())\n",
    "\n",
    "print(\"K-fold F1 mean (cvss_score) :\", hard_scores_cvss_score.mean())\n",
    "print(\"K-fold F1 std  (cvss_score) :\", hard_scores_cvss_score.std())\n",
    "\n",
    "print(classification_report(y_test[\"type\"], y_pred[:, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e81c827",
   "metadata": {},
   "source": [
    "# Soft Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af6b34db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Voting Accuracy for type: 0.9063106796116505\n",
      "Hard Voting Accuracy for cvss_score : 0.6786407766990291\n",
      "K-fold F1 mean (type): 0.8401265096947981\n",
      "K-fold F1 std  (type): 0.004807786214563205\n",
      "K-fold F1 mean (cvss_score) : 0.53279587273932\n",
      "K-fold F1 std  (cvss_score) : 0.00474278913678708\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.25      0.40        28\n",
      "         1.0       0.98      0.88      0.93        50\n",
      "         2.0       0.83      0.97      0.89       103\n",
      "         3.0       0.82      0.83      0.83       200\n",
      "         4.0       0.89      0.95      0.92       834\n",
      "         5.0       0.90      0.68      0.78        41\n",
      "         6.0       0.68      0.53      0.59        36\n",
      "         7.0       0.89      0.79      0.84       155\n",
      "         8.0       1.00      0.98      0.99       176\n",
      "         9.0       1.00      0.86      0.92        14\n",
      "        10.0       0.97      0.95      0.96       423\n",
      "\n",
      "    accuracy                           0.91      2060\n",
      "   macro avg       0.91      0.79      0.82      2060\n",
      "weighted avg       0.91      0.91      0.90      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model1 = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "model2 = ExtraTreesClassifier(n_estimators=200, random_state=42)\n",
    "model3 = LogisticRegression(max_iter=500)\n",
    "\n",
    "voting_hard = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', model1),\n",
    "        ('et', model2),\n",
    "        ('lr', model3)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "multi_voting_hard = MultiOutputClassifier(voting_hard)\n",
    "\n",
    "multi_voting_hard.fit(x_train, y_train)\n",
    "y_pred = multi_voting_hard.predict(x_test)\n",
    "\n",
    "soft_acc_type = accuracy_score(y_test[\"type\"], y_pred[:, 0])\n",
    "soft_acc_cvss_score  = accuracy_score(y_test[\"cvss_score\"],  y_pred[:, 1])\n",
    "\n",
    "soft_scores_type = cross_val_score(voting_hard, x, y['type'], cv=kf, scoring='f1_macro')\n",
    "soft_scores_cvss_score  = cross_val_score(voting_hard, x, y['cvss_score'],  cv=kf, scoring='f1_macro')\n",
    "\n",
    "\n",
    "print(\"Hard Voting Accuracy for type:\", soft_acc_type)\n",
    "print(\"Hard Voting Accuracy for cvss_score :\", soft_acc_cvss_score)\n",
    "\n",
    "print(\"K-fold F1 mean (type):\", soft_scores_type.mean())\n",
    "print(\"K-fold F1 std  (type):\", soft_scores_type.std())\n",
    "\n",
    "print(\"K-fold F1 mean (cvss_score) :\", soft_scores_cvss_score.mean())\n",
    "print(\"K-fold F1 std  (cvss_score) :\", soft_scores_cvss_score.std())\n",
    "\n",
    "print(classification_report(y_test[\"type\"], y_pred[:, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6851e405",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b83aa30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Accuracy for type: 0.9276699029126214\n",
      "Stacking Accuracy for cvss_score : 0.6936893203883495\n",
      "K-fold F1 mean (type): 0.8767479622209979\n",
      "K-fold F1 std  (type): 0.008390731109801819\n",
      "K-fold F1 mean (cvss_score) : 0.5663912915765845\n",
      "K-fold F1 std  (cvss_score) : 0.017561396746994373\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.43      0.56        28\n",
      "         1.0       1.00      0.92      0.96        50\n",
      "         2.0       0.85      0.93      0.89       103\n",
      "         3.0       0.83      0.90      0.86       200\n",
      "         4.0       0.94      0.96      0.95       834\n",
      "         5.0       0.88      0.71      0.78        41\n",
      "         6.0       0.72      0.58      0.65        36\n",
      "         7.0       0.90      0.83      0.86       155\n",
      "         8.0       1.00      0.98      0.99       176\n",
      "         9.0       0.88      1.00      0.93        14\n",
      "        10.0       0.97      0.97      0.97       423\n",
      "\n",
      "    accuracy                           0.93      2060\n",
      "   macro avg       0.89      0.84      0.85      2060\n",
      "weighted avg       0.93      0.93      0.93      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "base1 = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "base2 = ExtraTreesClassifier(n_estimators=200, random_state=42)\n",
    "base3 = LogisticRegression(max_iter=500)\n",
    "\n",
    "stacking = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', base1),\n",
    "        ('et', base2),\n",
    "        ('lr', base3)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(max_iter=500)\n",
    ")\n",
    "\n",
    "multi_stacking = MultiOutputClassifier(stacking)\n",
    "\n",
    "multi_stacking.fit(x_train, y_train)\n",
    "y_pred = multi_stacking.predict(x_test)\n",
    "\n",
    "stacking_acc_type = accuracy_score(y_test[\"type\"], y_pred[:, 0])\n",
    "stacking_acc_cvss_score  = accuracy_score(y_test[\"cvss_score\"],  y_pred[:, 1])\n",
    "\n",
    "stacking_scores_type = cross_val_score(stacking, x, y['type'], cv=kf, scoring='f1_macro')\n",
    "stacking_scores_cvss_score  = cross_val_score(stacking, x, y['cvss_score'],  cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"Stacking Accuracy for type:\", stacking_acc_type)\n",
    "print(\"Stacking Accuracy for cvss_score :\", stacking_acc_cvss_score)\n",
    "\n",
    "print(\"K-fold F1 mean (type):\", stacking_scores_type.mean())\n",
    "print(\"K-fold F1 std  (type):\", stacking_scores_type.std())\n",
    "\n",
    "print(\"K-fold F1 mean (cvss_score) :\", stacking_scores_cvss_score.mean())\n",
    "print(\"K-fold F1 std  (cvss_score) :\", stacking_scores_cvss_score.std())\n",
    "\n",
    "print(classification_report(y_test[\"type\"], y_pred[:, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fa245d",
   "metadata": {},
   "source": [
    "# Bagged KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be67e2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging OVR KNN Accuracy for 'type': 0.8121359223300971\n",
      "Bagging OVR KNN Accuracy for 'cvss_score': 0.6033980582524272\n",
      "K-Fold mean F1 (type): 0.6873676833719826\n",
      "K-Fold std  F1 (type): 0.008717440573823714\n",
      "K-Fold mean F1 (cvss_score): 0.4635969724423244\n",
      "K-Fold std  F1 (cvss_score): 0.005687439449563473\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.43      0.21      0.29        28\n",
      "         1.0       0.91      0.84      0.88        50\n",
      "         2.0       0.78      0.78      0.78       103\n",
      "         3.0       0.57      0.66      0.61       200\n",
      "         4.0       0.81      0.85      0.83       834\n",
      "         5.0       0.67      0.63      0.65        41\n",
      "         6.0       0.58      0.31      0.40        36\n",
      "         7.0       0.77      0.72      0.74       155\n",
      "         8.0       0.97      0.97      0.97       176\n",
      "         9.0       0.92      0.79      0.85        14\n",
      "        10.0       0.92      0.90      0.91       423\n",
      "\n",
      "    accuracy                           0.81      2060\n",
      "   macro avg       0.76      0.69      0.72      2060\n",
      "weighted avg       0.81      0.81      0.81      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.32      0.35       234\n",
      "         1.0       0.55      0.54      0.54       637\n",
      "         2.0       0.30      0.36      0.33        87\n",
      "         3.0       0.70      0.72      0.71      1102\n",
      "\n",
      "    accuracy                           0.60      2060\n",
      "   macro avg       0.48      0.48      0.48      2060\n",
      "weighted avg       0.60      0.60      0.60      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bag_knn = BaggingClassifier(\n",
    "    estimator=OneVsRestClassifier(KNeighborsClassifier(n_neighbors=1)),\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "multi_bag_knn = MultiOutputClassifier(bag_knn)\n",
    "\n",
    "multi_bag_knn.fit(x_train, y_train)\n",
    "y_pred = multi_bag_knn.predict(x_test)\n",
    "\n",
    "bag_knn_accuracy_type = accuracy_score(y_test['type'], y_pred[:, 0])\n",
    "bag_knn_accuracy_cvss_score = accuracy_score(y_test['cvss_score'], y_pred[:, 1])\n",
    "\n",
    "bag_knn_scores_type = cross_val_score(bag_knn, x_train, y_train['type'], cv=kf, scoring='f1_macro')\n",
    "bag_knn_scores_cvss_score = cross_val_score(bag_knn, x_train, y_train['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"Bagging OVR KNN Accuracy for 'type':\", bag_knn_accuracy_type)\n",
    "print(\"Bagging OVR KNN Accuracy for 'cvss_score':\", bag_knn_accuracy_cvss_score)\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", bag_knn_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", bag_knn_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", bag_knn_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", bag_knn_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1de10c",
   "metadata": {},
   "source": [
    "# Bagged DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f73f7583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging OVR DecisionTree Accuracy for 'type': 0.9533980582524272\n",
      "Bagging OVR DecisionTree Accuracy for 'cvss_score': 0.6820388349514563\n",
      "K-Fold mean F1 (type): 0.9384341702780987\n",
      "K-Fold std  F1 (type): 0.005387842230331369\n",
      "K-Fold mean F1 (cvss_score): 0.546868347330393\n",
      "K-Fold std  F1 (cvss_score): 0.01519763735774914\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.89      0.89        28\n",
      "         1.0       1.00      0.92      0.96        50\n",
      "         2.0       0.87      0.94      0.91       103\n",
      "         3.0       0.97      0.93      0.95       200\n",
      "         4.0       0.95      0.96      0.96       834\n",
      "         5.0       0.95      0.93      0.94        41\n",
      "         6.0       0.78      0.89      0.83        36\n",
      "         7.0       0.92      0.88      0.90       155\n",
      "         8.0       0.99      0.99      0.99       176\n",
      "         9.0       1.00      1.00      1.00        14\n",
      "        10.0       0.98      0.98      0.98       423\n",
      "\n",
      "    accuracy                           0.95      2060\n",
      "   macro avg       0.94      0.94      0.94      2060\n",
      "weighted avg       0.95      0.95      0.95      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.28      0.37       234\n",
      "         1.0       0.61      0.55      0.58       637\n",
      "         2.0       0.82      0.41      0.55        87\n",
      "         3.0       0.72      0.86      0.79      1102\n",
      "\n",
      "    accuracy                           0.68      2060\n",
      "   macro avg       0.67      0.53      0.57      2060\n",
      "weighted avg       0.67      0.68      0.66      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bag_dt = BaggingClassifier(\n",
    "    estimator=OneVsRestClassifier(DecisionTreeClassifier()),\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "multi_bag_dt = MultiOutputClassifier(bag_dt)\n",
    "\n",
    "multi_bag_dt.fit(x_train, y_train)\n",
    "y_pred = multi_bag_dt.predict(x_test)\n",
    "\n",
    "bag_dt_accuracy_type = accuracy_score(y_test['type'], y_pred[:, 0])\n",
    "bag_dt_accuracy_cvss_score = accuracy_score(y_test['cvss_score'], y_pred[:, 1])\n",
    "\n",
    "bag_dt_scores_type = cross_val_score(bag_dt, x_train, y_train['type'], cv=kf, scoring='f1_macro')\n",
    "bag_dt_scores_cvss_score = cross_val_score(bag_dt, x_train, y_train['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"Bagging OVR DecisionTree Accuracy for 'type':\", bag_dt_accuracy_type)\n",
    "print(\"Bagging OVR DecisionTree Accuracy for 'cvss_score':\", bag_dt_accuracy_cvss_score)\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", bag_dt_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", bag_dt_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", bag_dt_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", bag_dt_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "788ab8d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                  OVR Comparison                                                   </span>\n",
       "\n",
       "<span style=\"font-weight: bold\"> Algorithm          </span><span style=\"font-weight: bold\"> Type Acc </span><span style=\"font-weight: bold\"> K-Fold Mean </span><span style=\"font-weight: bold\"> K-Fold Std </span><span style=\"font-weight: bold\"> cvss_score Acc </span><span style=\"font-weight: bold\"> K-Fold Mean </span><span style=\"font-weight: bold\"> K-Fold Std </span><span style=\"font-weight: bold\"> Combined </span>\n",
       "\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Bagged DT</span>           <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.95</span>      <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.94</span>         <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.01</span>        <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.68</span>            <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.55</span>         <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.02</span>            <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.82</span> \n",
       "\n",
       " HistGradientBoost  0.95      0.93         0.00        0.68            0.55         0.01            0.82 \n",
       "\n",
       " LightGBM            0.95      0.94         0.01        0.68            0.56         0.01            0.81 \n",
       "\n",
       " Bagging             0.95      0.93         0.01        0.67            0.54         0.01            0.81 \n",
       "\n",
       " Stacking            0.93      0.88         0.01        0.69            0.57         0.02            0.81 \n",
       "\n",
       " RandomForest        0.93      0.86         0.01        0.68            0.56         0.01            0.80 \n",
       "\n",
       " GradientBoosting    0.94      0.93         0.01        0.65            0.51         0.01            0.80 \n",
       "\n",
       " Hard Voting         0.91      0.85         0.01        0.68            0.55         0.00            0.80 \n",
       "\n",
       " ExtraTrees          0.91      0.85         0.01        0.69            0.56         0.01            0.80 \n",
       "\n",
       " Soft Voting         0.91      0.84         0.00        0.68            0.53         0.00            0.79 \n",
       "\n",
       " DecisionTree        0.93      0.90         0.00        0.63            0.46         0.01            0.78 \n",
       "\n",
       " AdaBoost            0.93      0.90         0.01        0.60            0.36         0.00            0.77 \n",
       "\n",
       " SVM                 0.85      0.75         0.01        0.63            0.35         0.00            0.74 \n",
       "\n",
       " Bagged KNN          0.81      0.69         0.01        0.60            0.46         0.01            0.71 \n",
       "\n",
       " KNN                 0.81      0.70         0.01        0.60            0.48         0.00            0.71 \n",
       "\n",
       " <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">LogisticRegression</span>  <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.79</span>      <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.66</span>         <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.02</span>        <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.59</span>            <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.29</span>         <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.00</span>            <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.69</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                  OVR Comparison                                                   \u001b[0m\n",
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mAlgorithm         \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mType Acc\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mK-Fold Mean\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mK-Fold Std\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mcvss_score Acc\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mK-Fold Mean\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mK-Fold Std\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mCombined\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " \u001b[1;32mBagged DT\u001b[0m           \u001b[1;32m0.95\u001b[0m      \u001b[1;32m0.94\u001b[0m         \u001b[1;32m0.01\u001b[0m        \u001b[1;32m0.68\u001b[0m            \u001b[1;32m0.55\u001b[0m         \u001b[1;32m0.02\u001b[0m            \u001b[1;32m0.82\u001b[0m \n",
       "\n",
       " HistGradientBoost  0.95      0.93         0.00        0.68            0.55         0.01            0.82 \n",
       "\n",
       " LightGBM            0.95      0.94         0.01        0.68            0.56         0.01            0.81 \n",
       "\n",
       " Bagging             0.95      0.93         0.01        0.67            0.54         0.01            0.81 \n",
       "\n",
       " Stacking            0.93      0.88         0.01        0.69            0.57         0.02            0.81 \n",
       "\n",
       " RandomForest        0.93      0.86         0.01        0.68            0.56         0.01            0.80 \n",
       "\n",
       " GradientBoosting    0.94      0.93         0.01        0.65            0.51         0.01            0.80 \n",
       "\n",
       " Hard Voting         0.91      0.85         0.01        0.68            0.55         0.00            0.80 \n",
       "\n",
       " ExtraTrees          0.91      0.85         0.01        0.69            0.56         0.01            0.80 \n",
       "\n",
       " Soft Voting         0.91      0.84         0.00        0.68            0.53         0.00            0.79 \n",
       "\n",
       " DecisionTree        0.93      0.90         0.00        0.63            0.46         0.01            0.78 \n",
       "\n",
       " AdaBoost            0.93      0.90         0.01        0.60            0.36         0.00            0.77 \n",
       "\n",
       " SVM                 0.85      0.75         0.01        0.63            0.35         0.00            0.74 \n",
       "\n",
       " Bagged KNN          0.81      0.69         0.01        0.60            0.46         0.01            0.71 \n",
       "\n",
       " KNN                 0.81      0.70         0.01        0.60            0.48         0.00            0.71 \n",
       "\n",
       " \u001b[1;31mLogisticRegression\u001b[0m  \u001b[1;31m0.79\u001b[0m      \u001b[1;31m0.66\u001b[0m         \u001b[1;31m0.02\u001b[0m        \u001b[1;31m0.59\u001b[0m            \u001b[1;31m0.29\u001b[0m         \u001b[1;31m0.00\u001b[0m            \u001b[1;31m0.69\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.table import Table\n",
    "from rich.console import Console\n",
    "\n",
    "console = Console()\n",
    "\n",
    "results = [\n",
    "    ['LogisticRegression', lr_accuracy_type, lr_scores_type.mean(), lr_scores_type.std(), lr_accuracy_cvss_score, lr_scores_cvss_score.mean(), lr_scores_cvss_score.std()],\n",
    "    ['DecisionTree', dt_accuracy_type, dt_scores_type.mean(), dt_scores_type.std(), dt_accuracy_cvss_score, dt_scores_cvss_score.mean(), dt_scores_cvss_score.std()],\n",
    "    ['RandomForest', rf_accuracy_type, rf_scores_type.mean(), rf_scores_type.std(), rf_accuracy_cvss_score, rf_scores_cvss_score.mean(), rf_scores_cvss_score.std()],\n",
    "    ['ExtraTrees', et_accuracy_type, et_scores_type.mean(), et_scores_type.std(), et_accuracy_cvss_score, et_scores_cvss_score.mean(), et_scores_cvss_score.std()],\n",
    "    ['GradientBoosting', gb_accuracy_type, gb_scores_type.mean(), gb_scores_type.std(), gb_accuracy_cvss_score, gb_scores_cvss_score.mean(), gb_scores_cvss_score.std()],\n",
    "    ['HistGradientBoosting', hgb_accuracy_type, hgb_scores_type.mean(), hgb_scores_type.std(), hgb_accuracy_cvss_score, hgb_scores_cvss_score.mean(), hgb_scores_cvss_score.std()],\n",
    "    ['KNN', knn_accuracy_type, knn_scores_type.mean(), knn_scores_type.std(), knn_accuracy_cvss_score, knn_scores_cvss_score.mean(), knn_scores_cvss_score.std()],\n",
    "    ['AdaBoost', ab_accuracy_type, ab_scores_type.mean(), ab_scores_type.std(), ab_accuracy_cvss_score, ab_scores_cvss_score.mean(), ab_scores_cvss_score.std()],\n",
    "    ['LightGBM', lgbm_accuracy_type, lgbm_scores_type.mean(), lgbm_scores_type.std(), lgbm_accuracy_cvss_score, lgbm_scores_cvss_score.mean(), lgbm_scores_cvss_score.std()],\n",
    "    ['Bagging', bag_accuracy_type, bag_scores_type.mean(), bag_scores_type.std(), bag_accuracy_cvss_score, bag_scores_cvss_score.mean(), bag_scores_cvss_score.std()],\n",
    "    ['Hard Voting', hard_acc_type, hard_scores_type.mean(), hard_scores_type.std(), hard_acc_cvss_score, hard_scores_cvss_score.mean(), hard_scores_cvss_score.std()],\n",
    "    ['Soft Voting', soft_acc_type, soft_scores_type.mean(), soft_scores_type.std(), soft_acc_cvss_score, soft_scores_cvss_score.mean(), soft_scores_cvss_score.std()],\n",
    "    ['Stacking', stacking_acc_type, stacking_scores_type.mean(), stacking_scores_type.std(), stacking_acc_cvss_score, stacking_scores_cvss_score.mean(), stacking_scores_cvss_score.std()],\n",
    "    ['SVM', svc_accuracy_type, svc_scores_type.mean(), svc_scores_type.std(), svc_accuracy_cvss_score, svc_scores_cvss_score.mean(), svc_scores_cvss_score.std()],\n",
    "    ['Bagged KNN', bag_knn_accuracy_type, bag_knn_scores_type.mean(), bag_knn_scores_type.std(), bag_knn_accuracy_cvss_score, bag_knn_scores_cvss_score.mean(), bag_knn_scores_cvss_score.std()],\n",
    "    ['Bagged DT', bag_dt_accuracy_type, bag_dt_scores_type.mean(), bag_dt_scores_type.std(), bag_dt_accuracy_cvss_score, bag_dt_scores_cvss_score.mean(), bag_dt_scores_cvss_score.std()],\n",
    "]\n",
    "\n",
    "for row in results:\n",
    "    type_acc = row[1]\n",
    "    cvss_score_acc = row[4]\n",
    "    combined = (type_acc + cvss_score_acc) / 2\n",
    "    row.append(combined)\n",
    "\n",
    "result_sorted = sorted(results, key=lambda i: i[-1], reverse=True)\n",
    "\n",
    "best_model = max(results, key=lambda x: x[-1])\n",
    "worst_model = min(results, key=lambda x: x[-1])\n",
    "\n",
    "table = Table(title=\"OVR Comparison\", show_lines=True)\n",
    "table.add_column(\"Algorithm\")\n",
    "table.add_column(\"Type Acc\")\n",
    "table.add_column(\"K-Fold Mean\")\n",
    "table.add_column(\"K-Fold Std\")\n",
    "table.add_column(\"cvss_score Acc\")\n",
    "table.add_column(\"K-Fold Mean\")\n",
    "table.add_column(\"K-Fold Std\")\n",
    "table.add_column(\"Combined\", justify=\"right\")\n",
    "\n",
    "for row in result_sorted:\n",
    "    algo, type_acc, kmean_type, kstd_type, cvss_score_acc, kmean_cvss_score, kstd_cvss_score, combined = row\n",
    "\n",
    "    if row == best_model:\n",
    "        table.add_row(\n",
    "            f\"[bold green]{algo}[/bold green]\",\n",
    "            f\"[bold green]{type_acc:.2f}[/bold green]\",\n",
    "            f\"[bold green]{kmean_type:.2f}[/bold green]\",\n",
    "            f\"[bold green]{kstd_type:.2f}[/bold green]\",\n",
    "            f\"[bold green]{cvss_score_acc:.2f}[/bold green]\",\n",
    "            f\"[bold green]{kmean_cvss_score:.2f}[/bold green]\",\n",
    "            f\"[bold green]{kstd_cvss_score:.2f}[/bold green]\",\n",
    "            f\"[bold green]{combined:.2f}[/bold green]\",\n",
    "        )\n",
    "    elif row == worst_model:\n",
    "        table.add_row(\n",
    "            f\"[bold red]{algo}[/bold red]\",\n",
    "            f\"[bold red]{type_acc:.2f}[/bold red]\",\n",
    "            f\"[bold red]{kmean_type:.2f}[/bold red]\",\n",
    "            f\"[bold red]{kstd_type:.2f}[/bold red]\",\n",
    "            f\"[bold red]{cvss_score_acc:.2f}[/bold red]\",\n",
    "            f\"[bold red]{kmean_cvss_score:.2f}[/bold red]\",\n",
    "            f\"[bold red]{kstd_cvss_score:.2f}[/bold red]\",\n",
    "            f\"[bold red]{combined:.2f}[/bold red]\",\n",
    "        )\n",
    "    else:\n",
    "        table.add_row(\n",
    "            algo, f\"{type_acc:.2f}\", f\"{kmean_type:.2f}\", f\"{kstd_type:.2f}\",\n",
    "            f\"{cvss_score_acc:.2f}\", f\"{kmean_cvss_score:.2f}\", f\"{kstd_cvss_score:.2f}\", f\"{combined:.2f}\"\n",
    "        )\n",
    "\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08c28177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                  OVR Comparison                                                   </span>\n",
       "\n",
       "<span style=\"font-weight: bold\"> Algorithm          </span><span style=\"font-weight: bold\"> Type Acc </span><span style=\"font-weight: bold\"> K-Fold Mean </span><span style=\"font-weight: bold\"> K-Fold Std </span><span style=\"font-weight: bold\"> cvss_score Acc </span><span style=\"font-weight: bold\"> K-Fold Mean </span><span style=\"font-weight: bold\"> K-Fold Std </span><span style=\"font-weight: bold\"> Combined </span>\n",
       "\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Bagged DT</span>           <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.95</span>      <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.94</span>         <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.01</span>        <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.68</span>            <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.55</span>         <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.02</span>            <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.82</span> \n",
       "\n",
       " HistGradientBoost  0.95      0.93         0.00        0.68            0.55         0.01            0.82 \n",
       "\n",
       " LightGBM            0.95      0.94         0.01        0.68            0.56         0.01            0.81 \n",
       "\n",
       " Bagging             0.95      0.93         0.01        0.67            0.54         0.01            0.81 \n",
       "\n",
       " Stacking            0.93      0.88         0.01        0.69            0.57         0.02            0.81 \n",
       "\n",
       " RandomForest        0.93      0.86         0.01        0.68            0.56         0.01            0.80 \n",
       "\n",
       " GradientBoosting    0.94      0.93         0.01        0.65            0.51         0.01            0.80 \n",
       "\n",
       " Hard Voting         0.91      0.85         0.01        0.68            0.55         0.00            0.80 \n",
       "\n",
       " ExtraTrees          0.91      0.85         0.01        0.69            0.56         0.01            0.80 \n",
       "\n",
       " Soft Voting         0.91      0.84         0.00        0.68            0.53         0.00            0.79 \n",
       "\n",
       " DecisionTree        0.93      0.90         0.00        0.63            0.46         0.01            0.78 \n",
       "\n",
       " AdaBoost            0.93      0.90         0.01        0.60            0.36         0.00            0.77 \n",
       "\n",
       " SVM                 0.85      0.75         0.01        0.63            0.35         0.00            0.74 \n",
       "\n",
       " Bagged KNN          0.81      0.69         0.01        0.60            0.46         0.01            0.71 \n",
       "\n",
       " KNN                 0.81      0.70         0.01        0.60            0.48         0.00            0.71 \n",
       "\n",
       " <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">LogisticRegression</span>  <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.79</span>      <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.66</span>         <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.02</span>        <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.59</span>            <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.29</span>         <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.00</span>            <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.69</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                  OVR Comparison                                                   \u001b[0m\n",
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mAlgorithm         \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mType Acc\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mK-Fold Mean\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mK-Fold Std\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mcvss_score Acc\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mK-Fold Mean\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mK-Fold Std\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mCombined\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " \u001b[1;32mBagged DT\u001b[0m           \u001b[1;32m0.95\u001b[0m      \u001b[1;32m0.94\u001b[0m         \u001b[1;32m0.01\u001b[0m        \u001b[1;32m0.68\u001b[0m            \u001b[1;32m0.55\u001b[0m         \u001b[1;32m0.02\u001b[0m            \u001b[1;32m0.82\u001b[0m \n",
       "\n",
       " HistGradientBoost  0.95      0.93         0.00        0.68            0.55         0.01            0.82 \n",
       "\n",
       " LightGBM            0.95      0.94         0.01        0.68            0.56         0.01            0.81 \n",
       "\n",
       " Bagging             0.95      0.93         0.01        0.67            0.54         0.01            0.81 \n",
       "\n",
       " Stacking            0.93      0.88         0.01        0.69            0.57         0.02            0.81 \n",
       "\n",
       " RandomForest        0.93      0.86         0.01        0.68            0.56         0.01            0.80 \n",
       "\n",
       " GradientBoosting    0.94      0.93         0.01        0.65            0.51         0.01            0.80 \n",
       "\n",
       " Hard Voting         0.91      0.85         0.01        0.68            0.55         0.00            0.80 \n",
       "\n",
       " ExtraTrees          0.91      0.85         0.01        0.69            0.56         0.01            0.80 \n",
       "\n",
       " Soft Voting         0.91      0.84         0.00        0.68            0.53         0.00            0.79 \n",
       "\n",
       " DecisionTree        0.93      0.90         0.00        0.63            0.46         0.01            0.78 \n",
       "\n",
       " AdaBoost            0.93      0.90         0.01        0.60            0.36         0.00            0.77 \n",
       "\n",
       " SVM                 0.85      0.75         0.01        0.63            0.35         0.00            0.74 \n",
       "\n",
       " Bagged KNN          0.81      0.69         0.01        0.60            0.46         0.01            0.71 \n",
       "\n",
       " KNN                 0.81      0.70         0.01        0.60            0.48         0.00            0.71 \n",
       "\n",
       " \u001b[1;31mLogisticRegression\u001b[0m  \u001b[1;31m0.79\u001b[0m      \u001b[1;31m0.66\u001b[0m         \u001b[1;31m0.02\u001b[0m        \u001b[1;31m0.59\u001b[0m            \u001b[1;31m0.29\u001b[0m         \u001b[1;31m0.00\u001b[0m            \u001b[1;31m0.69\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "temp_console = Console(record=True)\n",
    "temp_console.print(table)\n",
    "text = temp_console.export_text()\n",
    "with open('results/OVO_OVR_compare.txt', 'a', encoding='utf-8') as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143c488e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
