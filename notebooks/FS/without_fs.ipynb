{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cd7a06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e410eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\SML_Projects\\SML_CVE_type_cwe_predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a13754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/preprocessed/preprocessed_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cba8b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 102963 entries, 0 to 102962\n",
      "Data columns (total 29 columns):\n",
      " #   Column                      Non-Null Count   Dtype  \n",
      "---  ------                      --------------   -----  \n",
      " 0   cve_id                      102963 non-null  float64\n",
      " 1   description                 102963 non-null  float64\n",
      " 2   cvss_score                  102963 non-null  float64\n",
      " 3   cwe                         102963 non-null  float64\n",
      " 4   vendor                      102963 non-null  float64\n",
      " 5   product                     102963 non-null  float64\n",
      " 6   publish_date                102963 non-null  float64\n",
      " 7   type                        102963 non-null  float64\n",
      " 8   vendor_freq                 102963 non-null  float64\n",
      " 9   product_freq                102963 non-null  float64\n",
      " 10  desc_len                    102963 non-null  float64\n",
      " 11  desc_word_count             102963 non-null  float64\n",
      " 12  desc_num_count              102963 non-null  float64\n",
      " 13  desc_upper_ratio            102963 non-null  float64\n",
      " 14  desc_exclamation            102963 non-null  float64\n",
      " 15  desc_question               102963 non-null  float64\n",
      " 16  vendor_product_interaction  102963 non-null  float64\n",
      " 17  XSS_score                   102963 non-null  float64\n",
      " 18  SQLi_score                  102963 non-null  float64\n",
      " 19  RCE_score                   102963 non-null  float64\n",
      " 20  DoS_score                   102963 non-null  float64\n",
      " 21  CSRF_score                  102963 non-null  float64\n",
      " 22  AuthBypass_score            102963 non-null  float64\n",
      " 23  PrivEsc_score               102963 non-null  float64\n",
      " 24  PathTraversal_score         102963 non-null  float64\n",
      " 25  SSRF_score                  102963 non-null  float64\n",
      " 26  InfoDisclosure_score        102963 non-null  float64\n",
      " 27  Other_score                 102963 non-null  float64\n",
      " 28  cvss_keywords_score         102963 non-null  float64\n",
      "dtypes: float64(29)\n",
      "memory usage: 22.8 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fcc147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=0.1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2763fadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10296 entries, 0 to 10295\n",
      "Data columns (total 29 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   cve_id                      10296 non-null  float64\n",
      " 1   description                 10296 non-null  float64\n",
      " 2   cvss_score                  10296 non-null  float64\n",
      " 3   cwe                         10296 non-null  float64\n",
      " 4   vendor                      10296 non-null  float64\n",
      " 5   product                     10296 non-null  float64\n",
      " 6   publish_date                10296 non-null  float64\n",
      " 7   type                        10296 non-null  float64\n",
      " 8   vendor_freq                 10296 non-null  float64\n",
      " 9   product_freq                10296 non-null  float64\n",
      " 10  desc_len                    10296 non-null  float64\n",
      " 11  desc_word_count             10296 non-null  float64\n",
      " 12  desc_num_count              10296 non-null  float64\n",
      " 13  desc_upper_ratio            10296 non-null  float64\n",
      " 14  desc_exclamation            10296 non-null  float64\n",
      " 15  desc_question               10296 non-null  float64\n",
      " 16  vendor_product_interaction  10296 non-null  float64\n",
      " 17  XSS_score                   10296 non-null  float64\n",
      " 18  SQLi_score                  10296 non-null  float64\n",
      " 19  RCE_score                   10296 non-null  float64\n",
      " 20  DoS_score                   10296 non-null  float64\n",
      " 21  CSRF_score                  10296 non-null  float64\n",
      " 22  AuthBypass_score            10296 non-null  float64\n",
      " 23  PrivEsc_score               10296 non-null  float64\n",
      " 24  PathTraversal_score         10296 non-null  float64\n",
      " 25  SSRF_score                  10296 non-null  float64\n",
      " 26  InfoDisclosure_score        10296 non-null  float64\n",
      " 27  Other_score                 10296 non-null  float64\n",
      " 28  cvss_keywords_score         10296 non-null  float64\n",
      "dtypes: float64(29)\n",
      "memory usage: 2.3 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81b82ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, HistGradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddfa1566",
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_types = df['type'].value_counts()[df['type'].value_counts() < 3].index\n",
    "rare_cvss_scores  = df['cvss_score'].value_counts()[df['cvss_score'].value_counts() < 3].index\n",
    "\n",
    "df = df[~df['type'].isin(rare_types)]\n",
    "df = df[~df['cvss_score'].isin(rare_cvss_scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81885af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(['type', 'cvss_score'], axis=1)   \n",
    "y = df[['type', 'cvss_score']] \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "157e91bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "4.0     4332\n",
      "10.0    1940\n",
      "3.0      997\n",
      "8.0      821\n",
      "7.0      760\n",
      "2.0      560\n",
      "5.0      256\n",
      "1.0      251\n",
      "6.0      184\n",
      "0.0      111\n",
      "9.0       84\n",
      "Name: count, dtype: int64\n",
      "cvss_score\n",
      "3.0    5471\n",
      "1.0    3358\n",
      "0.0    1030\n",
      "2.0     437\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y['type'].value_counts())\n",
    "print(y['cvss_score'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6df13b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da47661",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1bd307b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy for 'type': 0.7810679611650485\n",
      "Logistic Regression Accuracy for 'cvss_score' : 0.5907766990291262\n",
      "K-Fold mean F1 (type): 0.6766012960555426\n",
      "K-Fold std  F1 (type): 0.004935572706650293\n",
      "K-Fold mean F1 (cvss_score): 0.30941516011688497\n",
      "K-Fold std  F1 (cvss_score): 0.0085282374476804\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        28\n",
      "         1.0       0.97      0.78      0.87        50\n",
      "         2.0       0.79      0.93      0.86       103\n",
      "         3.0       0.25      0.13      0.17       200\n",
      "         4.0       0.72      0.85      0.78       834\n",
      "         5.0       0.90      0.68      0.78        41\n",
      "         6.0       0.47      0.44      0.46        36\n",
      "         7.0       0.83      0.71      0.76       155\n",
      "         8.0       1.00      0.98      0.99       176\n",
      "         9.0       1.00      0.64      0.78        14\n",
      "        10.0       0.95      0.95      0.95       423\n",
      "\n",
      "    accuracy                           0.78      2060\n",
      "   macro avg       0.72      0.65      0.67      2060\n",
      "weighted avg       0.75      0.78      0.76      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.44      0.03      0.06       234\n",
      "         1.0       0.49      0.39      0.43       637\n",
      "         2.0       0.00      0.00      0.00        87\n",
      "         3.0       0.63      0.87      0.73      1102\n",
      "\n",
      "    accuracy                           0.59      2060\n",
      "   macro avg       0.39      0.32      0.30      2060\n",
      "weighted avg       0.54      0.59      0.53      2060\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "multi_lr = MultiOutputClassifier(lr)\n",
    "\n",
    "multi_lr.fit(x_train, y_train)\n",
    "y_pred = multi_lr.predict(x_test)\n",
    "\n",
    "lr_accuracy_type = accuracy_score(y_test['type'], y_pred[:,0])\n",
    "lr_accuracy_cvss_score  = accuracy_score(y_test['cvss_score'], y_pred[:,1])\n",
    "\n",
    "lr_scores_type = cross_val_score(lr, x, y['type'], cv=kf, scoring='f1_macro')\n",
    "lr_scores_cvss_score  = cross_val_score(lr, x, y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"Logistic Regression Accuracy for 'type':\", lr_accuracy_type)\n",
    "print(\"Logistic Regression Accuracy for 'cvss_score' :\", lr_accuracy_cvss_score)\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", lr_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", lr_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", lr_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", lr_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a51c9eb",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbe4c0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy for 'type': 0.9310679611650485\n",
      "Decision Tree Accuracy for 'cvss_score' : 0.5941747572815534\n",
      "K-Fold mean F1 (type): 0.8890847351587636\n",
      "K-Fold std  F1 (type): 0.0125154743907962\n",
      "K-Fold mean F1 (cvss_score): 0.4802156599390739\n",
      "K-Fold std  F1 (cvss_score): 0.00939003549597154\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.75      0.84        28\n",
      "         1.0       1.00      0.96      0.98        50\n",
      "         2.0       0.83      0.85      0.84       103\n",
      "         3.0       0.94      0.93      0.93       200\n",
      "         4.0       0.93      0.95      0.94       834\n",
      "         5.0       0.97      0.88      0.92        41\n",
      "         6.0       0.82      0.86      0.84        36\n",
      "         7.0       0.82      0.81      0.81       155\n",
      "         8.0       0.97      1.00      0.98       176\n",
      "         9.0       0.82      1.00      0.90        14\n",
      "        10.0       0.98      0.96      0.97       423\n",
      "\n",
      "    accuracy                           0.93      2060\n",
      "   macro avg       0.91      0.90      0.91      2060\n",
      "weighted avg       0.93      0.93      0.93      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.36      0.38       234\n",
      "         1.0       0.50      0.52      0.51       637\n",
      "         2.0       0.37      0.39      0.38        87\n",
      "         3.0       0.71      0.70      0.71      1102\n",
      "\n",
      "    accuracy                           0.59      2060\n",
      "   macro avg       0.49      0.49      0.49      2060\n",
      "weighted avg       0.60      0.59      0.59      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "multi_dt = MultiOutputClassifier(dt)\n",
    "\n",
    "multi_dt.fit(x_train, y_train)\n",
    "y_pred = multi_dt.predict(x_test)\n",
    "\n",
    "dt_accuracy_type = accuracy_score(y_test['type'], y_pred[:,0])\n",
    "dt_accuracy_cvss_score  = accuracy_score(y_test['cvss_score'], y_pred[:,1])\n",
    "\n",
    "dt_scores_type = cross_val_score(dt, x, y['type'], cv=kf, scoring='f1_macro')\n",
    "dt_scores_cvss_score  = cross_val_score(dt, x, y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"Decision Tree Accuracy for 'type':\", dt_accuracy_type)\n",
    "print(\"Decision Tree Accuracy for 'cvss_score' :\", dt_accuracy_cvss_score)\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", dt_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", dt_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", dt_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", dt_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0744e4c7",
   "metadata": {},
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18be08cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy for 'type': 0.9228155339805825\n",
      "Random Forest Accuracy for 'cvss_score' : 0.683009708737864\n",
      "K-Fold mean F1 (type): 0.8546050849298507\n",
      "K-Fold std  F1 (type): 0.0001264475514053921\n",
      "K-Fold mean F1 (cvss_score): 0.5566261246000302\n",
      "K-Fold std  F1 (cvss_score): 0.010168859867065548\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.29      0.44        28\n",
      "         1.0       0.98      0.92      0.95        50\n",
      "         2.0       0.85      0.97      0.91       103\n",
      "         3.0       0.82      0.91      0.86       200\n",
      "         4.0       0.93      0.96      0.94       834\n",
      "         5.0       0.90      0.68      0.78        41\n",
      "         6.0       0.68      0.53      0.59        36\n",
      "         7.0       0.91      0.81      0.86       155\n",
      "         8.0       0.99      0.98      0.99       176\n",
      "         9.0       1.00      1.00      1.00        14\n",
      "        10.0       0.97      0.96      0.96       423\n",
      "\n",
      "    accuracy                           0.92      2060\n",
      "   macro avg       0.91      0.82      0.84      2060\n",
      "weighted avg       0.92      0.92      0.92      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.28      0.37       234\n",
      "         1.0       0.61      0.56      0.58       637\n",
      "         2.0       0.88      0.40      0.55        87\n",
      "         3.0       0.72      0.86      0.79      1102\n",
      "\n",
      "    accuracy                           0.68      2060\n",
      "   macro avg       0.69      0.53      0.57      2060\n",
      "weighted avg       0.67      0.68      0.67      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=42)\n",
    "multi_rf = MultiOutputClassifier(rf)\n",
    "\n",
    "multi_rf.fit(x_train, y_train)\n",
    "y_pred = multi_rf.predict(x_test)\n",
    "\n",
    "rf_accuracy_type = accuracy_score(y_test['type'], y_pred[:,0])\n",
    "rf_accuracy_cvss_score  = accuracy_score(y_test['cvss_score'], y_pred[:,1])\n",
    "\n",
    "rf_scores_type = cross_val_score(rf, x, y['type'], cv=kf, scoring='f1_macro')\n",
    "rf_scores_cvss_score  = cross_val_score(rf, x, y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"Random Forest Accuracy for 'type':\", rf_accuracy_type)\n",
    "print(\"Random Forest Accuracy for 'cvss_score' :\", rf_accuracy_cvss_score)\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", rf_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", rf_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", rf_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", rf_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03ef01f",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14fd80c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Accuracy for 'type': 0.9567961165048544\n",
      "Gradient Boosting Accuracy for 'cvss_score' : 0.6839805825242719\n",
      "K-Fold mean F1 (type): 0.9348999179738388\n",
      "K-Fold std  F1 (type): 0.0068855156516761514\n",
      "K-Fold mean F1 (cvss_score): 0.5569627265091422\n",
      "K-Fold std  F1 (cvss_score): 0.012710957395447\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.82      0.90        28\n",
      "         1.0       1.00      0.94      0.97        50\n",
      "         2.0       0.88      0.95      0.91       103\n",
      "         3.0       0.97      0.94      0.95       200\n",
      "         4.0       0.96      0.96      0.96       834\n",
      "         5.0       0.95      0.93      0.94        41\n",
      "         6.0       0.78      0.89      0.83        36\n",
      "         7.0       0.92      0.92      0.92       155\n",
      "         8.0       0.99      0.98      0.99       176\n",
      "         9.0       0.88      1.00      0.93        14\n",
      "        10.0       0.97      0.98      0.98       423\n",
      "\n",
      "    accuracy                           0.96      2060\n",
      "   macro avg       0.94      0.94      0.93      2060\n",
      "weighted avg       0.96      0.96      0.96      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.27      0.36       234\n",
      "         1.0       0.60      0.57      0.58       637\n",
      "         2.0       0.80      0.43      0.56        87\n",
      "         3.0       0.73      0.86      0.79      1102\n",
      "\n",
      "    accuracy                           0.68      2060\n",
      "   macro avg       0.67      0.53      0.57      2060\n",
      "weighted avg       0.67      0.68      0.67      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=220, max_depth=5, random_state=42)\n",
    "multi_gb = MultiOutputClassifier(gb)\n",
    "\n",
    "multi_gb.fit(x_train, y_train)\n",
    "y_pred = multi_gb.predict(x_test)\n",
    "\n",
    "gb_accuracy_type = accuracy_score(y_test['type'], y_pred[:,0])\n",
    "gb_accuracy_cvss_score  = accuracy_score(y_test['cvss_score'], y_pred[:,1])\n",
    "\n",
    "gb_scores_type = cross_val_score(gb, x, y['type'], cv=kf, scoring='f1_macro')\n",
    "gb_scores_cvss_score  = cross_val_score(gb, x, y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"Gradient Boosting Accuracy for 'type':\", gb_accuracy_type)\n",
    "print(\"Gradient Boosting Accuracy for 'cvss_score' :\", gb_accuracy_cvss_score)\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", gb_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", gb_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", gb_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", gb_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ae8c89",
   "metadata": {},
   "source": [
    "# Extra Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59df5f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Tree Accuracy for 'type': 0.9101941747572816\n",
      "Extra Tree Accuracy for 'cvss_score' : 0.6839805825242719\n",
      "K-Fold mean F1 (type): 0.8538555537385529\n",
      "K-Fold std  F1 (type): 0.00747740817360574\n",
      "K-Fold mean F1 (cvss_score): 0.5581033006256725\n",
      "K-Fold std  F1 (cvss_score): 0.013277231717926339\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.25      0.39        28\n",
      "         1.0       1.00      0.90      0.95        50\n",
      "         2.0       0.83      0.95      0.89       103\n",
      "         3.0       0.80      0.82      0.81       200\n",
      "         4.0       0.91      0.95      0.93       834\n",
      "         5.0       0.91      0.71      0.79        41\n",
      "         6.0       0.76      0.61      0.68        36\n",
      "         7.0       0.90      0.82      0.86       155\n",
      "         8.0       0.99      0.98      0.99       176\n",
      "         9.0       0.82      1.00      0.90        14\n",
      "        10.0       0.97      0.95      0.96       423\n",
      "\n",
      "    accuracy                           0.91      2060\n",
      "   macro avg       0.89      0.81      0.83      2060\n",
      "weighted avg       0.91      0.91      0.91      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.54      0.24      0.33       234\n",
      "         1.0       0.62      0.56      0.59       637\n",
      "         2.0       0.84      0.47      0.60        87\n",
      "         3.0       0.72      0.87      0.79      1102\n",
      "\n",
      "    accuracy                           0.68      2060\n",
      "   macro avg       0.68      0.53      0.58      2060\n",
      "weighted avg       0.67      0.68      0.67      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "et = ExtraTreesClassifier(random_state=42)\n",
    "multi_et = MultiOutputClassifier(et)\n",
    "\n",
    "multi_et.fit(x_train, y_train)\n",
    "y_pred = multi_et.predict(x_test)\n",
    "\n",
    "et_accuracy_type = accuracy_score(y_test['type'], y_pred[:,0])\n",
    "et_accuracy_cvss_score  = accuracy_score(y_test['cvss_score'], y_pred[:,1])\n",
    "\n",
    "et_scores_type = cross_val_score(et, x, y['type'], cv=kf, scoring='f1_macro')\n",
    "et_scores_cvss_score  = cross_val_score(et, x, y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"Extra Tree Accuracy for 'type':\", et_accuracy_type)\n",
    "print(\"Extra Tree Accuracy for 'cvss_score' :\", et_accuracy_cvss_score)\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", et_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", et_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", et_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", et_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64464e82",
   "metadata": {},
   "source": [
    "# Hist Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65c73a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hist Gradient Boosting Accuracy for 'type': 0.9495145631067962\n",
      "Hist Gradient Boosting Accuracy for 'cvss_score' : 0.6786407766990291\n",
      "\n",
      "K-Fold mean F1 (type): 0.9253536832602839\n",
      "K-Fold std  F1 (type): 0.008089240204701052\n",
      "\n",
      "K-Fold mean F1 (cvss_score): 0.55081528466233\n",
      "K-Fold std  F1 (cvss_score): 0.009037546901454434\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.89      0.94        28\n",
      "         1.0       0.98      0.96      0.97        50\n",
      "         2.0       0.86      0.92      0.89       103\n",
      "         3.0       0.95      0.93      0.94       200\n",
      "         4.0       0.95      0.96      0.95       834\n",
      "         5.0       0.95      0.93      0.94        41\n",
      "         6.0       0.84      0.89      0.86        36\n",
      "         7.0       0.90      0.84      0.87       155\n",
      "         8.0       0.99      0.99      0.99       176\n",
      "         9.0       1.00      1.00      1.00        14\n",
      "        10.0       0.98      0.98      0.98       423\n",
      "\n",
      "    accuracy                           0.95      2060\n",
      "   macro avg       0.94      0.94      0.94      2060\n",
      "weighted avg       0.95      0.95      0.95      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.54      0.32      0.40       234\n",
      "         1.0       0.59      0.55      0.57       637\n",
      "         2.0       0.82      0.43      0.56        87\n",
      "         3.0       0.73      0.85      0.78      1102\n",
      "\n",
      "    accuracy                           0.68      2060\n",
      "   macro avg       0.67      0.53      0.58      2060\n",
      "weighted avg       0.67      0.68      0.66      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hgb = HistGradientBoostingClassifier(max_iter=200, random_state=42)\n",
    "multi_hgb = MultiOutputClassifier(hgb)\n",
    "\n",
    "multi_hgb.fit(x_train, y_train)\n",
    "y_pred = multi_hgb.predict(x_test)\n",
    "\n",
    "hgb_accuracy_type = accuracy_score(y_test['type'], y_pred[:,0])\n",
    "hgb_accuracy_cvss_score  = accuracy_score(y_test['cvss_score'], y_pred[:,1])\n",
    "\n",
    "hgb_scores_type = cross_val_score(hgb, x, y['type'], cv=kf, scoring='f1_macro')\n",
    "hgb_scores_cvss_score  = cross_val_score(hgb, x, y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"Hist Gradient Boosting Accuracy for 'type':\", hgb_accuracy_type)\n",
    "print(\"Hist Gradient Boosting Accuracy for 'cvss_score' :\", hgb_accuracy_cvss_score)\n",
    "\n",
    "print(\"\\nK-Fold mean F1 (type):\", hgb_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", hgb_scores_type.std())\n",
    "\n",
    "print(\"\\nK-Fold mean F1 (cvss_score):\", hgb_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", hgb_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c481b109",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "181533b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy for 'type': 0.8121359223300971\n",
      "KNN Accuracy for 'cvss_score' : 0.6029126213592233\n",
      "K-Fold mean F1 (type): 0.703665024476167\n",
      "K-Fold std  F1 (type): 0.009585380349919074\n",
      "K-Fold mean F1 (cvss_score): 0.47555034828233395\n",
      "K-Fold std  F1 (cvss_score): 0.001946083182048487\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.43      0.21      0.29        28\n",
      "         1.0       0.91      0.84      0.88        50\n",
      "         2.0       0.78      0.78      0.78       103\n",
      "         3.0       0.57      0.66      0.61       200\n",
      "         4.0       0.81      0.85      0.83       834\n",
      "         5.0       0.67      0.63      0.65        41\n",
      "         6.0       0.58      0.31      0.40        36\n",
      "         7.0       0.77      0.72      0.74       155\n",
      "         8.0       0.97      0.97      0.97       176\n",
      "         9.0       0.92      0.79      0.85        14\n",
      "        10.0       0.92      0.90      0.91       423\n",
      "\n",
      "    accuracy                           0.81      2060\n",
      "   macro avg       0.76      0.69      0.72      2060\n",
      "weighted avg       0.81      0.81      0.81      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.31      0.34       234\n",
      "         1.0       0.55      0.54      0.54       637\n",
      "         2.0       0.30      0.36      0.33        87\n",
      "         3.0       0.70      0.72      0.71      1102\n",
      "\n",
      "    accuracy                           0.60      2060\n",
      "   macro avg       0.48      0.48      0.48      2060\n",
      "weighted avg       0.60      0.60      0.60      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "multi_knn = MultiOutputClassifier(knn)\n",
    "\n",
    "multi_knn.fit(x_train, y_train)\n",
    "y_pred = multi_knn.predict(x_test)\n",
    "\n",
    "knn_accuracy_type = accuracy_score(y_test['type'], y_pred[:,0])\n",
    "knn_accuracy_cvss_score  = accuracy_score(y_test['cvss_score'], y_pred[:,1])\n",
    "\n",
    "knn_scores_type = cross_val_score(knn, x, y['type'], cv=kf, scoring='f1_macro')\n",
    "knn_scores_cvss_score  = cross_val_score(knn, x, y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"KNN Accuracy for 'type':\", knn_accuracy_type)\n",
    "print(\"KNN Accuracy for 'cvss_score' :\", knn_accuracy_cvss_score)\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", knn_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", knn_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", knn_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", knn_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce6e040",
   "metadata": {},
   "source": [
    "# Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "427d2fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost Accuracy for 'type': 0.8097087378640777\n",
      "Adaboost Accuracy for 'cvss_score' : 0.587378640776699\n",
      "K-Fold mean F1 (type): 0.6867271894229664\n",
      "K-Fold std  F1 (type): 0.040149916689530336\n",
      "K-Fold mean F1 (cvss_score): 0.3198572912736683\n",
      "K-Fold std  F1 (cvss_score): 0.004907609514319969\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.25      0.40        28\n",
      "         1.0       0.93      0.80      0.86        50\n",
      "         2.0       0.65      0.75      0.70       103\n",
      "         3.0       0.75      0.57      0.65       200\n",
      "         4.0       0.76      0.90      0.83       834\n",
      "         5.0       0.91      0.51      0.66        41\n",
      "         6.0       0.42      0.47      0.45        36\n",
      "         7.0       0.80      0.63      0.71       155\n",
      "         8.0       1.00      0.74      0.85       176\n",
      "         9.0       0.92      0.79      0.85        14\n",
      "        10.0       0.95      0.94      0.95       423\n",
      "\n",
      "    accuracy                           0.81      2060\n",
      "   macro avg       0.83      0.67      0.72      2060\n",
      "weighted avg       0.82      0.81      0.81      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.09      0.15       234\n",
      "         1.0       0.47      0.37      0.42       637\n",
      "         2.0       0.00      0.00      0.00        87\n",
      "         3.0       0.63      0.86      0.73      1102\n",
      "\n",
      "    accuracy                           0.59      2060\n",
      "   macro avg       0.41      0.33      0.32      2060\n",
      "weighted avg       0.54      0.59      0.53      2060\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "ab = AdaBoostClassifier(n_estimators=200)\n",
    "multi_ab = MultiOutputClassifier(ab)\n",
    "\n",
    "multi_ab.fit(x_train, y_train)\n",
    "y_pred = multi_ab.predict(x_test)\n",
    "\n",
    "ab_accuracy_type = accuracy_score(y_test['type'], y_pred[:,0])\n",
    "ab_accuracy_cvss_score  = accuracy_score(y_test['cvss_score'], y_pred[:,1])\n",
    "\n",
    "ab_scores_type = cross_val_score(ab, x, y['type'], cv=kf, scoring='f1_macro')\n",
    "ab_scores_cvss_score  = cross_val_score(ab, x, y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"Adaboost Accuracy for 'type':\", ab_accuracy_type)\n",
    "print(\"Adaboost Accuracy for 'cvss_score' :\", ab_accuracy_cvss_score)\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", ab_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", ab_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", ab_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", ab_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0ba78d",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65f0c01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000839 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2988\n",
      "[LightGBM] [Info] Number of data points in the train set: 8236, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -4.597429\n",
      "[LightGBM] [Info] Start training from score -3.712965\n",
      "[LightGBM] [Info] Start training from score -2.891587\n",
      "[LightGBM] [Info] Start training from score -2.335415\n",
      "[LightGBM] [Info] Start training from score -0.856323\n",
      "[LightGBM] [Info] Start training from score -3.645632\n",
      "[LightGBM] [Info] Start training from score -4.019058\n",
      "[LightGBM] [Info] Start training from score -2.611042\n",
      "[LightGBM] [Info] Start training from score -2.547020\n",
      "[LightGBM] [Info] Start training from score -4.767775\n",
      "[LightGBM] [Info] Start training from score -1.691780\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001380 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2988\n",
      "[LightGBM] [Info] Number of data points in the train set: 8236, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -2.336671\n",
      "[LightGBM] [Info] Start training from score -1.107515\n",
      "[LightGBM] [Info] Start training from score -3.158337\n",
      "[LightGBM] [Info] Start training from score -0.633981\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000732 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2969\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -4.571366\n",
      "[LightGBM] [Info] Start training from score -3.704147\n",
      "[LightGBM] [Info] Start training from score -2.878208\n",
      "[LightGBM] [Info] Start training from score -2.322300\n",
      "[LightGBM] [Info] Start training from score -0.853680\n",
      "[LightGBM] [Info] Start training from score -3.652262\n",
      "[LightGBM] [Info] Start training from score -4.005732\n",
      "[LightGBM] [Info] Start training from score -2.623446\n",
      "[LightGBM] [Info] Start training from score -2.582142\n",
      "[LightGBM] [Info] Start training from score -4.942225\n",
      "[LightGBM] [Info] Start training from score -1.684914\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000649 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -4.503312\n",
      "[LightGBM] [Info] Start training from score -3.710082\n",
      "[LightGBM] [Info] Start training from score -2.964749\n",
      "[LightGBM] [Info] Start training from score -2.338780\n",
      "[LightGBM] [Info] Start training from score -0.877219\n",
      "[LightGBM] [Info] Start training from score -3.692382\n",
      "[LightGBM] [Info] Start training from score -4.063361\n",
      "[LightGBM] [Info] Start training from score -2.549911\n",
      "[LightGBM] [Info] Start training from score -2.491924\n",
      "[LightGBM] [Info] Start training from score -4.790994\n",
      "[LightGBM] [Info] Start training from score -1.664696\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000705 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -4.516558\n",
      "[LightGBM] [Info] Start training from score -3.728100\n",
      "[LightGBM] [Info] Start training from score -2.893874\n",
      "[LightGBM] [Info] Start training from score -2.343322\n",
      "[LightGBM] [Info] Start training from score -0.866419\n",
      "[LightGBM] [Info] Start training from score -3.740295\n",
      "[LightGBM] [Info] Start training from score -4.005732\n",
      "[LightGBM] [Info] Start training from score -2.647837\n",
      "[LightGBM] [Info] Start training from score -2.515078\n",
      "[LightGBM] [Info] Start training from score -4.706911\n",
      "[LightGBM] [Info] Start training from score -1.657791\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000680 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2969\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -2.349410\n",
      "[LightGBM] [Info] Start training from score -1.123392\n",
      "[LightGBM] [Info] Start training from score -3.133602\n",
      "[LightGBM] [Info] Start training from score -0.623921\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000697 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -2.270190\n",
      "[LightGBM] [Info] Start training from score -1.117140\n",
      "[LightGBM] [Info] Start training from score -3.195691\n",
      "[LightGBM] [Info] Start training from score -0.637609\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000751 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -2.288696\n",
      "[LightGBM] [Info] Start training from score -1.120708\n",
      "[LightGBM] [Info] Start training from score -3.150466\n",
      "[LightGBM] [Info] Start training from score -0.635406\n",
      "LightGBM Accuracy for 'type': 0.9533980582524272\n",
      "LightGBM Accuracy for 'cvss_score' : 0.6786407766990291\n",
      "K-Fold mean F1 (type): 0.9380881385322223\n",
      "K-Fold std  F1 (type): 0.006445440924520106\n",
      "K-Fold mean F1 (cvss_score): 0.5620766161742332\n",
      "K-Fold std  F1 (cvss_score): 0.015545681589758652\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.89      0.94        28\n",
      "         1.0       0.98      0.94      0.96        50\n",
      "         2.0       0.84      0.94      0.89       103\n",
      "         3.0       0.97      0.92      0.94       200\n",
      "         4.0       0.95      0.97      0.96       834\n",
      "         5.0       0.95      0.93      0.94        41\n",
      "         6.0       0.82      0.86      0.84        36\n",
      "         7.0       0.92      0.87      0.89       155\n",
      "         8.0       0.99      0.99      0.99       176\n",
      "         9.0       0.93      1.00      0.97        14\n",
      "        10.0       0.98      0.98      0.98       423\n",
      "\n",
      "    accuracy                           0.95      2060\n",
      "   macro avg       0.94      0.94      0.94      2060\n",
      "weighted avg       0.95      0.95      0.95      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.28      0.38       234\n",
      "         1.0       0.61      0.52      0.56       637\n",
      "         2.0       0.86      0.41      0.56        87\n",
      "         3.0       0.71      0.88      0.78      1102\n",
      "\n",
      "    accuracy                           0.68      2060\n",
      "   macro avg       0.69      0.52      0.57      2060\n",
      "weighted avg       0.67      0.68      0.66      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "lgbm = LGBMClassifier(random_state=42)\n",
    "multi_lgbm = MultiOutputClassifier(lgbm)\n",
    "\n",
    "multi_lgbm.fit(x_train, y_train)\n",
    "y_pred = multi_lgbm.predict(x_test)\n",
    "\n",
    "lgbm_accuracy_type = accuracy_score(y_test['type'], y_pred[:,0])\n",
    "lgbm_accuracy_cvss_score  = accuracy_score(y_test['cvss_score'], y_pred[:,1])\n",
    "\n",
    "lgbm_scores_type = cross_val_score(lgbm, x, y['type'], cv=kf, scoring='f1_macro')\n",
    "lgbm_scores_cvss_score  = cross_val_score(lgbm, x, y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"LightGBM Accuracy for 'type':\", lgbm_accuracy_type)\n",
    "print(\"LightGBM Accuracy for 'cvss_score' :\", lgbm_accuracy_cvss_score)\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", lgbm_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", lgbm_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", lgbm_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", lgbm_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef7422a",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "107b6bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Accuracy for 'type': 0.9446601941747573\n",
      "Bagging Accuracy for 'cvss_score' : 0.6495145631067961\n",
      "K-Fold mean F1 (type): 0.9154417840234023\n",
      "K-Fold std  F1 (type): 0.005004021888812271\n",
      "K-Fold mean F1 (cvss_score): 0.539174382489437\n",
      "K-Fold std  F1 (cvss_score): 0.010126404334934839\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.86      0.92        28\n",
      "         1.0       1.00      0.94      0.97        50\n",
      "         2.0       0.82      0.94      0.87       103\n",
      "         3.0       0.97      0.91      0.94       200\n",
      "         4.0       0.94      0.96      0.95       834\n",
      "         5.0       0.97      0.88      0.92        41\n",
      "         6.0       0.78      0.89      0.83        36\n",
      "         7.0       0.90      0.86      0.88       155\n",
      "         8.0       0.98      1.00      0.99       176\n",
      "         9.0       0.82      1.00      0.90        14\n",
      "        10.0       0.98      0.97      0.97       423\n",
      "\n",
      "    accuracy                           0.94      2060\n",
      "   macro avg       0.92      0.93      0.92      2060\n",
      "weighted avg       0.95      0.94      0.94      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.30      0.38       234\n",
      "         1.0       0.54      0.57      0.56       637\n",
      "         2.0       0.72      0.41      0.53        87\n",
      "         3.0       0.72      0.79      0.75      1102\n",
      "\n",
      "    accuracy                           0.65      2060\n",
      "   macro avg       0.62      0.52      0.55      2060\n",
      "weighted avg       0.64      0.65      0.64      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bagging = BaggingClassifier(random_state=42)\n",
    "multi_bag = MultiOutputClassifier(bagging)\n",
    "\n",
    "multi_bag.fit(x_train, y_train)\n",
    "y_pred = multi_bag.predict(x_test)\n",
    "\n",
    "bag_accuracy_type = accuracy_score(y_test['type'], y_pred[:,0])\n",
    "bag_accuracy_cvss_score  = accuracy_score(y_test['cvss_score'], y_pred[:,1])\n",
    "\n",
    "bag_scores_type = cross_val_score(bagging, x, y['type'], cv=kf, scoring='f1_macro')\n",
    "bag_scores_cvss_score  = cross_val_score(bagging, x, y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"Bagging Accuracy for 'type':\", bag_accuracy_type)\n",
    "print(\"Bagging Accuracy for 'cvss_score' :\", bag_accuracy_cvss_score)\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", bag_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", bag_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", bag_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", bag_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b43fe01",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fd858b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Accuracy for 'type': 0.8495145631067961\n",
      "SVC Accuracy for 'cvss_score' : 0.6330097087378641\n",
      "K-Fold mean F1 (type): 0.7522140407362387\n",
      "K-Fold std  F1 (type): 0.005939275133587859\n",
      "K-Fold mean F1 (cvss_score): 0.35284066989479984\n",
      "K-Fold std  F1 (cvss_score): 0.0032606218597976996\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.07      0.13        28\n",
      "         1.0       0.95      0.78      0.86        50\n",
      "         2.0       0.77      0.95      0.85       103\n",
      "         3.0       0.62      0.62      0.62       200\n",
      "         4.0       0.84      0.88      0.86       834\n",
      "         5.0       0.93      0.68      0.79        41\n",
      "         6.0       0.58      0.53      0.55        36\n",
      "         7.0       0.86      0.77      0.82       155\n",
      "         8.0       1.00      0.96      0.98       176\n",
      "         9.0       0.85      0.79      0.81        14\n",
      "        10.0       0.95      0.95      0.95       423\n",
      "\n",
      "    accuracy                           0.85      2060\n",
      "   macro avg       0.82      0.73      0.75      2060\n",
      "weighted avg       0.85      0.85      0.84      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.11      0.19       234\n",
      "         1.0       0.58      0.44      0.50       637\n",
      "         2.0       0.00      0.00      0.00        87\n",
      "         3.0       0.65      0.91      0.76      1102\n",
      "\n",
      "    accuracy                           0.63      2060\n",
      "   macro avg       0.46      0.36      0.36      2060\n",
      "weighted avg       0.59      0.63      0.58      2060\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(kernel='rbf', C=5, probability=True)\n",
    "multi_svc = MultiOutputClassifier(svc)\n",
    "\n",
    "multi_svc.fit(x_train, y_train)\n",
    "y_pred = multi_svc.predict(x_test)\n",
    "\n",
    "svc_accuracy_type = accuracy_score(y_test['type'], y_pred[:,0])\n",
    "svc_accuracy_cvss_score  = accuracy_score(y_test['cvss_score'], y_pred[:,1])\n",
    "\n",
    "svc_scores_type = cross_val_score(svc, x, y['type'], cv=kf, scoring='f1_macro')\n",
    "svc_scores_cvss_score  = cross_val_score(svc, x, y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"SVC Accuracy for 'type':\", svc_accuracy_type)\n",
    "print(\"SVC Accuracy for 'cvss_score' :\", svc_accuracy_cvss_score)\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", svc_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", svc_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", svc_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", svc_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ed28ba",
   "metadata": {},
   "source": [
    "# Hard Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1edd6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Voting Accuracy for type: 0.9092233009708738\n",
      "Hard Voting Accuracy for cvss_score : 0.6849514563106797\n",
      "K-fold F1 mean (type): 0.8535412937991548\n",
      "K-fold F1 std  (type): 0.00542323256362824\n",
      "K-fold F1 mean (cvss_score) : 0.546800250140797\n",
      "K-fold F1 std  (cvss_score) : 0.004329239698999266\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.29      0.43        28\n",
      "         1.0       0.98      0.90      0.94        50\n",
      "         2.0       0.82      0.95      0.88       103\n",
      "         3.0       0.82      0.83      0.83       200\n",
      "         4.0       0.90      0.95      0.93       834\n",
      "         5.0       0.90      0.68      0.78        41\n",
      "         6.0       0.68      0.53      0.59        36\n",
      "         7.0       0.90      0.81      0.85       155\n",
      "         8.0       0.99      0.98      0.99       176\n",
      "         9.0       1.00      1.00      1.00        14\n",
      "        10.0       0.97      0.95      0.96       423\n",
      "\n",
      "    accuracy                           0.91      2060\n",
      "   macro avg       0.90      0.81      0.83      2060\n",
      "weighted avg       0.91      0.91      0.91      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "model1 = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "model2 = ExtraTreesClassifier(n_estimators=200, random_state=42)\n",
    "model3 = LogisticRegression(max_iter=500)\n",
    "\n",
    "voting_hard = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', model1),\n",
    "        ('et', model2),\n",
    "        ('lr', model3)\n",
    "    ],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "multi_voting_hard = MultiOutputClassifier(voting_hard)\n",
    "\n",
    "multi_voting_hard.fit(x_train, y_train)\n",
    "y_pred = multi_voting_hard.predict(x_test)\n",
    "\n",
    "hard_acc_type = accuracy_score(y_test[\"type\"], y_pred[:, 0])\n",
    "hard_acc_cvss_score  = accuracy_score(y_test[\"cvss_score\"],  y_pred[:, 1])\n",
    "\n",
    "\n",
    "hard_scores_type = cross_val_score(voting_hard, x, y['type'], cv=kf, scoring='f1_macro')\n",
    "hard_scores_cvss_score  = cross_val_score(voting_hard, x, y['cvss_score'],  cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"Hard Voting Accuracy for type:\", hard_acc_type)\n",
    "print(\"Hard Voting Accuracy for cvss_score :\", hard_acc_cvss_score)\n",
    "\n",
    "print(\"K-fold F1 mean (type):\", hard_scores_type.mean())\n",
    "print(\"K-fold F1 std  (type):\", hard_scores_type.std())\n",
    "\n",
    "print(\"K-fold F1 mean (cvss_score) :\", hard_scores_cvss_score.mean())\n",
    "print(\"K-fold F1 std  (cvss_score) :\", hard_scores_cvss_score.std())\n",
    "\n",
    "print(classification_report(y_test[\"type\"], y_pred[:, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e1122d",
   "metadata": {},
   "source": [
    "# Soft Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16e3a5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Voting Accuracy for type: 0.9063106796116505\n",
      "Hard Voting Accuracy for cvss_score : 0.6786407766990291\n",
      "K-fold F1 mean (type): 0.8401265096947981\n",
      "K-fold F1 std  (type): 0.004807786214563205\n",
      "K-fold F1 mean (cvss_score) : 0.53279587273932\n",
      "K-fold F1 std  (cvss_score) : 0.00474278913678708\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.25      0.40        28\n",
      "         1.0       0.98      0.88      0.93        50\n",
      "         2.0       0.83      0.97      0.89       103\n",
      "         3.0       0.82      0.83      0.83       200\n",
      "         4.0       0.89      0.95      0.92       834\n",
      "         5.0       0.90      0.68      0.78        41\n",
      "         6.0       0.68      0.53      0.59        36\n",
      "         7.0       0.89      0.79      0.84       155\n",
      "         8.0       1.00      0.98      0.99       176\n",
      "         9.0       1.00      0.86      0.92        14\n",
      "        10.0       0.97      0.95      0.96       423\n",
      "\n",
      "    accuracy                           0.91      2060\n",
      "   macro avg       0.91      0.79      0.82      2060\n",
      "weighted avg       0.91      0.91      0.90      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model1 = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "model2 = ExtraTreesClassifier(n_estimators=200, random_state=42)\n",
    "model3 = LogisticRegression(max_iter=500)\n",
    "\n",
    "voting_hard = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', model1),\n",
    "        ('et', model2),\n",
    "        ('lr', model3)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "multi_voting_hard = MultiOutputClassifier(voting_hard)\n",
    "\n",
    "multi_voting_hard.fit(x_train, y_train)\n",
    "y_pred = multi_voting_hard.predict(x_test)\n",
    "\n",
    "soft_acc_type = accuracy_score(y_test[\"type\"], y_pred[:, 0])\n",
    "soft_acc_cvss_score  = accuracy_score(y_test[\"cvss_score\"],  y_pred[:, 1])\n",
    "\n",
    "soft_scores_type = cross_val_score(voting_hard, x, y['type'], cv=kf, scoring='f1_macro')\n",
    "soft_scores_cvss_score  = cross_val_score(voting_hard, x, y['cvss_score'],  cv=kf, scoring='f1_macro')\n",
    "\n",
    "\n",
    "print(\"Hard Voting Accuracy for type:\", soft_acc_type)\n",
    "print(\"Hard Voting Accuracy for cvss_score :\", soft_acc_cvss_score)\n",
    "\n",
    "print(\"K-fold F1 mean (type):\", soft_scores_type.mean())\n",
    "print(\"K-fold F1 std  (type):\", soft_scores_type.std())\n",
    "\n",
    "print(\"K-fold F1 mean (cvss_score) :\", soft_scores_cvss_score.mean())\n",
    "print(\"K-fold F1 std  (cvss_score) :\", soft_scores_cvss_score.std())\n",
    "\n",
    "print(classification_report(y_test[\"type\"], y_pred[:, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28826002",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f805bfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Accuracy for type: 0.9276699029126214\n",
      "Stacking Accuracy for cvss_score : 0.6936893203883495\n",
      "K-fold F1 mean (type): 0.8767479622209979\n",
      "K-fold F1 std  (type): 0.008390731109801819\n",
      "K-fold F1 mean (cvss_score) : 0.5663912915765845\n",
      "K-fold F1 std  (cvss_score) : 0.017561396746994373\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.43      0.56        28\n",
      "         1.0       1.00      0.92      0.96        50\n",
      "         2.0       0.85      0.93      0.89       103\n",
      "         3.0       0.83      0.90      0.86       200\n",
      "         4.0       0.94      0.96      0.95       834\n",
      "         5.0       0.88      0.71      0.78        41\n",
      "         6.0       0.72      0.58      0.65        36\n",
      "         7.0       0.90      0.83      0.86       155\n",
      "         8.0       1.00      0.98      0.99       176\n",
      "         9.0       0.88      1.00      0.93        14\n",
      "        10.0       0.97      0.97      0.97       423\n",
      "\n",
      "    accuracy                           0.93      2060\n",
      "   macro avg       0.89      0.84      0.85      2060\n",
      "weighted avg       0.93      0.93      0.93      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "base1 = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "base2 = ExtraTreesClassifier(n_estimators=200, random_state=42)\n",
    "base3 = LogisticRegression(max_iter=500)\n",
    "\n",
    "stacking = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', base1),\n",
    "        ('et', base2),\n",
    "        ('lr', base3)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(max_iter=500)\n",
    ")\n",
    "\n",
    "multi_stacking = MultiOutputClassifier(stacking)\n",
    "\n",
    "multi_stacking.fit(x_train, y_train)\n",
    "y_pred = multi_stacking.predict(x_test)\n",
    "\n",
    "stacking_acc_type = accuracy_score(y_test[\"type\"], y_pred[:, 0])\n",
    "stacking_acc_cvss_score  = accuracy_score(y_test[\"cvss_score\"],  y_pred[:, 1])\n",
    "\n",
    "stacking_scores_type = cross_val_score(stacking, x, y['type'], cv=kf, scoring='f1_macro')\n",
    "stacking_scores_cvss_score  = cross_val_score(stacking, x, y['cvss_score'],  cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"Stacking Accuracy for type:\", stacking_acc_type)\n",
    "print(\"Stacking Accuracy for cvss_score :\", stacking_acc_cvss_score)\n",
    "\n",
    "print(\"K-fold F1 mean (type):\", stacking_scores_type.mean())\n",
    "print(\"K-fold F1 std  (type):\", stacking_scores_type.std())\n",
    "\n",
    "print(\"K-fold F1 mean (cvss_score) :\", stacking_scores_cvss_score.mean())\n",
    "print(\"K-fold F1 std  (cvss_score) :\", stacking_scores_cvss_score.std())\n",
    "\n",
    "print(classification_report(y_test[\"type\"], y_pred[:, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1562279",
   "metadata": {},
   "source": [
    "# Bagged KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21057e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging KNN Accuracy for 'type': 0.8121359223300971\n",
      "Bagging KNN Accuracy for 'cvss_score': 0.6242718446601941\n",
      "K-Fold mean F1 (type): 0.6668287934933416\n",
      "K-Fold std  F1 (type): 0.009369176389608894\n",
      "K-Fold mean F1 (cvss_score): 0.47975514375633527\n",
      "K-Fold std  F1 (cvss_score): 0.015508790326115724\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.07      0.12        28\n",
      "         1.0       0.85      0.78      0.81        50\n",
      "         2.0       0.83      0.71      0.76       103\n",
      "         3.0       0.56      0.66      0.61       200\n",
      "         4.0       0.78      0.89      0.83       834\n",
      "         5.0       0.96      0.59      0.73        41\n",
      "         6.0       0.78      0.19      0.31        36\n",
      "         7.0       0.83      0.65      0.73       155\n",
      "         8.0       0.98      0.95      0.97       176\n",
      "         9.0       1.00      0.43      0.60        14\n",
      "        10.0       0.93      0.91      0.92       423\n",
      "\n",
      "    accuracy                           0.81      2060\n",
      "   macro avg       0.80      0.62      0.67      2060\n",
      "weighted avg       0.82      0.81      0.81      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.45      0.23      0.31       234\n",
      "         1.0       0.55      0.50      0.53       637\n",
      "         2.0       0.46      0.33      0.39        87\n",
      "         3.0       0.68      0.80      0.74      1102\n",
      "\n",
      "    accuracy                           0.62      2060\n",
      "   macro avg       0.54      0.47      0.49      2060\n",
      "weighted avg       0.61      0.62      0.61      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "bag_knn = BaggingClassifier(estimator=knn, n_estimators=100, random_state=42)\n",
    "multi_bag_knn = MultiOutputClassifier(bag_knn)\n",
    "\n",
    "multi_bag_knn.fit(x_train, y_train)\n",
    "y_pred = multi_bag_knn.predict(x_test)\n",
    "\n",
    "bag_knn_accuracy_type = accuracy_score(y_test['type'], y_pred[:,0])\n",
    "bag_knn_accuracy_cvss_score = accuracy_score(y_test['cvss_score'], y_pred[:,1])\n",
    "\n",
    "bag_knn_scores_type = cross_val_score(bag_knn, x, y['type'], cv=kf, scoring='f1_macro')\n",
    "bag_knn_scores_cvss_score = cross_val_score(bag_knn, x, y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"Bagging KNN Accuracy for 'type':\", bag_knn_accuracy_type)\n",
    "print(\"Bagging KNN Accuracy for 'cvss_score':\", bag_knn_accuracy_cvss_score)\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", bag_knn_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", bag_knn_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", bag_knn_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", bag_knn_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8a3492",
   "metadata": {},
   "source": [
    "# Bagged DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88aa69aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Decision Tree Accuracy for 'type': 0.95\n",
      "Bagging Decision Tree Accuracy for 'cvss_score': 0.6694174757281554\n",
      "K-Fold mean F1 (type): 0.9231444116667983\n",
      "K-Fold std  F1 (type): 0.005892972772159492\n",
      "K-Fold mean F1 (cvss_score): 0.5533553548248852\n",
      "K-Fold std  F1 (cvss_score): 0.008555680493433773\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.86      0.92        28\n",
      "         1.0       1.00      0.96      0.98        50\n",
      "         2.0       0.82      0.91      0.87       103\n",
      "         3.0       0.98      0.93      0.95       200\n",
      "         4.0       0.95      0.96      0.95       834\n",
      "         5.0       0.97      0.90      0.94        41\n",
      "         6.0       0.79      0.92      0.85        36\n",
      "         7.0       0.90      0.88      0.89       155\n",
      "         8.0       0.99      1.00      1.00       176\n",
      "         9.0       0.82      1.00      0.90        14\n",
      "        10.0       0.98      0.97      0.98       423\n",
      "\n",
      "    accuracy                           0.95      2060\n",
      "   macro avg       0.93      0.94      0.93      2060\n",
      "weighted avg       0.95      0.95      0.95      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.28      0.37       234\n",
      "         1.0       0.58      0.54      0.56       637\n",
      "         2.0       0.86      0.41      0.56        87\n",
      "         3.0       0.71      0.85      0.78      1102\n",
      "\n",
      "    accuracy                           0.67      2060\n",
      "   macro avg       0.67      0.52      0.57      2060\n",
      "weighted avg       0.66      0.67      0.65      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "bag_dt = BaggingClassifier(estimator=dt, n_estimators=100, random_state=42)\n",
    "multi_bag_dt = MultiOutputClassifier(bag_dt)\n",
    "\n",
    "multi_bag_dt.fit(x_train, y_train)\n",
    "\n",
    "y_pred = multi_bag_dt.predict(x_test)\n",
    "\n",
    "bag_dt_accuracy_type = accuracy_score(y_test['type'], y_pred[:,0])\n",
    "bag_dt_accuracy_cvss_score = accuracy_score(y_test['cvss_score'], y_pred[:,1])\n",
    "\n",
    "bag_dt_scores_type = cross_val_score(bag_dt, x, y['type'], cv=kf, scoring='f1_macro')\n",
    "bag_dt_scores_cvss_score = cross_val_score(bag_dt, x, y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"Bagging Decision Tree Accuracy for 'type':\", bag_dt_accuracy_type)\n",
    "print(\"Bagging Decision Tree Accuracy for 'cvss_score':\", bag_dt_accuracy_cvss_score)\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", bag_dt_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", bag_dt_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", bag_dt_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", bag_dt_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd8a36ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                               Without FS Comparison                                               </span>\n",
       "\n",
       "<span style=\"font-weight: bold\"> Algorithm          </span><span style=\"font-weight: bold\"> Type Acc </span><span style=\"font-weight: bold\"> K-Fold Mean </span><span style=\"font-weight: bold\"> K-Fold Std </span><span style=\"font-weight: bold\"> cvss_score Acc </span><span style=\"font-weight: bold\"> K-Fold Mean </span><span style=\"font-weight: bold\"> K-Fold Std </span><span style=\"font-weight: bold\"> Combined </span>\n",
       "\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">GradientBoosting</span>    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.96</span>      <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.93</span>         <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.01</span>        <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.68</span>            <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.56</span>         <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.01</span>            <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.82</span> \n",
       "\n",
       " LightGBM            0.95      0.94         0.01        0.68            0.56         0.02            0.82 \n",
       "\n",
       " HistGradientBoost  0.95      0.93         0.01        0.68            0.55         0.01            0.81 \n",
       "\n",
       " Stacking            0.93      0.88         0.01        0.69            0.57         0.02            0.81 \n",
       "\n",
       " Bagged DT           0.95      0.92         0.01        0.67            0.55         0.01            0.81 \n",
       "\n",
       " RandomForest        0.92      0.85         0.00        0.68            0.56         0.01            0.80 \n",
       "\n",
       " ExtraTrees          0.91      0.85         0.01        0.68            0.56         0.01            0.80 \n",
       "\n",
       " Bagging             0.94      0.92         0.01        0.65            0.54         0.01            0.80 \n",
       "\n",
       " Hard Voting         0.91      0.85         0.01        0.68            0.55         0.00            0.80 \n",
       "\n",
       " Soft Voting         0.91      0.84         0.00        0.68            0.53         0.00            0.79 \n",
       "\n",
       " DecisionTree        0.93      0.89         0.01        0.59            0.48         0.01            0.76 \n",
       "\n",
       " SVM                 0.85      0.75         0.01        0.63            0.35         0.00            0.74 \n",
       "\n",
       " Bagged KNN          0.81      0.67         0.01        0.62            0.48         0.02            0.72 \n",
       "\n",
       " KNN                 0.81      0.70         0.01        0.60            0.48         0.00            0.71 \n",
       "\n",
       " AdaBoost            0.81      0.69         0.04        0.59            0.32         0.00            0.70 \n",
       "\n",
       " <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">LogisticRegression</span>  <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.78</span>      <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.68</span>         <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.00</span>        <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.59</span>            <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.31</span>         <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.01</span>            <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.69</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                               Without FS Comparison                                               \u001b[0m\n",
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mAlgorithm         \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mType Acc\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mK-Fold Mean\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mK-Fold Std\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mcvss_score Acc\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mK-Fold Mean\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mK-Fold Std\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mCombined\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " \u001b[1;32mGradientBoosting\u001b[0m    \u001b[1;32m0.96\u001b[0m      \u001b[1;32m0.93\u001b[0m         \u001b[1;32m0.01\u001b[0m        \u001b[1;32m0.68\u001b[0m            \u001b[1;32m0.56\u001b[0m         \u001b[1;32m0.01\u001b[0m            \u001b[1;32m0.82\u001b[0m \n",
       "\n",
       " LightGBM            0.95      0.94         0.01        0.68            0.56         0.02            0.82 \n",
       "\n",
       " HistGradientBoost  0.95      0.93         0.01        0.68            0.55         0.01            0.81 \n",
       "\n",
       " Stacking            0.93      0.88         0.01        0.69            0.57         0.02            0.81 \n",
       "\n",
       " Bagged DT           0.95      0.92         0.01        0.67            0.55         0.01            0.81 \n",
       "\n",
       " RandomForest        0.92      0.85         0.00        0.68            0.56         0.01            0.80 \n",
       "\n",
       " ExtraTrees          0.91      0.85         0.01        0.68            0.56         0.01            0.80 \n",
       "\n",
       " Bagging             0.94      0.92         0.01        0.65            0.54         0.01            0.80 \n",
       "\n",
       " Hard Voting         0.91      0.85         0.01        0.68            0.55         0.00            0.80 \n",
       "\n",
       " Soft Voting         0.91      0.84         0.00        0.68            0.53         0.00            0.79 \n",
       "\n",
       " DecisionTree        0.93      0.89         0.01        0.59            0.48         0.01            0.76 \n",
       "\n",
       " SVM                 0.85      0.75         0.01        0.63            0.35         0.00            0.74 \n",
       "\n",
       " Bagged KNN          0.81      0.67         0.01        0.62            0.48         0.02            0.72 \n",
       "\n",
       " KNN                 0.81      0.70         0.01        0.60            0.48         0.00            0.71 \n",
       "\n",
       " AdaBoost            0.81      0.69         0.04        0.59            0.32         0.00            0.70 \n",
       "\n",
       " \u001b[1;31mLogisticRegression\u001b[0m  \u001b[1;31m0.78\u001b[0m      \u001b[1;31m0.68\u001b[0m         \u001b[1;31m0.00\u001b[0m        \u001b[1;31m0.59\u001b[0m            \u001b[1;31m0.31\u001b[0m         \u001b[1;31m0.01\u001b[0m            \u001b[1;31m0.69\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.table import Table\n",
    "from rich.console import Console\n",
    "\n",
    "console = Console()\n",
    "\n",
    "results = [\n",
    "    ['LogisticRegression', lr_accuracy_type, lr_scores_type.mean(), lr_scores_type.std(), lr_accuracy_cvss_score, lr_scores_cvss_score.mean(), lr_scores_cvss_score.std()],\n",
    "    ['DecisionTree', dt_accuracy_type, dt_scores_type.mean(), dt_scores_type.std(), dt_accuracy_cvss_score, dt_scores_cvss_score.mean(), dt_scores_cvss_score.std()],\n",
    "    ['RandomForest', rf_accuracy_type, rf_scores_type.mean(), rf_scores_type.std(), rf_accuracy_cvss_score, rf_scores_cvss_score.mean(), rf_scores_cvss_score.std()],\n",
    "    ['ExtraTrees', et_accuracy_type, et_scores_type.mean(), et_scores_type.std(), et_accuracy_cvss_score, et_scores_cvss_score.mean(), et_scores_cvss_score.std()],\n",
    "    ['GradientBoosting', gb_accuracy_type, gb_scores_type.mean(), gb_scores_type.std(), gb_accuracy_cvss_score, gb_scores_cvss_score.mean(), gb_scores_cvss_score.std()],\n",
    "    ['HistGradientBoosting', hgb_accuracy_type, hgb_scores_type.mean(), hgb_scores_type.std(), hgb_accuracy_cvss_score, hgb_scores_cvss_score.mean(), hgb_scores_cvss_score.std()],\n",
    "    ['KNN', knn_accuracy_type, knn_scores_type.mean(), knn_scores_type.std(), knn_accuracy_cvss_score, knn_scores_cvss_score.mean(), knn_scores_cvss_score.std()],\n",
    "    ['AdaBoost', ab_accuracy_type, ab_scores_type.mean(), ab_scores_type.std(), ab_accuracy_cvss_score, ab_scores_cvss_score.mean(), ab_scores_cvss_score.std()],\n",
    "    ['LightGBM', lgbm_accuracy_type, lgbm_scores_type.mean(), lgbm_scores_type.std(), lgbm_accuracy_cvss_score, lgbm_scores_cvss_score.mean(), lgbm_scores_cvss_score.std()],\n",
    "    ['Bagging', bag_accuracy_type, bag_scores_type.mean(), bag_scores_type.std(), bag_accuracy_cvss_score, bag_scores_cvss_score.mean(), bag_scores_cvss_score.std()],\n",
    "    ['Hard Voting', hard_acc_type, hard_scores_type.mean(), hard_scores_type.std(), hard_acc_cvss_score, hard_scores_cvss_score.mean(), hard_scores_cvss_score.std()],\n",
    "    ['Soft Voting', soft_acc_type, soft_scores_type.mean(), soft_scores_type.std(), soft_acc_cvss_score, soft_scores_cvss_score.mean(), soft_scores_cvss_score.std()],\n",
    "    ['Stacking', stacking_acc_type, stacking_scores_type.mean(), stacking_scores_type.std(), stacking_acc_cvss_score, stacking_scores_cvss_score.mean(), stacking_scores_cvss_score.std()],\n",
    "    ['SVM', svc_accuracy_type, svc_scores_type.mean(), svc_scores_type.std(), svc_accuracy_cvss_score, svc_scores_cvss_score.mean(), svc_scores_cvss_score.std()],\n",
    "    ['Bagged KNN', bag_knn_accuracy_type, bag_knn_scores_type.mean(), bag_knn_scores_type.std(), bag_knn_accuracy_cvss_score, bag_knn_scores_cvss_score.mean(), bag_knn_scores_cvss_score.std()],\n",
    "    ['Bagged DT', bag_dt_accuracy_type, bag_dt_scores_type.mean(), bag_dt_scores_type.std(), bag_dt_accuracy_cvss_score, bag_dt_scores_cvss_score.mean(), bag_dt_scores_cvss_score.std()],\n",
    "]\n",
    "\n",
    "for row in results:\n",
    "    type_acc = row[1]\n",
    "    cvss_score_acc = row[4]\n",
    "    combined = (type_acc + cvss_score_acc) / 2\n",
    "    row.append(combined)\n",
    "\n",
    "result_sorted = sorted(results, key=lambda i: i[-1], reverse=True)\n",
    "\n",
    "best_model = max(results, key=lambda x: x[-1])\n",
    "worst_model = min(results, key=lambda x: x[-1])\n",
    "\n",
    "table = Table(title=\"Without FS Comparison\", show_lines=True)\n",
    "table.add_column(\"Algorithm\")\n",
    "table.add_column(\"Type Acc\")\n",
    "table.add_column(\"K-Fold Mean\")\n",
    "table.add_column(\"K-Fold Std\")\n",
    "table.add_column(\"cvss_score Acc\")\n",
    "table.add_column(\"K-Fold Mean\")\n",
    "table.add_column(\"K-Fold Std\")\n",
    "table.add_column(\"Combined\", justify=\"right\")\n",
    "\n",
    "for row in result_sorted:\n",
    "    algo, type_acc, kmean_type, kstd_type, cvss_score_acc, kmean_cvss_score, kstd_cvss_score, combined = row\n",
    "\n",
    "    if row == best_model:\n",
    "        table.add_row(\n",
    "            f\"[bold green]{algo}[/bold green]\",\n",
    "            f\"[bold green]{type_acc:.2f}[/bold green]\",\n",
    "            f\"[bold green]{kmean_type:.2f}[/bold green]\",\n",
    "            f\"[bold green]{kstd_type:.2f}[/bold green]\",\n",
    "            f\"[bold green]{cvss_score_acc:.2f}[/bold green]\",\n",
    "            f\"[bold green]{kmean_cvss_score:.2f}[/bold green]\",\n",
    "            f\"[bold green]{kstd_cvss_score:.2f}[/bold green]\",\n",
    "            f\"[bold green]{combined:.2f}[/bold green]\",\n",
    "        )\n",
    "    elif row == worst_model:\n",
    "        table.add_row(\n",
    "            f\"[bold red]{algo}[/bold red]\",\n",
    "            f\"[bold red]{type_acc:.2f}[/bold red]\",\n",
    "            f\"[bold red]{kmean_type:.2f}[/bold red]\",\n",
    "            f\"[bold red]{kstd_type:.2f}[/bold red]\",\n",
    "            f\"[bold red]{cvss_score_acc:.2f}[/bold red]\",\n",
    "            f\"[bold red]{kmean_cvss_score:.2f}[/bold red]\",\n",
    "            f\"[bold red]{kstd_cvss_score:.2f}[/bold red]\",\n",
    "            f\"[bold red]{combined:.2f}[/bold red]\",\n",
    "        )\n",
    "    else:\n",
    "        table.add_row(\n",
    "            algo, f\"{type_acc:.2f}\", f\"{kmean_type:.2f}\", f\"{kstd_type:.2f}\",\n",
    "            f\"{cvss_score_acc:.2f}\", f\"{kmean_cvss_score:.2f}\", f\"{kstd_cvss_score:.2f}\", f\"{combined:.2f}\"\n",
    "        )\n",
    "\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e5ce4b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                               Without FS Comparison                                               </span>\n",
       "\n",
       "<span style=\"font-weight: bold\"> Algorithm          </span><span style=\"font-weight: bold\"> Type Acc </span><span style=\"font-weight: bold\"> K-Fold Mean </span><span style=\"font-weight: bold\"> K-Fold Std </span><span style=\"font-weight: bold\"> cvss_score Acc </span><span style=\"font-weight: bold\"> K-Fold Mean </span><span style=\"font-weight: bold\"> K-Fold Std </span><span style=\"font-weight: bold\"> Combined </span>\n",
       "\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">GradientBoosting</span>    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.96</span>      <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.93</span>         <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.01</span>        <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.68</span>            <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.56</span>         <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.01</span>            <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.82</span> \n",
       "\n",
       " LightGBM            0.95      0.94         0.01        0.68            0.56         0.02            0.82 \n",
       "\n",
       " HistGradientBoost  0.95      0.93         0.01        0.68            0.55         0.01            0.81 \n",
       "\n",
       " Stacking            0.93      0.88         0.01        0.69            0.57         0.02            0.81 \n",
       "\n",
       " Bagged DT           0.95      0.92         0.01        0.67            0.55         0.01            0.81 \n",
       "\n",
       " RandomForest        0.92      0.85         0.00        0.68            0.56         0.01            0.80 \n",
       "\n",
       " ExtraTrees          0.91      0.85         0.01        0.68            0.56         0.01            0.80 \n",
       "\n",
       " Bagging             0.94      0.92         0.01        0.65            0.54         0.01            0.80 \n",
       "\n",
       " Hard Voting         0.91      0.85         0.01        0.68            0.55         0.00            0.80 \n",
       "\n",
       " Soft Voting         0.91      0.84         0.00        0.68            0.53         0.00            0.79 \n",
       "\n",
       " DecisionTree        0.93      0.89         0.01        0.59            0.48         0.01            0.76 \n",
       "\n",
       " SVM                 0.85      0.75         0.01        0.63            0.35         0.00            0.74 \n",
       "\n",
       " Bagged KNN          0.81      0.67         0.01        0.62            0.48         0.02            0.72 \n",
       "\n",
       " KNN                 0.81      0.70         0.01        0.60            0.48         0.00            0.71 \n",
       "\n",
       " AdaBoost            0.81      0.69         0.04        0.59            0.32         0.00            0.70 \n",
       "\n",
       " <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">LogisticRegression</span>  <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.78</span>      <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.68</span>         <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.00</span>        <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.59</span>            <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.31</span>         <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.01</span>            <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.69</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                               Without FS Comparison                                               \u001b[0m\n",
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mAlgorithm         \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mType Acc\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mK-Fold Mean\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mK-Fold Std\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mcvss_score Acc\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mK-Fold Mean\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mK-Fold Std\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mCombined\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " \u001b[1;32mGradientBoosting\u001b[0m    \u001b[1;32m0.96\u001b[0m      \u001b[1;32m0.93\u001b[0m         \u001b[1;32m0.01\u001b[0m        \u001b[1;32m0.68\u001b[0m            \u001b[1;32m0.56\u001b[0m         \u001b[1;32m0.01\u001b[0m            \u001b[1;32m0.82\u001b[0m \n",
       "\n",
       " LightGBM            0.95      0.94         0.01        0.68            0.56         0.02            0.82 \n",
       "\n",
       " HistGradientBoost  0.95      0.93         0.01        0.68            0.55         0.01            0.81 \n",
       "\n",
       " Stacking            0.93      0.88         0.01        0.69            0.57         0.02            0.81 \n",
       "\n",
       " Bagged DT           0.95      0.92         0.01        0.67            0.55         0.01            0.81 \n",
       "\n",
       " RandomForest        0.92      0.85         0.00        0.68            0.56         0.01            0.80 \n",
       "\n",
       " ExtraTrees          0.91      0.85         0.01        0.68            0.56         0.01            0.80 \n",
       "\n",
       " Bagging             0.94      0.92         0.01        0.65            0.54         0.01            0.80 \n",
       "\n",
       " Hard Voting         0.91      0.85         0.01        0.68            0.55         0.00            0.80 \n",
       "\n",
       " Soft Voting         0.91      0.84         0.00        0.68            0.53         0.00            0.79 \n",
       "\n",
       " DecisionTree        0.93      0.89         0.01        0.59            0.48         0.01            0.76 \n",
       "\n",
       " SVM                 0.85      0.75         0.01        0.63            0.35         0.00            0.74 \n",
       "\n",
       " Bagged KNN          0.81      0.67         0.01        0.62            0.48         0.02            0.72 \n",
       "\n",
       " KNN                 0.81      0.70         0.01        0.60            0.48         0.00            0.71 \n",
       "\n",
       " AdaBoost            0.81      0.69         0.04        0.59            0.32         0.00            0.70 \n",
       "\n",
       " \u001b[1;31mLogisticRegression\u001b[0m  \u001b[1;31m0.78\u001b[0m      \u001b[1;31m0.68\u001b[0m         \u001b[1;31m0.00\u001b[0m        \u001b[1;31m0.59\u001b[0m            \u001b[1;31m0.31\u001b[0m         \u001b[1;31m0.01\u001b[0m            \u001b[1;31m0.69\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "temp_console = Console(record=True)\n",
    "temp_console.print(table)\n",
    "text = temp_console.export_text()\n",
    "with open('results/feature_selection_compare.txt', 'a', encoding='utf-8') as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d723a6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
