{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9ccea25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a99f5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\SML_Projects\\SML_CVE_type_cwe_predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2060fdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/preprocessed/preprocessed_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc93a7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 102963 entries, 0 to 102962\n",
      "Data columns (total 29 columns):\n",
      " #   Column                      Non-Null Count   Dtype  \n",
      "---  ------                      --------------   -----  \n",
      " 0   cve_id                      102963 non-null  float64\n",
      " 1   description                 102963 non-null  float64\n",
      " 2   cvss_score                  102963 non-null  float64\n",
      " 3   cwe                         102963 non-null  float64\n",
      " 4   vendor                      102963 non-null  float64\n",
      " 5   product                     102963 non-null  float64\n",
      " 6   publish_date                102963 non-null  float64\n",
      " 7   type                        102963 non-null  float64\n",
      " 8   vendor_freq                 102963 non-null  float64\n",
      " 9   product_freq                102963 non-null  float64\n",
      " 10  desc_len                    102963 non-null  float64\n",
      " 11  desc_word_count             102963 non-null  float64\n",
      " 12  desc_num_count              102963 non-null  float64\n",
      " 13  desc_upper_ratio            102963 non-null  float64\n",
      " 14  desc_exclamation            102963 non-null  float64\n",
      " 15  desc_question               102963 non-null  float64\n",
      " 16  vendor_product_interaction  102963 non-null  float64\n",
      " 17  XSS_score                   102963 non-null  float64\n",
      " 18  SQLi_score                  102963 non-null  float64\n",
      " 19  RCE_score                   102963 non-null  float64\n",
      " 20  DoS_score                   102963 non-null  float64\n",
      " 21  CSRF_score                  102963 non-null  float64\n",
      " 22  AuthBypass_score            102963 non-null  float64\n",
      " 23  PrivEsc_score               102963 non-null  float64\n",
      " 24  PathTraversal_score         102963 non-null  float64\n",
      " 25  SSRF_score                  102963 non-null  float64\n",
      " 26  InfoDisclosure_score        102963 non-null  float64\n",
      " 27  Other_score                 102963 non-null  float64\n",
      " 28  cvss_keywords_score         102963 non-null  float64\n",
      "dtypes: float64(29)\n",
      "memory usage: 22.8 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f484f6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=0.1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "050c8a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10296 entries, 0 to 10295\n",
      "Data columns (total 29 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   cve_id                      10296 non-null  float64\n",
      " 1   description                 10296 non-null  float64\n",
      " 2   cvss_score                  10296 non-null  float64\n",
      " 3   cwe                         10296 non-null  float64\n",
      " 4   vendor                      10296 non-null  float64\n",
      " 5   product                     10296 non-null  float64\n",
      " 6   publish_date                10296 non-null  float64\n",
      " 7   type                        10296 non-null  float64\n",
      " 8   vendor_freq                 10296 non-null  float64\n",
      " 9   product_freq                10296 non-null  float64\n",
      " 10  desc_len                    10296 non-null  float64\n",
      " 11  desc_word_count             10296 non-null  float64\n",
      " 12  desc_num_count              10296 non-null  float64\n",
      " 13  desc_upper_ratio            10296 non-null  float64\n",
      " 14  desc_exclamation            10296 non-null  float64\n",
      " 15  desc_question               10296 non-null  float64\n",
      " 16  vendor_product_interaction  10296 non-null  float64\n",
      " 17  XSS_score                   10296 non-null  float64\n",
      " 18  SQLi_score                  10296 non-null  float64\n",
      " 19  RCE_score                   10296 non-null  float64\n",
      " 20  DoS_score                   10296 non-null  float64\n",
      " 21  CSRF_score                  10296 non-null  float64\n",
      " 22  AuthBypass_score            10296 non-null  float64\n",
      " 23  PrivEsc_score               10296 non-null  float64\n",
      " 24  PathTraversal_score         10296 non-null  float64\n",
      " 25  SSRF_score                  10296 non-null  float64\n",
      " 26  InfoDisclosure_score        10296 non-null  float64\n",
      " 27  Other_score                 10296 non-null  float64\n",
      " 28  cvss_keywords_score         10296 non-null  float64\n",
      "dtypes: float64(29)\n",
      "memory usage: 2.3 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0482c6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(['type', 'cvss_score'], axis=1)   \n",
    "y = df[['type', 'cvss_score']] \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44fffd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c5411f",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f7a20d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy for 'type': 0.7490291262135922\n",
      "Logistic Regression Accuracy for 'cvss_score' : 0.5859223300970874\n",
      "K-Fold mean F1 (type): 0.5915255055022438\n",
      "K-Fold std  F1 (type): 0.0077130697323266615\n",
      "K-Fold mean F1 (cvss_score): 0.28092874200716555\n",
      "K-Fold std  F1 (cvss_score): 0.0032352530956103287\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        28\n",
      "         1.0       0.91      0.80      0.85        50\n",
      "         2.0       0.81      0.92      0.86       103\n",
      "         3.0       0.02      0.01      0.01       200\n",
      "         4.0       0.67      0.83      0.74       834\n",
      "         5.0       0.90      0.68      0.78        41\n",
      "         6.0       0.41      0.47      0.44        36\n",
      "         7.0       0.84      0.61      0.70       155\n",
      "         8.0       1.00      0.98      0.99       176\n",
      "         9.0       0.00      0.00      0.00        14\n",
      "        10.0       0.95      0.95      0.95       423\n",
      "\n",
      "    accuracy                           0.75      2060\n",
      "   macro avg       0.59      0.57      0.58      2060\n",
      "weighted avg       0.71      0.75      0.72      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       234\n",
      "         1.0       0.47      0.35      0.40       637\n",
      "         2.0       0.00      0.00      0.00        87\n",
      "         3.0       0.62      0.89      0.73      1102\n",
      "\n",
      "    accuracy                           0.59      2060\n",
      "   macro avg       0.27      0.31      0.28      2060\n",
      "weighted avg       0.48      0.59      0.52      2060\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "multi_lr = MultiOutputClassifier(lr)\n",
    "\n",
    "multi_lr.fit(x_train, y_train)\n",
    "\n",
    "importances = np.mean([np.mean(np.abs(est.coef_), axis=0) for est in multi_lr.estimators_], axis=0)\n",
    "\n",
    "selected_features = x_train.columns[importances > np.mean(importances)]\n",
    "\n",
    "multi_lr.fit(x_train[selected_features], y_train)\n",
    "y_pred = multi_lr.predict(x_test[selected_features])\n",
    "\n",
    "lr_accuracy_type = accuracy_score(y_test['type'], y_pred[:, 0])\n",
    "lr_accuracy_cvss_score  = accuracy_score(y_test['cvss_score'],  y_pred[:, 1])\n",
    "\n",
    "lr_scores_type = cross_val_score(lr, x[selected_features], y['type'], cv=kf, scoring='f1_macro')\n",
    "lr_scores_cvss_score  = cross_val_score(lr, x[selected_features], y['cvss_score'],  cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"Logistic Regression Accuracy for 'type':\", lr_accuracy_type)\n",
    "print(\"Logistic Regression Accuracy for 'cvss_score' :\", lr_accuracy_cvss_score)\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", lr_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", lr_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", lr_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", lr_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:, 0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'],  y_pred[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989b2e97",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f907ea9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy for 'type': 0.9004854368932039\n",
      "Decision Tree Accuracy for 'cvss_score' : 0.5912621359223301\n",
      "\n",
      "K-Fold mean F1 (type): 0.8406609308704635\n",
      "K-Fold std  F1 (type): 0.009677579359983248\n",
      "\n",
      "K-Fold mean F1 (cvss_score): 0.46141262424894536\n",
      "K-Fold std  F1 (cvss_score): 0.0135999436186787\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.71      0.73        28\n",
      "         1.0       0.98      0.86      0.91        50\n",
      "         2.0       0.81      0.82      0.81       103\n",
      "         3.0       0.85      0.83      0.84       200\n",
      "         4.0       0.92      0.91      0.91       834\n",
      "         5.0       0.83      0.93      0.87        41\n",
      "         6.0       0.49      0.53      0.51        36\n",
      "         7.0       0.85      0.86      0.85       155\n",
      "         8.0       0.98      1.00      0.99       176\n",
      "         9.0       0.87      0.93      0.90        14\n",
      "        10.0       0.95      0.95      0.95       423\n",
      "\n",
      "    accuracy                           0.90      2060\n",
      "   macro avg       0.84      0.85      0.84      2060\n",
      "weighted avg       0.90      0.90      0.90      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.32      0.28      0.30       234\n",
      "         1.0       0.50      0.50      0.50       637\n",
      "         2.0       0.38      0.43      0.40        87\n",
      "         3.0       0.71      0.72      0.72      1102\n",
      "\n",
      "    accuracy                           0.59      2060\n",
      "   macro avg       0.48      0.48      0.48      2060\n",
      "weighted avg       0.59      0.59      0.59      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "multi_dt = MultiOutputClassifier(dt)\n",
    "\n",
    "multi_dt.fit(x_train, y_train)\n",
    "\n",
    "importances = np.mean([est.feature_importances_ for est in multi_dt.estimators_], axis=0)\n",
    "selected_features = x_train.columns[importances > np.mean(importances)]\n",
    "\n",
    "multi_dt.fit(x_train[selected_features], y_train)\n",
    "y_pred = multi_dt.predict(x_test[selected_features])\n",
    "\n",
    "dt_accuracy_type = accuracy_score(y_test['type'], y_pred[:, 0])\n",
    "dt_accuracy_cvss_score  = accuracy_score(y_test['cvss_score'], y_pred[:, 1])\n",
    "\n",
    "dt_scores_type = cross_val_score(dt, x[selected_features], y['type'], cv=kf, scoring='f1_macro')\n",
    "dt_scores_cvss_score  = cross_val_score(dt, x[selected_features], y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"Decision Tree Accuracy for 'type':\", dt_accuracy_type)\n",
    "print(\"Decision Tree Accuracy for 'cvss_score' :\", dt_accuracy_cvss_score)\n",
    "\n",
    "print(\"\\nK-Fold mean F1 (type):\", dt_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", dt_scores_type.std())\n",
    "\n",
    "print(\"\\nK-Fold mean F1 (cvss_score):\", dt_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", dt_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f878bcc1",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4924ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy for 'type': 0.8776699029126214\n",
      "Random Forest Accuracy for 'cvss_score' : 0.6815533980582524\n",
      "K-Fold mean F1 (type): 0.6786973747481492\n",
      "K-Fold std  F1 (type): 0.007657113318103228\n",
      "K-Fold mean F1 (cvss_score): 0.5442442506227819\n",
      "K-Fold std  F1 (cvss_score): 0.007055799496073992\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.07      0.13        28\n",
      "         1.0       0.98      0.82      0.89        50\n",
      "         2.0       0.78      0.46      0.58       103\n",
      "         3.0       0.77      0.89      0.82       200\n",
      "         4.0       0.84      0.96      0.90       834\n",
      "         5.0       0.93      0.63      0.75        41\n",
      "         6.0       0.75      0.08      0.15        36\n",
      "         7.0       0.89      0.83      0.86       155\n",
      "         8.0       1.00      0.99      0.99       176\n",
      "         9.0       1.00      0.36      0.53        14\n",
      "        10.0       0.97      0.95      0.96       423\n",
      "\n",
      "    accuracy                           0.88      2060\n",
      "   macro avg       0.90      0.64      0.69      2060\n",
      "weighted avg       0.88      0.88      0.86      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.26      0.36       234\n",
      "         1.0       0.60      0.55      0.57       637\n",
      "         2.0       0.88      0.40      0.55        87\n",
      "         3.0       0.72      0.87      0.79      1102\n",
      "\n",
      "    accuracy                           0.68      2060\n",
      "   macro avg       0.70      0.52      0.57      2060\n",
      "weighted avg       0.67      0.68      0.66      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "multi_rf = MultiOutputClassifier(rf)\n",
    "\n",
    "multi_rf.fit(x_train, y_train)\n",
    "\n",
    "importances = np.mean([est.feature_importances_ for est in multi_rf.estimators_], axis=0)\n",
    "selected_features = x_train.columns[importances > np.mean(importances)]\n",
    "\n",
    "multi_rf.fit(x_train[selected_features], y_train)\n",
    "y_pred = multi_rf.predict(x_test[selected_features])\n",
    "\n",
    "rf_accuracy_type = accuracy_score(y_test['type'], y_pred[:, 0])\n",
    "rf_accuracy_cvss_score  = accuracy_score(y_test['cvss_score'], y_pred[:, 1])\n",
    "\n",
    "rf_scores_type = cross_val_score(rf, x[selected_features], y['type'], cv=kf, scoring='f1_macro')\n",
    "rf_scores_cvss_score  = cross_val_score(rf, x[selected_features], y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"Random Forest Accuracy for 'type':\", rf_accuracy_type)\n",
    "print(\"Random Forest Accuracy for 'cvss_score' :\", rf_accuracy_cvss_score)\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", rf_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", rf_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", rf_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", rf_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f5574d",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ecbb122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Accuracy for 'type': 0.9296116504854369\n",
      "Gradient Boosting Accuracy for 'cvss_score' : 0.6480582524271845\n",
      "K-Fold mean F1 (type): 0.8832641900418396\n",
      "K-Fold std  F1 (type): 0.006370413942588808\n",
      "K-Fold mean F1 (cvss_score): 0.518344292076997\n",
      "K-Fold std  F1 (cvss_score): 0.006949892293444006\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.79      0.83        28\n",
      "         1.0       0.98      0.84      0.90        50\n",
      "         2.0       0.82      0.96      0.88       103\n",
      "         3.0       0.93      0.89      0.91       200\n",
      "         4.0       0.92      0.94      0.93       834\n",
      "         5.0       0.95      0.93      0.94        41\n",
      "         6.0       0.83      0.56      0.67        36\n",
      "         7.0       0.89      0.85      0.86       155\n",
      "         8.0       0.99      0.98      0.99       176\n",
      "         9.0       0.88      1.00      0.93        14\n",
      "        10.0       0.97      0.99      0.98       423\n",
      "\n",
      "    accuracy                           0.93      2060\n",
      "   macro avg       0.91      0.88      0.89      2060\n",
      "weighted avg       0.93      0.93      0.93      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.28      0.38       234\n",
      "         1.0       0.56      0.42      0.48       637\n",
      "         2.0       0.85      0.39      0.54        87\n",
      "         3.0       0.68      0.88      0.76      1102\n",
      "\n",
      "    accuracy                           0.65      2060\n",
      "   macro avg       0.67      0.49      0.54      2060\n",
      "weighted avg       0.64      0.65      0.62      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "multi_gbr = MultiOutputClassifier(gb)\n",
    "\n",
    "multi_gbr.fit(x_train, y_train)\n",
    "\n",
    "importances = np.mean([est.feature_importances_ for est in multi_gbr.estimators_], axis=0)\n",
    "selected_features = x_train.columns[importances > np.mean(importances)]\n",
    "\n",
    "multi_gbr.fit(x_train[selected_features], y_train)\n",
    "y_pred = multi_gbr.predict(x_test[selected_features])\n",
    "\n",
    "gb_accuracy_type = accuracy_score(y_test['type'], y_pred[:, 0])\n",
    "gb_accuracy_cvss_score  = accuracy_score(y_test['cvss_score'], y_pred[:, 1])\n",
    "\n",
    "gb_scores_type = cross_val_score(gb, x[selected_features], y['type'], cv=kf, scoring='f1_macro')\n",
    "gb_scores_cvss_score  = cross_val_score(gb, x[selected_features], y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"Gradient Boosting Accuracy for 'type':\", gb_accuracy_type)\n",
    "print(\"Gradient Boosting Accuracy for 'cvss_score' :\", gb_accuracy_cvss_score)\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", gb_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", gb_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", gb_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", gb_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a44274c",
   "metadata": {},
   "source": [
    "# Extra Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e8d1f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees Accuracy for 'type': 0.8844660194174757\n",
      "Extra Trees Accuracy for 'cvss_score' : 0.6747572815533981\n",
      "K-Fold mean F1 (type): 0.7035314423420451\n",
      "K-Fold std  F1 (type): 0.014804209292476082\n",
      "K-Fold mean F1 (cvss_score): 0.5506485639942478\n",
      "K-Fold std  F1 (cvss_score): 0.017242510035925297\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.11      0.19        28\n",
      "         1.0       0.98      0.80      0.88        50\n",
      "         2.0       0.82      0.95      0.88       103\n",
      "         3.0       0.75      0.79      0.77       200\n",
      "         4.0       0.86      0.95      0.91       834\n",
      "         5.0       0.92      0.59      0.72        41\n",
      "         6.0       0.89      0.22      0.36        36\n",
      "         7.0       0.87      0.81      0.84       155\n",
      "         8.0       0.99      0.99      0.99       176\n",
      "         9.0       1.00      0.07      0.13        14\n",
      "        10.0       0.97      0.94      0.96       423\n",
      "\n",
      "    accuracy                           0.88      2060\n",
      "   macro avg       0.89      0.66      0.69      2060\n",
      "weighted avg       0.89      0.88      0.87      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.25      0.35       234\n",
      "         1.0       0.60      0.56      0.58       637\n",
      "         2.0       0.85      0.45      0.59        87\n",
      "         3.0       0.71      0.85      0.78      1102\n",
      "\n",
      "    accuracy                           0.67      2060\n",
      "   macro avg       0.68      0.53      0.57      2060\n",
      "weighted avg       0.66      0.67      0.66      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "et = ExtraTreesClassifier(random_state=42)\n",
    "multi_et = MultiOutputClassifier(et)\n",
    "\n",
    "multi_et.fit(x_train, y_train)\n",
    "\n",
    "importances = np.mean([est.feature_importances_ for est in multi_et.estimators_], axis=0)\n",
    "selected_features = x_train.columns[importances > np.mean(importances)]\n",
    "\n",
    "multi_et.fit(x_train[selected_features], y_train)\n",
    "y_pred = multi_et.predict(x_test[selected_features])\n",
    "\n",
    "et_accuracy_type = accuracy_score(y_test['type'], y_pred[:, 0])\n",
    "et_accuracy_cvss_score  = accuracy_score(y_test['cvss_score'], y_pred[:, 1])\n",
    "\n",
    "et_scores_type = cross_val_score(et, x[selected_features], y['type'], cv=kf, scoring='f1_macro')\n",
    "et_scores_cvss_score  = cross_val_score(et, x[selected_features], y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"Extra Trees Accuracy for 'type':\", et_accuracy_type)\n",
    "print(\"Extra Trees Accuracy for 'cvss_score' :\", et_accuracy_cvss_score)\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", et_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", et_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", et_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", et_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5839ed79",
   "metadata": {},
   "source": [
    "# Hist Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0a17a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HistGradientBoosting Accuracy for 'type': 0.9495145631067962\n",
      "HistGradientBoosting Accuracy for 'cvss_score' : 0.6805825242718446\n",
      "K-Fold mean F1 (type): 0.859650400572609\n",
      "K-Fold std  F1 (type): 0.0005043169989493918\n",
      "K-Fold mean F1 (cvss_score): 0.5529562162823665\n",
      "K-Fold std  F1 (cvss_score): 0.012298547389334093\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.89      0.94        28\n",
      "         1.0       0.98      0.96      0.97        50\n",
      "         2.0       0.85      0.91      0.88       103\n",
      "         3.0       0.94      0.93      0.94       200\n",
      "         4.0       0.95      0.96      0.95       834\n",
      "         5.0       0.95      0.93      0.94        41\n",
      "         6.0       0.82      0.89      0.85        36\n",
      "         7.0       0.90      0.84      0.87       155\n",
      "         8.0       0.99      0.99      0.99       176\n",
      "         9.0       1.00      1.00      1.00        14\n",
      "        10.0       0.99      0.98      0.98       423\n",
      "\n",
      "    accuracy                           0.95      2060\n",
      "   macro avg       0.94      0.93      0.94      2060\n",
      "weighted avg       0.95      0.95      0.95      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.31      0.40       234\n",
      "         1.0       0.60      0.54      0.57       637\n",
      "         2.0       0.83      0.39      0.53        87\n",
      "         3.0       0.72      0.86      0.79      1102\n",
      "\n",
      "    accuracy                           0.68      2060\n",
      "   macro avg       0.68      0.53      0.57      2060\n",
      "weighted avg       0.67      0.68      0.66      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "hgb = HistGradientBoostingClassifier(random_state=42)\n",
    "multi_hgb = MultiOutputClassifier(hgb)\n",
    "\n",
    "multi_hgb.fit(x_train, y_train)\n",
    "y_pred = multi_hgb.predict(x_test)\n",
    "\n",
    "hgb_accuracy_type = accuracy_score(y_test['type'], y_pred[:, 0])\n",
    "hgb_accuracy_cvss_score  = accuracy_score(y_test['cvss_score'], y_pred[:, 1])\n",
    "\n",
    "hgb_scores_type = cross_val_score(hgb, x[selected_features], y['type'], cv=kf, scoring='f1_macro')\n",
    "hgb_scores_cvss_score  = cross_val_score(hgb, x[selected_features], y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"HistGradientBoosting Accuracy for 'type':\", hgb_accuracy_type)\n",
    "print(\"HistGradientBoosting Accuracy for 'cvss_score' :\", hgb_accuracy_cvss_score)\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", hgb_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", hgb_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", hgb_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", hgb_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe251b6",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c789a957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy for 'type': 0.8004854368932038\n",
      "KNN Accuracy for 'cvss_score' : 0.6121359223300971\n",
      "K-Fold mean F1 (type): 0.5419164462924732\n",
      "K-Fold std  F1 (type): 0.002867879009922753\n",
      "K-Fold mean F1 (cvss_score): 0.4973094519432144\n",
      "K-Fold std  F1 (cvss_score): 0.011297243174919617\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.04      0.07        28\n",
      "         1.0       0.83      0.78      0.80        50\n",
      "         2.0       0.85      0.67      0.75       103\n",
      "         3.0       0.55      0.66      0.60       200\n",
      "         4.0       0.76      0.89      0.82       834\n",
      "         5.0       0.96      0.56      0.71        41\n",
      "         6.0       0.71      0.14      0.23        36\n",
      "         7.0       0.88      0.61      0.72       155\n",
      "         8.0       1.00      0.94      0.97       176\n",
      "         9.0       1.00      0.21      0.35        14\n",
      "        10.0       0.93      0.89      0.91       423\n",
      "\n",
      "    accuracy                           0.80      2060\n",
      "   macro avg       0.82      0.58      0.63      2060\n",
      "weighted avg       0.81      0.80      0.79      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.44      0.22      0.30       234\n",
      "         1.0       0.53      0.52      0.53       637\n",
      "         2.0       0.52      0.32      0.40        87\n",
      "         3.0       0.67      0.77      0.72      1102\n",
      "\n",
      "    accuracy                           0.61      2060\n",
      "   macro avg       0.54      0.46      0.48      2060\n",
      "weighted avg       0.60      0.61      0.60      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "multi_knn = MultiOutputClassifier(knn)\n",
    "\n",
    "multi_knn.fit(x_train, y_train)\n",
    "y_pred = multi_knn.predict(x_test)\n",
    "\n",
    "knn_accuracy_type = accuracy_score(y_test['type'], y_pred[:, 0])\n",
    "knn_accuracy_cvss_score  = accuracy_score(y_test['cvss_score'], y_pred[:, 1])\n",
    "\n",
    "knn_scores_type = cross_val_score(knn, x[selected_features], y['type'], cv=kf, scoring='f1_macro')\n",
    "knn_scores_cvss_score  = cross_val_score(knn, x[selected_features], y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"KNN Accuracy for 'type':\", knn_accuracy_type)\n",
    "print(\"KNN Accuracy for 'cvss_score' :\", knn_accuracy_cvss_score)\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", knn_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", knn_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", knn_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", knn_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9223f063",
   "metadata": {},
   "source": [
    "# Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4e80f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy for 'type': 0.7126213592233009\n",
      "AdaBoost Accuracy for 'cvss_score' : 0.5849514563106796\n",
      "K-Fold mean F1 (type): 0.43684599452652995\n",
      "K-Fold std  F1 (type): 0.054786432642828814\n",
      "K-Fold mean F1 (cvss_score): 0.2806172113398212\n",
      "K-Fold std  F1 (cvss_score): 0.0021172211937914463\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        28\n",
      "         1.0       0.71      0.80      0.75        50\n",
      "         2.0       0.63      0.74      0.68       103\n",
      "         3.0       0.09      0.03      0.04       200\n",
      "         4.0       0.64      0.91      0.76       834\n",
      "         5.0       0.00      0.00      0.00        41\n",
      "         6.0       0.00      0.00      0.00        36\n",
      "         7.0       0.72      0.45      0.55       155\n",
      "         8.0       1.00      0.74      0.85       176\n",
      "         9.0       0.00      0.00      0.00        14\n",
      "        10.0       0.95      0.91      0.93       423\n",
      "\n",
      "    accuracy                           0.71      2060\n",
      "   macro avg       0.43      0.42      0.41      2060\n",
      "weighted avg       0.65      0.71      0.67      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.02      0.03       234\n",
      "         1.0       0.54      0.25      0.34       637\n",
      "         2.0       0.00      0.00      0.00        87\n",
      "         3.0       0.59      0.95      0.73      1102\n",
      "\n",
      "    accuracy                           0.58      2060\n",
      "   macro avg       0.37      0.30      0.28      2060\n",
      "weighted avg       0.52      0.58      0.50      2060\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ab = AdaBoostClassifier(random_state=42)\n",
    "multi_ada = MultiOutputClassifier(ab)\n",
    "\n",
    "multi_ada.fit(x_train, y_train)\n",
    "\n",
    "importances = np.mean([est.feature_importances_ for est in multi_ada.estimators_], axis=0)\n",
    "selected_features = x_train.columns[importances > np.mean(importances)]\n",
    "\n",
    "multi_ada.fit(x_train[selected_features], y_train)\n",
    "y_pred = multi_ada.predict(x_test[selected_features])\n",
    "\n",
    "ab_accuracy_type = accuracy_score(y_test['type'], y_pred[:, 0])\n",
    "ab_accuracy_cvss_score  = accuracy_score(y_test['cvss_score'], y_pred[:, 1])\n",
    "\n",
    "ab_scores_type = cross_val_score(ab, x[selected_features], y['type'], cv=kf, scoring='f1_macro')\n",
    "ab_scores_cvss_score  = cross_val_score(ab, x[selected_features], y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"AdaBoost Accuracy for 'type':\", ab_accuracy_type)\n",
    "print(\"AdaBoost Accuracy for 'cvss_score' :\", ab_accuracy_cvss_score)\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", ab_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", ab_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", ab_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", ab_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba0609c",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cb9b6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001279 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2988\n",
      "[LightGBM] [Info] Number of data points in the train set: 8236, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -4.597429\n",
      "[LightGBM] [Info] Start training from score -3.712965\n",
      "[LightGBM] [Info] Start training from score -2.891587\n",
      "[LightGBM] [Info] Start training from score -2.335415\n",
      "[LightGBM] [Info] Start training from score -0.856323\n",
      "[LightGBM] [Info] Start training from score -3.645632\n",
      "[LightGBM] [Info] Start training from score -4.019058\n",
      "[LightGBM] [Info] Start training from score -2.611042\n",
      "[LightGBM] [Info] Start training from score -2.547020\n",
      "[LightGBM] [Info] Start training from score -4.767775\n",
      "[LightGBM] [Info] Start training from score -1.691780\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000713 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2988\n",
      "[LightGBM] [Info] Number of data points in the train set: 8236, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -2.336671\n",
      "[LightGBM] [Info] Start training from score -1.107515\n",
      "[LightGBM] [Info] Start training from score -3.158337\n",
      "[LightGBM] [Info] Start training from score -0.633981\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000222 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2927\n",
      "[LightGBM] [Info] Number of data points in the train set: 8236, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score -4.597429\n",
      "[LightGBM] [Info] Start training from score -3.712965\n",
      "[LightGBM] [Info] Start training from score -2.891587\n",
      "[LightGBM] [Info] Start training from score -2.335415\n",
      "[LightGBM] [Info] Start training from score -0.856323\n",
      "[LightGBM] [Info] Start training from score -3.645632\n",
      "[LightGBM] [Info] Start training from score -4.019058\n",
      "[LightGBM] [Info] Start training from score -2.611042\n",
      "[LightGBM] [Info] Start training from score -2.547020\n",
      "[LightGBM] [Info] Start training from score -4.767775\n",
      "[LightGBM] [Info] Start training from score -1.691780\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000212 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2927\n",
      "[LightGBM] [Info] Number of data points in the train set: 8236, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score -2.336671\n",
      "[LightGBM] [Info] Start training from score -1.107515\n",
      "[LightGBM] [Info] Start training from score -3.158337\n",
      "[LightGBM] [Info] Start training from score -0.633981\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000239 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2908\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score -4.571366\n",
      "[LightGBM] [Info] Start training from score -3.704147\n",
      "[LightGBM] [Info] Start training from score -2.878208\n",
      "[LightGBM] [Info] Start training from score -2.322300\n",
      "[LightGBM] [Info] Start training from score -0.853680\n",
      "[LightGBM] [Info] Start training from score -3.652262\n",
      "[LightGBM] [Info] Start training from score -4.005732\n",
      "[LightGBM] [Info] Start training from score -2.623446\n",
      "[LightGBM] [Info] Start training from score -2.582142\n",
      "[LightGBM] [Info] Start training from score -4.942225\n",
      "[LightGBM] [Info] Start training from score -1.684914\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000246 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2895\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score -4.503312\n",
      "[LightGBM] [Info] Start training from score -3.710082\n",
      "[LightGBM] [Info] Start training from score -2.964749\n",
      "[LightGBM] [Info] Start training from score -2.338780\n",
      "[LightGBM] [Info] Start training from score -0.877219\n",
      "[LightGBM] [Info] Start training from score -3.692382\n",
      "[LightGBM] [Info] Start training from score -4.063361\n",
      "[LightGBM] [Info] Start training from score -2.549911\n",
      "[LightGBM] [Info] Start training from score -2.491924\n",
      "[LightGBM] [Info] Start training from score -4.790994\n",
      "[LightGBM] [Info] Start training from score -1.664696\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000241 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2894\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score -4.516558\n",
      "[LightGBM] [Info] Start training from score -3.728100\n",
      "[LightGBM] [Info] Start training from score -2.893874\n",
      "[LightGBM] [Info] Start training from score -2.343322\n",
      "[LightGBM] [Info] Start training from score -0.866419\n",
      "[LightGBM] [Info] Start training from score -3.740295\n",
      "[LightGBM] [Info] Start training from score -4.005732\n",
      "[LightGBM] [Info] Start training from score -2.647837\n",
      "[LightGBM] [Info] Start training from score -2.515078\n",
      "[LightGBM] [Info] Start training from score -4.706911\n",
      "[LightGBM] [Info] Start training from score -1.657791\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000237 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2908\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score -2.349410\n",
      "[LightGBM] [Info] Start training from score -1.123392\n",
      "[LightGBM] [Info] Start training from score -3.133602\n",
      "[LightGBM] [Info] Start training from score -0.623921\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000234 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2895\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score -2.270190\n",
      "[LightGBM] [Info] Start training from score -1.117140\n",
      "[LightGBM] [Info] Start training from score -3.195691\n",
      "[LightGBM] [Info] Start training from score -0.637609\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000202 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2894\n",
      "[LightGBM] [Info] Number of data points in the train set: 6864, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score -2.288696\n",
      "[LightGBM] [Info] Start training from score -1.120708\n",
      "[LightGBM] [Info] Start training from score -3.150466\n",
      "[LightGBM] [Info] Start training from score -0.635406\n",
      "LightGBM Accuracy for 'type': 0.9004854368932039\n",
      "LightGBM Accuracy for 'cvss_score' : 0.6509708737864077\n",
      "K-Fold mean F1 (type): 0.8278507289593344\n",
      "K-Fold std  F1 (type): 0.0010900398493623074\n",
      "K-Fold mean F1 (cvss_score): 0.5324652577672494\n",
      "K-Fold std  F1 (cvss_score): 0.010850896458448542\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.82      0.90        28\n",
      "         1.0       0.86      0.86      0.86        50\n",
      "         2.0       0.74      0.56      0.64       103\n",
      "         3.0       0.93      0.89      0.91       200\n",
      "         4.0       0.87      0.96      0.92       834\n",
      "         5.0       0.88      0.85      0.86        41\n",
      "         6.0       0.86      0.53      0.66        36\n",
      "         7.0       0.84      0.69      0.76       155\n",
      "         8.0       0.98      0.97      0.97       176\n",
      "         9.0       1.00      1.00      1.00        14\n",
      "        10.0       0.96      0.96      0.96       423\n",
      "\n",
      "    accuracy                           0.90      2060\n",
      "   macro avg       0.90      0.83      0.86      2060\n",
      "weighted avg       0.90      0.90      0.90      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      0.27      0.36       234\n",
      "         1.0       0.55      0.50      0.53       637\n",
      "         2.0       0.88      0.33      0.48        87\n",
      "         3.0       0.70      0.84      0.76      1102\n",
      "\n",
      "    accuracy                           0.65      2060\n",
      "   macro avg       0.67      0.49      0.53      2060\n",
      "weighted avg       0.64      0.65      0.63      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgbm = LGBMClassifier(random_state=42)\n",
    "multi_lgbm = MultiOutputClassifier(lgbm)\n",
    "\n",
    "multi_lgbm.fit(x_train, y_train)\n",
    "\n",
    "importances = np.mean([est.feature_importances_ for est in multi_lgbm.estimators_], axis=0)\n",
    "selected_features = x_train.columns[importances > np.mean(importances)]\n",
    "\n",
    "multi_lgbm.fit(x_train[selected_features], y_train)\n",
    "y_pred = multi_lgbm.predict(x_test[selected_features])\n",
    "\n",
    "lgbm_accuracy_type = accuracy_score(y_test['type'], y_pred[:, 0])\n",
    "lgbm_accuracy_cvss_score  = accuracy_score(y_test['cvss_score'], y_pred[:, 1])\n",
    "\n",
    "lgbm_scores_type = cross_val_score(lgbm, x[selected_features], y['type'], cv=kf, scoring='f1_macro')\n",
    "lgbm_scores_cvss_score  = cross_val_score(lgbm, x[selected_features], y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"LightGBM Accuracy for 'type':\", lgbm_accuracy_type)\n",
    "print(\"LightGBM Accuracy for 'cvss_score' :\", lgbm_accuracy_cvss_score)\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", lgbm_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", lgbm_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", lgbm_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", lgbm_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6733d3",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b168918a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier Accuracy for \"type\": 0.9446601941747573\n",
      "Bagging Classifier Accuracy for \"cvss_score\" : 0.6495145631067961\n",
      "K-Fold mean F1 (type): 0.7984691475527715\n",
      "K-Fold std  F1 (type): 0.014819302691681416\n",
      "K-Fold mean F1 (cvss_score): 0.5174672441015332\n",
      "K-Fold std  F1 (cvss_score): 0.013548073649350388\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.86      0.92        28\n",
      "         1.0       1.00      0.94      0.97        50\n",
      "         2.0       0.82      0.94      0.87       103\n",
      "         3.0       0.97      0.91      0.94       200\n",
      "         4.0       0.94      0.96      0.95       834\n",
      "         5.0       0.97      0.88      0.92        41\n",
      "         6.0       0.78      0.89      0.83        36\n",
      "         7.0       0.90      0.86      0.88       155\n",
      "         8.0       0.98      1.00      0.99       176\n",
      "         9.0       0.82      1.00      0.90        14\n",
      "        10.0       0.98      0.97      0.97       423\n",
      "\n",
      "    accuracy                           0.94      2060\n",
      "   macro avg       0.92      0.93      0.92      2060\n",
      "weighted avg       0.95      0.94      0.94      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.30      0.38       234\n",
      "         1.0       0.54      0.57      0.56       637\n",
      "         2.0       0.72      0.41      0.53        87\n",
      "         3.0       0.72      0.79      0.75      1102\n",
      "\n",
      "    accuracy                           0.65      2060\n",
      "   macro avg       0.62      0.52      0.55      2060\n",
      "weighted avg       0.64      0.65      0.64      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bagging = BaggingClassifier(random_state=42)\n",
    "multi_bagging = MultiOutputClassifier(bagging)\n",
    "\n",
    "multi_bagging.fit(x_train, y_train)\n",
    "y_pred = multi_bagging.predict(x_test)\n",
    "\n",
    "bag_accuracy_type = accuracy_score(y_test['type'], y_pred[:, 0])\n",
    "bag_accuracy_cvss_score  = accuracy_score(y_test['cvss_score'], y_pred[:, 1])\n",
    "\n",
    "bag_scores_type = cross_val_score(bagging, x[selected_features], y['type'], cv=kf, scoring='f1_macro')\n",
    "bag_scores_cvss_score  = cross_val_score(bagging, x[selected_features], y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(f'Bagging Classifier Accuracy for \"type\": {bag_accuracy_type}')\n",
    "print(f'Bagging Classifier Accuracy for \"cvss_score\" : {bag_accuracy_cvss_score}')\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", bag_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", bag_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", bag_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", bag_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60ea184",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0810176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Accuracy for \"type\": 0.6087378640776699\n",
      "SVC Accuracy for \"cvss_score\" : 0.579126213592233\n",
      "K-Fold mean F1 (type): 0.320467579315816\n",
      "K-Fold std  F1 (type): 0.0066060380923177965\n",
      "K-Fold mean F1 (cvss_score): 0.26584172063699024\n",
      "K-Fold std  F1 (cvss_score): 0.0067490062036051806\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        28\n",
      "         1.0       0.53      0.66      0.59        50\n",
      "         2.0       0.67      0.06      0.11       103\n",
      "         3.0       0.50      0.69      0.58       200\n",
      "         4.0       0.59      0.85      0.70       834\n",
      "         5.0       0.00      0.00      0.00        41\n",
      "         6.0       0.00      0.00      0.00        36\n",
      "         7.0       0.60      0.37      0.46       155\n",
      "         8.0       0.60      0.40      0.48       176\n",
      "         9.0       0.00      0.00      0.00        14\n",
      "        10.0       0.80      0.58      0.67       423\n",
      "\n",
      "    accuracy                           0.61      2060\n",
      "   macro avg       0.39      0.33      0.33      2060\n",
      "weighted avg       0.59      0.61      0.57      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       234\n",
      "         1.0       0.56      0.24      0.34       637\n",
      "         2.0       0.00      0.00      0.00        87\n",
      "         3.0       0.58      0.94      0.72      1102\n",
      "\n",
      "    accuracy                           0.58      2060\n",
      "   macro avg       0.29      0.30      0.27      2060\n",
      "weighted avg       0.49      0.58      0.49      2060\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(kernel='rbf', C=5, probability=True)\n",
    "multi_svc = MultiOutputClassifier(svc)\n",
    "\n",
    "multi_svc.fit(x_train[selected_features], y_train)\n",
    "y_pred = multi_svc.predict(x_test[selected_features])\n",
    "\n",
    "svc_accuracy_type = accuracy_score(y_test['type'], y_pred[:, 0])\n",
    "svc_accuracy_cvss_score  = accuracy_score(y_test['cvss_score'], y_pred[:, 1])\n",
    "\n",
    "svc_scores_type = cross_val_score(svc, x[selected_features], y['type'], cv=kf, scoring='f1_macro')\n",
    "svc_scores_cvss_score  = cross_val_score(svc, x[selected_features], y['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(f'SVC Accuracy for \"type\": {svc_accuracy_type}')\n",
    "print(f'SVC Accuracy for \"cvss_score\" : {svc_accuracy_cvss_score}')\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", svc_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", svc_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", svc_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", svc_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0139787d",
   "metadata": {},
   "source": [
    "# Hard Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08cb5b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Voting Accuracy for type: 0.8684466019417476\n",
      "Hard Voting Accuracy for cvss_score : 0.6728155339805825\n",
      "K-fold F1 mean (type): 0.6620942774664145\n",
      "K-fold F1 std  (type): 0.010224171572265451\n",
      "K-fold F1 mean (cvss_score) : 0.5383844188044781\n",
      "K-fold F1 std  (cvss_score) : 0.0062594570041507955\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.07      0.13        28\n",
      "         1.0       0.95      0.78      0.86        50\n",
      "         2.0       0.77      0.45      0.56       103\n",
      "         3.0       0.76      0.83      0.79       200\n",
      "         4.0       0.82      0.97      0.89       834\n",
      "         5.0       1.00      0.56      0.72        41\n",
      "         6.0       0.75      0.08      0.15        36\n",
      "         7.0       0.91      0.81      0.85       155\n",
      "         8.0       1.00      0.99      0.99       176\n",
      "         9.0       1.00      0.21      0.35        14\n",
      "        10.0       0.97      0.95      0.96       423\n",
      "\n",
      "    accuracy                           0.87      2060\n",
      "   macro avg       0.90      0.61      0.66      2060\n",
      "weighted avg       0.87      0.87      0.85      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.54      0.22      0.32       234\n",
      "         1.0       0.61      0.51      0.56       637\n",
      "         2.0       0.89      0.37      0.52        87\n",
      "         3.0       0.70      0.89      0.78      1102\n",
      "\n",
      "    accuracy                           0.67      2060\n",
      "   macro avg       0.69      0.50      0.54      2060\n",
      "weighted avg       0.66      0.67      0.65      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "rf_fs = RandomForestClassifier(n_estimators=300, random_state=42)\n",
    "multi_rf_fs = MultiOutputClassifier(rf_fs)\n",
    "\n",
    "multi_rf_fs.fit(x_train, y_train)\n",
    "\n",
    "importances = np.mean([est.feature_importances_ for est in multi_rf_fs.estimators_], axis=0)\n",
    "selected_features = x_train.columns[importances > np.mean(importances)]\n",
    "\n",
    "model1 = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "model2 = ExtraTreesClassifier(n_estimators=200, random_state=42)\n",
    "model3 = LogisticRegression(max_iter=500)\n",
    "\n",
    "voting_hard = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', model1),\n",
    "        ('et', model2),\n",
    "        ('lr', model3)\n",
    "    ],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "multi_voting_hard = MultiOutputClassifier(voting_hard)\n",
    "\n",
    "multi_voting_hard.fit(x_train[selected_features], y_train)\n",
    "y_pred = multi_voting_hard.predict(x_test[selected_features])\n",
    "\n",
    "hard_acc_type = accuracy_score(y_test[\"type\"], y_pred[:, 0])\n",
    "hard_acc_cvss_score  = accuracy_score(y_test[\"cvss_score\"],  y_pred[:, 1])\n",
    "\n",
    "hard_scores_type = cross_val_score(voting_hard, x[selected_features], y['type'], cv=kf, scoring='f1_macro')\n",
    "hard_scores_cvss_score  = cross_val_score(voting_hard, x[selected_features], y['cvss_score'],  cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"Hard Voting Accuracy for type:\", hard_acc_type)\n",
    "print(\"Hard Voting Accuracy for cvss_score :\", hard_acc_cvss_score)\n",
    "\n",
    "print(\"K-fold F1 mean (type):\", hard_scores_type.mean())\n",
    "print(\"K-fold F1 std  (type):\", hard_scores_type.std())\n",
    "\n",
    "print(\"K-fold F1 mean (cvss_score) :\", hard_scores_cvss_score.mean())\n",
    "print(\"K-fold F1 std  (cvss_score) :\", hard_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test[\"type\"], y_pred[:, 0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test[\"cvss_score\"],  y_pred[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8835f1d9",
   "metadata": {},
   "source": [
    "# Soft Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61a77dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft Voting Accuracy for type: 0.858252427184466\n",
      "Soft Voting Accuracy for cvss_score : 0.670873786407767\n",
      "K-fold F1 mean (type): 0.6141315241406103\n",
      "K-fold F1 std  (type): 0.007163728409382124\n",
      "K-fold F1 mean (cvss_score) : 0.5155132108741932\n",
      "K-fold F1 std  (cvss_score) : 0.00968546053559784\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.04      0.07        28\n",
      "         1.0       0.98      0.80      0.88        50\n",
      "         2.0       0.80      0.36      0.50       103\n",
      "         3.0       0.74      0.81      0.78       200\n",
      "         4.0       0.81      0.97      0.88       834\n",
      "         5.0       0.96      0.54      0.69        41\n",
      "         6.0       1.00      0.06      0.11        36\n",
      "         7.0       0.87      0.79      0.83       155\n",
      "         8.0       1.00      0.99      0.99       176\n",
      "         9.0       0.00      0.00      0.00        14\n",
      "        10.0       0.97      0.95      0.96       423\n",
      "\n",
      "    accuracy                           0.86      2060\n",
      "   macro avg       0.83      0.57      0.61      2060\n",
      "weighted avg       0.86      0.86      0.84      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.21      0.31       234\n",
      "         1.0       0.61      0.50      0.55       637\n",
      "         2.0       0.88      0.33      0.48        87\n",
      "         3.0       0.69      0.90      0.78      1102\n",
      "\n",
      "    accuracy                           0.67      2060\n",
      "   macro avg       0.70      0.48      0.53      2060\n",
      "weighted avg       0.67      0.67      0.64      2060\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "rf_fs = RandomForestClassifier(n_estimators=300, random_state=42)\n",
    "multi_rf_fs = MultiOutputClassifier(rf_fs)\n",
    "\n",
    "multi_rf_fs.fit(x_train, y_train)\n",
    "\n",
    "importances = np.mean([est.feature_importances_ for est in multi_rf_fs.estimators_], axis=0)\n",
    "selected_features = x_train.columns[importances > np.mean(importances)]\n",
    "\n",
    "model1 = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "model2 = ExtraTreesClassifier(n_estimators=200, random_state=42)\n",
    "model3 = LogisticRegression(max_iter=500)\n",
    "\n",
    "voting_soft = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', model1),\n",
    "        ('et', model2),\n",
    "        ('lr', model3)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "multi_voting_soft = MultiOutputClassifier(voting_soft)\n",
    "\n",
    "multi_voting_soft.fit(x_train[selected_features], y_train)\n",
    "y_pred = multi_voting_soft.predict(x_test[selected_features])\n",
    "\n",
    "soft_acc_type = accuracy_score(y_test[\"type\"], y_pred[:, 0])\n",
    "soft_acc_cvss_score  = accuracy_score(y_test[\"cvss_score\"],  y_pred[:, 1])\n",
    "\n",
    "soft_scores_type = cross_val_score(voting_soft, x[selected_features], y['type'], cv=kf, scoring='f1_macro')\n",
    "soft_scores_cvss_score  = cross_val_score(voting_soft, x[selected_features], y['cvss_score'],  cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"Soft Voting Accuracy for type:\", soft_acc_type)\n",
    "print(\"Soft Voting Accuracy for cvss_score :\", soft_acc_cvss_score)\n",
    "\n",
    "print(\"K-fold F1 mean (type):\", soft_scores_type.mean())\n",
    "print(\"K-fold F1 std  (type):\", soft_scores_type.std())\n",
    "\n",
    "print(\"K-fold F1 mean (cvss_score) :\", soft_scores_cvss_score.mean())\n",
    "print(\"K-fold F1 std  (cvss_score) :\", soft_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test[\"type\"], y_pred[:, 0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test[\"cvss_score\"],  y_pred[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd7ff14",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f5bae8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Accuracy for type: 0.8849514563106796\n",
      "Stacking Accuracy for cvss_score : 0.6776699029126214\n",
      "K-fold F1 mean (type): 0.73703770188323\n",
      "K-fold F1 std  (type): 0.012695507400019512\n",
      "K-fold F1 mean (cvss_score) : 0.5484020365417034\n",
      "K-fold F1 std  (cvss_score) : 0.012797966442802915\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.14      0.23        28\n",
      "         1.0       0.98      0.84      0.90        50\n",
      "         2.0       0.72      0.48      0.57       103\n",
      "         3.0       0.80      0.87      0.83       200\n",
      "         4.0       0.86      0.96      0.91       834\n",
      "         5.0       0.89      0.76      0.82        41\n",
      "         6.0       0.70      0.19      0.30        36\n",
      "         7.0       0.87      0.83      0.85       155\n",
      "         8.0       1.00      0.99      0.99       176\n",
      "         9.0       1.00      0.57      0.73        14\n",
      "        10.0       0.97      0.96      0.97       423\n",
      "\n",
      "    accuracy                           0.88      2060\n",
      "   macro avg       0.85      0.69      0.74      2060\n",
      "weighted avg       0.88      0.88      0.87      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.58      0.26      0.36       234\n",
      "         1.0       0.60      0.55      0.57       637\n",
      "         2.0       0.84      0.41      0.55        87\n",
      "         3.0       0.72      0.86      0.78      1102\n",
      "\n",
      "    accuracy                           0.68      2060\n",
      "   macro avg       0.68      0.52      0.57      2060\n",
      "weighted avg       0.67      0.68      0.66      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "rf_fs = RandomForestClassifier(n_estimators=300, random_state=42)\n",
    "multi_rf_fs = MultiOutputClassifier(rf_fs)\n",
    "multi_rf_fs.fit(x_train, y_train)\n",
    "\n",
    "importances = np.mean([est.feature_importances_ for est in multi_rf_fs.estimators_], axis=0)\n",
    "selected_features = x_train.columns[importances > np.mean(importances)]\n",
    "\n",
    "base1 = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "base2 = ExtraTreesClassifier(n_estimators=200, random_state=42)\n",
    "base3 = LogisticRegression(max_iter=500)\n",
    "\n",
    "stacking = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', base1),\n",
    "        ('et', base2),\n",
    "        ('lr', base3)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(max_iter=500)\n",
    ")\n",
    "\n",
    "multi_stacking = MultiOutputClassifier(stacking)\n",
    "multi_stacking.fit(x_train[selected_features], y_train)\n",
    "y_pred = multi_stacking.predict(x_test[selected_features])\n",
    "\n",
    "stacking_acc_type = accuracy_score(y_test[\"type\"], y_pred[:, 0])\n",
    "stacking_acc_cvss_score  = accuracy_score(y_test[\"cvss_score\"],  y_pred[:, 1])\n",
    "\n",
    "stacking_scores_type = cross_val_score(stacking, x[selected_features], y['type'], cv=kf, scoring='f1_macro')\n",
    "stacking_scores_cvss_score  = cross_val_score(stacking, x[selected_features], y['cvss_score'],  cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"Stacking Accuracy for type:\", stacking_acc_type)\n",
    "print(\"Stacking Accuracy for cvss_score :\", stacking_acc_cvss_score)\n",
    "print(\"K-fold F1 mean (type):\", stacking_scores_type.mean())\n",
    "print(\"K-fold F1 std  (type):\", stacking_scores_type.std())\n",
    "print(\"K-fold F1 mean (cvss_score) :\", stacking_scores_cvss_score.mean())\n",
    "print(\"K-fold F1 std  (cvss_score) :\", stacking_scores_cvss_score.std())\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test[\"type\"], y_pred[:, 0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test[\"cvss_score\"], y_pred[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed620d",
   "metadata": {},
   "source": [
    "# Bagged KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f3e463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging KNN Accuracy for \"type\": 0.8121359223300971\n",
      "Bagging KNN Accuracy for \"cvss_score\": 0.6242718446601941\n",
      "K-Fold mean F1 (type): 0.4772369616127343\n",
      "K-Fold std  F1 (type): 0.011757416078566299\n",
      "K-Fold mean F1 (cvss_score): 0.46596920817337417\n",
      "K-Fold std  F1 (cvss_score): 0.00621545773156151\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.07      0.12        28\n",
      "         1.0       0.85      0.78      0.81        50\n",
      "         2.0       0.83      0.71      0.76       103\n",
      "         3.0       0.56      0.66      0.61       200\n",
      "         4.0       0.78      0.89      0.83       834\n",
      "         5.0       0.96      0.59      0.73        41\n",
      "         6.0       0.78      0.19      0.31        36\n",
      "         7.0       0.83      0.65      0.73       155\n",
      "         8.0       0.98      0.95      0.97       176\n",
      "         9.0       1.00      0.43      0.60        14\n",
      "        10.0       0.93      0.91      0.92       423\n",
      "\n",
      "    accuracy                           0.81      2060\n",
      "   macro avg       0.80      0.62      0.67      2060\n",
      "weighted avg       0.82      0.81      0.81      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.45      0.23      0.31       234\n",
      "         1.0       0.55      0.50      0.53       637\n",
      "         2.0       0.46      0.33      0.39        87\n",
      "         3.0       0.68      0.80      0.74      1102\n",
      "\n",
      "    accuracy                           0.62      2060\n",
      "   macro avg       0.54      0.47      0.49      2060\n",
      "weighted avg       0.61      0.62      0.61      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bag_knn = BaggingClassifier(\n",
    "    estimator=KNeighborsClassifier(),\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "multi_bag_knn = MultiOutputClassifier(bag_knn)\n",
    "\n",
    "multi_bag_knn.fit(x_train, y_train)\n",
    "y_pred = multi_bag_knn.predict(x_test)\n",
    "\n",
    "bag_knn_accuracy_type = accuracy_score(y_test['type'], y_pred[:, 0])\n",
    "bag_knn_accuracy_cvss_score = accuracy_score(y_test['cvss_score'], y_pred[:, 1])\n",
    "\n",
    "bag_knn_scores_type = cross_val_score(bag_knn, x_train, y_train['type'], cv=kf, scoring='f1_macro')\n",
    "bag_knn_scores_cvss_score = cross_val_score(bag_knn, x_train, y_train['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(f'Bagging KNN Accuracy for \"type\": {bag_knn_accuracy_type}')\n",
    "print(f'Bagging KNN Accuracy for \"cvss_score\": {bag_knn_accuracy_cvss_score}')\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", bag_knn_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", bag_knn_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", bag_knn_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", bag_knn_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cf4546",
   "metadata": {},
   "source": [
    "# Bagged DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14f47805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging DecisionTree Accuracy for 'type': 0.95\n",
      "Bagging DecisionTree Accuracy for 'cvss_score': 0.6694174757281554\n",
      "K-Fold mean F1 (type): 0.9243748215703596\n",
      "K-Fold std  F1 (type): 0.006297110023684206\n",
      "K-Fold mean F1 (cvss_score): 0.5389630682376138\n",
      "K-Fold std  F1 (cvss_score): 0.020637064502082584\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.86      0.92        28\n",
      "         1.0       1.00      0.96      0.98        50\n",
      "         2.0       0.82      0.91      0.87       103\n",
      "         3.0       0.98      0.93      0.95       200\n",
      "         4.0       0.95      0.96      0.95       834\n",
      "         5.0       0.97      0.90      0.94        41\n",
      "         6.0       0.79      0.92      0.85        36\n",
      "         7.0       0.90      0.88      0.89       155\n",
      "         8.0       0.99      1.00      1.00       176\n",
      "         9.0       0.82      1.00      0.90        14\n",
      "        10.0       0.98      0.97      0.98       423\n",
      "\n",
      "    accuracy                           0.95      2060\n",
      "   macro avg       0.93      0.94      0.93      2060\n",
      "weighted avg       0.95      0.95      0.95      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.28      0.37       234\n",
      "         1.0       0.58      0.54      0.56       637\n",
      "         2.0       0.86      0.41      0.56        87\n",
      "         3.0       0.71      0.85      0.78      1102\n",
      "\n",
      "    accuracy                           0.67      2060\n",
      "   macro avg       0.67      0.52      0.57      2060\n",
      "weighted avg       0.66      0.67      0.65      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bag_dt = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "multi_bag_dt = MultiOutputClassifier(bag_dt)\n",
    "\n",
    "multi_bag_dt.fit(x_train, y_train)\n",
    "\n",
    "multi_bag_dt.fit(x_train, y_train)\n",
    "y_pred = multi_bag_dt.predict(x_test)\n",
    "\n",
    "bag_dt_accuracy_type = accuracy_score(y_test['type'], y_pred[:, 0])\n",
    "bag_dt_accuracy_cvss_score = accuracy_score(y_test['cvss_score'], y_pred[:, 1])\n",
    "\n",
    "bag_dt_scores_type = cross_val_score(bag_dt, x_train, y_train['type'], cv=kf, scoring='f1_macro')\n",
    "bag_dt_scores_cvss_score = cross_val_score(bag_dt, x_train, y_train['cvss_score'], cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(\"Bagging DecisionTree Accuracy for 'type':\", bag_dt_accuracy_type)\n",
    "print(\"Bagging DecisionTree Accuracy for 'cvss_score':\", bag_dt_accuracy_cvss_score)\n",
    "\n",
    "print(\"K-Fold mean F1 (type):\", bag_dt_scores_type.mean())\n",
    "print(\"K-Fold std  F1 (type):\", bag_dt_scores_type.std())\n",
    "\n",
    "print(\"K-Fold mean F1 (cvss_score):\", bag_dt_scores_cvss_score.mean())\n",
    "print(\"K-Fold std  F1 (cvss_score):\", bag_dt_scores_cvss_score.std())\n",
    "\n",
    "print(\"\\nClassification Report for 'type':\\n\", classification_report(y_test['type'], y_pred[:,0]))\n",
    "print(\"\\nClassification Report for 'cvss_score':\\n\", classification_report(y_test['cvss_score'], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c80ae74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                            Embedded Method Comparison                                             </span>\n",
       "\n",
       "<span style=\"font-weight: bold\"> Algorithm          </span><span style=\"font-weight: bold\"> Type Acc </span><span style=\"font-weight: bold\"> K-Fold Mean </span><span style=\"font-weight: bold\"> K-Fold Std </span><span style=\"font-weight: bold\"> cvss_score Acc </span><span style=\"font-weight: bold\"> K-Fold Mean </span><span style=\"font-weight: bold\"> K-Fold Std </span><span style=\"font-weight: bold\"> Combined </span>\n",
       "\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">HistGradientBoost</span>  <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.95</span>      <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.86</span>         <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.00</span>        <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.68</span>            <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.55</span>         <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.01</span>            <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.82</span> \n",
       "\n",
       " Bagged DT           0.95      0.92         0.01        0.67            0.54         0.02            0.81 \n",
       "\n",
       " Bagging             0.94      0.80         0.01        0.65            0.52         0.01            0.80 \n",
       "\n",
       " GradientBoosting    0.93      0.88         0.01        0.65            0.52         0.01            0.79 \n",
       "\n",
       " Stacking            0.88      0.74         0.01        0.68            0.55         0.01            0.78 \n",
       "\n",
       " RandomForest        0.88      0.68         0.01        0.68            0.54         0.01            0.78 \n",
       "\n",
       " ExtraTrees          0.88      0.70         0.01        0.67            0.55         0.02            0.78 \n",
       "\n",
       " LightGBM            0.90      0.83         0.00        0.65            0.53         0.01            0.78 \n",
       "\n",
       " Hard Voting         0.87      0.66         0.01        0.67            0.54         0.01            0.77 \n",
       "\n",
       " Soft Voting         0.86      0.61         0.01        0.67            0.52         0.01            0.76 \n",
       "\n",
       " DecisionTree        0.90      0.84         0.01        0.59            0.46         0.01            0.75 \n",
       "\n",
       " Bagged KNN          0.81      0.48         0.01        0.62            0.47         0.01            0.72 \n",
       "\n",
       " KNN                 0.80      0.54         0.00        0.61            0.50         0.01            0.71 \n",
       "\n",
       " LogisticRegression  0.75      0.59         0.01        0.59            0.28         0.00            0.67 \n",
       "\n",
       " AdaBoost            0.71      0.44         0.05        0.58            0.28         0.00            0.65 \n",
       "\n",
       " <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">SVM</span>                 <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.61</span>      <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.32</span>         <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.01</span>        <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.58</span>            <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.27</span>         <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.01</span>            <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.59</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                            Embedded Method Comparison                                             \u001b[0m\n",
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mAlgorithm         \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mType Acc\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mK-Fold Mean\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mK-Fold Std\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mcvss_score Acc\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mK-Fold Mean\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mK-Fold Std\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mCombined\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " \u001b[1;32mHistGradientBoost\u001b[0m  \u001b[1;32m0.95\u001b[0m      \u001b[1;32m0.86\u001b[0m         \u001b[1;32m0.00\u001b[0m        \u001b[1;32m0.68\u001b[0m            \u001b[1;32m0.55\u001b[0m         \u001b[1;32m0.01\u001b[0m            \u001b[1;32m0.82\u001b[0m \n",
       "\n",
       " Bagged DT           0.95      0.92         0.01        0.67            0.54         0.02            0.81 \n",
       "\n",
       " Bagging             0.94      0.80         0.01        0.65            0.52         0.01            0.80 \n",
       "\n",
       " GradientBoosting    0.93      0.88         0.01        0.65            0.52         0.01            0.79 \n",
       "\n",
       " Stacking            0.88      0.74         0.01        0.68            0.55         0.01            0.78 \n",
       "\n",
       " RandomForest        0.88      0.68         0.01        0.68            0.54         0.01            0.78 \n",
       "\n",
       " ExtraTrees          0.88      0.70         0.01        0.67            0.55         0.02            0.78 \n",
       "\n",
       " LightGBM            0.90      0.83         0.00        0.65            0.53         0.01            0.78 \n",
       "\n",
       " Hard Voting         0.87      0.66         0.01        0.67            0.54         0.01            0.77 \n",
       "\n",
       " Soft Voting         0.86      0.61         0.01        0.67            0.52         0.01            0.76 \n",
       "\n",
       " DecisionTree        0.90      0.84         0.01        0.59            0.46         0.01            0.75 \n",
       "\n",
       " Bagged KNN          0.81      0.48         0.01        0.62            0.47         0.01            0.72 \n",
       "\n",
       " KNN                 0.80      0.54         0.00        0.61            0.50         0.01            0.71 \n",
       "\n",
       " LogisticRegression  0.75      0.59         0.01        0.59            0.28         0.00            0.67 \n",
       "\n",
       " AdaBoost            0.71      0.44         0.05        0.58            0.28         0.00            0.65 \n",
       "\n",
       " \u001b[1;31mSVM\u001b[0m                 \u001b[1;31m0.61\u001b[0m      \u001b[1;31m0.32\u001b[0m         \u001b[1;31m0.01\u001b[0m        \u001b[1;31m0.58\u001b[0m            \u001b[1;31m0.27\u001b[0m         \u001b[1;31m0.01\u001b[0m            \u001b[1;31m0.59\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.table import Table\n",
    "from rich.console import Console\n",
    "\n",
    "console = Console()\n",
    "\n",
    "results = [\n",
    "    ['LogisticRegression', lr_accuracy_type, lr_scores_type.mean(), lr_scores_type.std(), lr_accuracy_cvss_score, lr_scores_cvss_score.mean(), lr_scores_cvss_score.std()],\n",
    "    ['DecisionTree', dt_accuracy_type, dt_scores_type.mean(), dt_scores_type.std(), dt_accuracy_cvss_score, dt_scores_cvss_score.mean(), dt_scores_cvss_score.std()],\n",
    "    ['RandomForest', rf_accuracy_type, rf_scores_type.mean(), rf_scores_type.std(), rf_accuracy_cvss_score, rf_scores_cvss_score.mean(), rf_scores_cvss_score.std()],\n",
    "    ['ExtraTrees', et_accuracy_type, et_scores_type.mean(), et_scores_type.std(), et_accuracy_cvss_score, et_scores_cvss_score.mean(), et_scores_cvss_score.std()],\n",
    "    ['GradientBoosting', gb_accuracy_type, gb_scores_type.mean(), gb_scores_type.std(), gb_accuracy_cvss_score, gb_scores_cvss_score.mean(), gb_scores_cvss_score.std()],\n",
    "    ['HistGradientBoosting', hgb_accuracy_type, hgb_scores_type.mean(), hgb_scores_type.std(), hgb_accuracy_cvss_score, hgb_scores_cvss_score.mean(), hgb_scores_cvss_score.std()],\n",
    "    ['KNN', knn_accuracy_type, knn_scores_type.mean(), knn_scores_type.std(), knn_accuracy_cvss_score, knn_scores_cvss_score.mean(), knn_scores_cvss_score.std()],\n",
    "    ['AdaBoost', ab_accuracy_type, ab_scores_type.mean(), ab_scores_type.std(), ab_accuracy_cvss_score, ab_scores_cvss_score.mean(), ab_scores_cvss_score.std()],\n",
    "    ['LightGBM', lgbm_accuracy_type, lgbm_scores_type.mean(), lgbm_scores_type.std(), lgbm_accuracy_cvss_score, lgbm_scores_cvss_score.mean(), lgbm_scores_cvss_score.std()],\n",
    "    ['Bagging', bag_accuracy_type, bag_scores_type.mean(), bag_scores_type.std(), bag_accuracy_cvss_score, bag_scores_cvss_score.mean(), bag_scores_cvss_score.std()],\n",
    "    ['Hard Voting', hard_acc_type, hard_scores_type.mean(), hard_scores_type.std(), hard_acc_cvss_score, hard_scores_cvss_score.mean(), hard_scores_cvss_score.std()],\n",
    "    ['Soft Voting', soft_acc_type, soft_scores_type.mean(), soft_scores_type.std(), soft_acc_cvss_score, soft_scores_cvss_score.mean(), soft_scores_cvss_score.std()],\n",
    "    ['Stacking', stacking_acc_type, stacking_scores_type.mean(), stacking_scores_type.std(), stacking_acc_cvss_score, stacking_scores_cvss_score.mean(), stacking_scores_cvss_score.std()],\n",
    "    ['SVM', svc_accuracy_type, svc_scores_type.mean(), svc_scores_type.std(), svc_accuracy_cvss_score, svc_scores_cvss_score.mean(), svc_scores_cvss_score.std()],\n",
    "    ['Bagged KNN', bag_knn_accuracy_type, bag_knn_scores_type.mean(), bag_knn_scores_type.std(), bag_knn_accuracy_cvss_score, bag_knn_scores_cvss_score.mean(), bag_knn_scores_cvss_score.std()],\n",
    "    ['Bagged DT', bag_dt_accuracy_type, bag_dt_scores_type.mean(), bag_dt_scores_type.std(), bag_dt_accuracy_cvss_score, bag_dt_scores_cvss_score.mean(), bag_dt_scores_cvss_score.std()],\n",
    "]\n",
    "\n",
    "\n",
    "for row in results:\n",
    "    type_acc = row[1]\n",
    "    cvss_score_acc = row[4]\n",
    "    combined = (type_acc + cvss_score_acc) / 2\n",
    "    row.append(combined)\n",
    "\n",
    "result_sorted = sorted(results, key=lambda i: i[-1], reverse=True)\n",
    "\n",
    "best_model = max(results, key=lambda x: x[-1])\n",
    "worst_model = min(results, key=lambda x: x[-1])\n",
    "\n",
    "table = Table(title=\"Embedded Method Comparison\", show_lines=True)\n",
    "table.add_column(\"Algorithm\")\n",
    "table.add_column(\"Type Acc\")\n",
    "table.add_column(\"K-Fold Mean\")\n",
    "table.add_column(\"K-Fold Std\")\n",
    "table.add_column(\"cvss_score Acc\")\n",
    "table.add_column(\"K-Fold Mean\")\n",
    "table.add_column(\"K-Fold Std\")\n",
    "table.add_column(\"Combined\", justify=\"right\")\n",
    "\n",
    "for row in result_sorted:\n",
    "    algo, type_acc, kmean_type, kstd_type, cvss_score_acc, kmean_cvss_score, kstd_cvss_score, combined = row\n",
    "\n",
    "    if row == best_model:\n",
    "        table.add_row(\n",
    "            f\"[bold green]{algo}[/bold green]\",\n",
    "            f\"[bold green]{type_acc:.2f}[/bold green]\",\n",
    "            f\"[bold green]{kmean_type:.2f}[/bold green]\",\n",
    "            f\"[bold green]{kstd_type:.2f}[/bold green]\",\n",
    "            f\"[bold green]{cvss_score_acc:.2f}[/bold green]\",\n",
    "            f\"[bold green]{kmean_cvss_score:.2f}[/bold green]\",\n",
    "            f\"[bold green]{kstd_cvss_score:.2f}[/bold green]\",\n",
    "            f\"[bold green]{combined:.2f}[/bold green]\",\n",
    "        )\n",
    "    elif row == worst_model:\n",
    "        table.add_row(\n",
    "            f\"[bold red]{algo}[/bold red]\",\n",
    "            f\"[bold red]{type_acc:.2f}[/bold red]\",\n",
    "            f\"[bold red]{kmean_type:.2f}[/bold red]\",\n",
    "            f\"[bold red]{kstd_type:.2f}[/bold red]\",\n",
    "            f\"[bold red]{cvss_score_acc:.2f}[/bold red]\",\n",
    "            f\"[bold red]{kmean_cvss_score:.2f}[/bold red]\",\n",
    "            f\"[bold red]{kstd_cvss_score:.2f}[/bold red]\",\n",
    "            f\"[bold red]{combined:.2f}[/bold red]\",\n",
    "        )\n",
    "    else:\n",
    "        table.add_row(\n",
    "            algo, f\"{type_acc:.2f}\", f\"{kmean_type:.2f}\", f\"{kstd_type:.2f}\",\n",
    "            f\"{cvss_score_acc:.2f}\", f\"{kmean_cvss_score:.2f}\", f\"{kstd_cvss_score:.2f}\", f\"{combined:.2f}\"\n",
    "        )\n",
    "\n",
    "console.print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd3d22b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                            Embedded Method Comparison                                             </span>\n",
       "\n",
       "<span style=\"font-weight: bold\"> Algorithm          </span><span style=\"font-weight: bold\"> Type Acc </span><span style=\"font-weight: bold\"> K-Fold Mean </span><span style=\"font-weight: bold\"> K-Fold Std </span><span style=\"font-weight: bold\"> cvss_score Acc </span><span style=\"font-weight: bold\"> K-Fold Mean </span><span style=\"font-weight: bold\"> K-Fold Std </span><span style=\"font-weight: bold\"> Combined </span>\n",
       "\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">HistGradientBoost</span>  <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.95</span>      <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.86</span>         <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.00</span>        <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.68</span>            <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.55</span>         <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.01</span>            <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.82</span> \n",
       "\n",
       " Bagged DT           0.95      0.92         0.01        0.67            0.54         0.02            0.81 \n",
       "\n",
       " Bagging             0.94      0.80         0.01        0.65            0.52         0.01            0.80 \n",
       "\n",
       " GradientBoosting    0.93      0.88         0.01        0.65            0.52         0.01            0.79 \n",
       "\n",
       " Stacking            0.88      0.74         0.01        0.68            0.55         0.01            0.78 \n",
       "\n",
       " RandomForest        0.88      0.68         0.01        0.68            0.54         0.01            0.78 \n",
       "\n",
       " ExtraTrees          0.88      0.70         0.01        0.67            0.55         0.02            0.78 \n",
       "\n",
       " LightGBM            0.90      0.83         0.00        0.65            0.53         0.01            0.78 \n",
       "\n",
       " Hard Voting         0.87      0.66         0.01        0.67            0.54         0.01            0.77 \n",
       "\n",
       " Soft Voting         0.86      0.61         0.01        0.67            0.52         0.01            0.76 \n",
       "\n",
       " DecisionTree        0.90      0.84         0.01        0.59            0.46         0.01            0.75 \n",
       "\n",
       " Bagged KNN          0.81      0.48         0.01        0.62            0.47         0.01            0.72 \n",
       "\n",
       " KNN                 0.80      0.54         0.00        0.61            0.50         0.01            0.71 \n",
       "\n",
       " LogisticRegression  0.75      0.59         0.01        0.59            0.28         0.00            0.67 \n",
       "\n",
       " AdaBoost            0.71      0.44         0.05        0.58            0.28         0.00            0.65 \n",
       "\n",
       " <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">SVM</span>                 <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.61</span>      <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.32</span>         <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.01</span>        <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.58</span>            <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.27</span>         <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.01</span>            <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.59</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                            Embedded Method Comparison                                             \u001b[0m\n",
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mAlgorithm         \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mType Acc\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mK-Fold Mean\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mK-Fold Std\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mcvss_score Acc\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mK-Fold Mean\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mK-Fold Std\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mCombined\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " \u001b[1;32mHistGradientBoost\u001b[0m  \u001b[1;32m0.95\u001b[0m      \u001b[1;32m0.86\u001b[0m         \u001b[1;32m0.00\u001b[0m        \u001b[1;32m0.68\u001b[0m            \u001b[1;32m0.55\u001b[0m         \u001b[1;32m0.01\u001b[0m            \u001b[1;32m0.82\u001b[0m \n",
       "\n",
       " Bagged DT           0.95      0.92         0.01        0.67            0.54         0.02            0.81 \n",
       "\n",
       " Bagging             0.94      0.80         0.01        0.65            0.52         0.01            0.80 \n",
       "\n",
       " GradientBoosting    0.93      0.88         0.01        0.65            0.52         0.01            0.79 \n",
       "\n",
       " Stacking            0.88      0.74         0.01        0.68            0.55         0.01            0.78 \n",
       "\n",
       " RandomForest        0.88      0.68         0.01        0.68            0.54         0.01            0.78 \n",
       "\n",
       " ExtraTrees          0.88      0.70         0.01        0.67            0.55         0.02            0.78 \n",
       "\n",
       " LightGBM            0.90      0.83         0.00        0.65            0.53         0.01            0.78 \n",
       "\n",
       " Hard Voting         0.87      0.66         0.01        0.67            0.54         0.01            0.77 \n",
       "\n",
       " Soft Voting         0.86      0.61         0.01        0.67            0.52         0.01            0.76 \n",
       "\n",
       " DecisionTree        0.90      0.84         0.01        0.59            0.46         0.01            0.75 \n",
       "\n",
       " Bagged KNN          0.81      0.48         0.01        0.62            0.47         0.01            0.72 \n",
       "\n",
       " KNN                 0.80      0.54         0.00        0.61            0.50         0.01            0.71 \n",
       "\n",
       " LogisticRegression  0.75      0.59         0.01        0.59            0.28         0.00            0.67 \n",
       "\n",
       " AdaBoost            0.71      0.44         0.05        0.58            0.28         0.00            0.65 \n",
       "\n",
       " \u001b[1;31mSVM\u001b[0m                 \u001b[1;31m0.61\u001b[0m      \u001b[1;31m0.32\u001b[0m         \u001b[1;31m0.01\u001b[0m        \u001b[1;31m0.58\u001b[0m            \u001b[1;31m0.27\u001b[0m         \u001b[1;31m0.01\u001b[0m            \u001b[1;31m0.59\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "temp_console = Console(record=True)\n",
    "temp_console.print(table)\n",
    "text = temp_console.export_text()\n",
    "with open('results/feature_selection_compare.txt', 'a', encoding='utf-8') as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0ff40c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
